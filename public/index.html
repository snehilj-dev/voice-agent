<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Voice AI Agent</title>
  <link rel="icon" type="image/svg+xml" href="/favicon.svg">
  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.19.0/dist/ort.min.js"></script>
  <style>
    :root {
      --primary-color: #667eea;
      --secondary-color: #764ba2;
      --bg-gradient: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      --glass-bg: rgba(255, 255, 255, 0.95);
      --text-color: #333;
      --error-color: #dc3545;
      --success-color: #28a745;
    }

    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    body {
      font-family: 'Segoe UI', Roboto, Helvetica, Arial, sans-serif;
      background: var(--bg-gradient);
      min-height: 100vh;
      display: flex;
      justify-content: center;
      align-items: center;
      color: var(--text-color);
    }

    .app {
      width: 100%;
      padding: 20px;
      display: flex;
      justify-content: center;
    }

    .container {
      background: var(--glass-bg);
      backdrop-filter: blur(10px);
      border-radius: 20px;
      padding: 40px;
      width: 100%;
      max-width: 500px;
      box-shadow: 0 20px 60px rgba(0, 0, 0, 0.3);
      text-align: center;
    }

    .header {
      margin-bottom: 30px;
    }

    .header h1 {
      font-size: 28px;
      margin-bottom: 10px;
      color: #333;
    }

    .subtitle {
      color: #666;
      font-size: 16px;
    }

    .status-card {
      background: #f8f9fa;
      border-radius: 15px;
      padding: 20px;
      margin-bottom: 30px;
      border: 1px solid #e9ecef;
      display: flex;
      flex-direction: column;
      align-items: center;
      gap: 15px;
    }

    .status-indicator {
      display: flex;
      flex-direction: column;
      align-items: center;
      gap: 10px;
    }

    .pulse {
      width: 15px;
      height: 15px;
      border-radius: 50%;
      background-color: #ccc;
      transition: all 0.3s ease;
    }

    .pulse.listening {
      background-color: var(--error-color);
      box-shadow: 0 0 0 0 rgba(220, 53, 69, 0.7);
      animation: pulse-red 1.5s infinite;
    }

    .pulse.speaking {
      background-color: var(--success-color);
      box-shadow: 0 0 0 0 rgba(40, 167, 69, 0.7);
      animation: pulse-green 1.5s infinite;
    }

    .pulse.processing {
      background-color: var(--primary-color);
      animation: spin 1s infinite linear;
      /* Make it look like a spinner for processing if desired, or just pulse blue */
      animation: pulse-blue 1.5s infinite;
    }

    @keyframes pulse-red {
      0% {
        transform: scale(0.95);
        box-shadow: 0 0 0 0 rgba(220, 53, 69, 0.7);
      }

      70% {
        transform: scale(1);
        box-shadow: 0 0 0 10px rgba(220, 53, 69, 0);
      }

      100% {
        transform: scale(0.95);
        box-shadow: 0 0 0 0 rgba(220, 53, 69, 0);
      }
    }

    @keyframes pulse-green {
      0% {
        transform: scale(0.95);
        box-shadow: 0 0 0 0 rgba(40, 167, 69, 0.7);
      }

      70% {
        transform: scale(1);
        box-shadow: 0 0 0 10px rgba(40, 167, 69, 0);
      }

      100% {
        transform: scale(0.95);
        box-shadow: 0 0 0 0 rgba(40, 167, 69, 0);
      }
    }

    @keyframes pulse-blue {
      0% {
        transform: scale(0.95);
        box-shadow: 0 0 0 0 rgba(102, 126, 234, 0.7);
      }

      70% {
        transform: scale(1);
        box-shadow: 0 0 0 10px rgba(102, 126, 234, 0);
      }

      100% {
        transform: scale(0.95);
        box-shadow: 0 0 0 0 rgba(102, 126, 234, 0);
      }
    }

    .status-text {
      font-weight: 600;
      color: #444;
      font-size: 18px;
    }

    .connection-status {
      font-size: 12px;
      color: #666;
      display: flex;
      align-items: center;
      gap: 5px;
    }

    .status-dot {
      width: 8px;
      height: 8px;
      border-radius: 50%;
      background-color: #ccc;
    }

    .status-dot.connected {
      background-color: var(--success-color);
    }

    .status-dot.disconnected {
      background-color: var(--error-color);
    }

    .error-message {
      color: var(--error-color);
      font-size: 14px;
      background: #fff5f5;
      padding: 8px 12px;
      border-radius: 6px;
      width: 100%;
    }

    .controls {
      display: flex;
      justify-content: center;
      margin-bottom: 30px;
    }

    .btn {
      padding: 15px 40px;
      border: none;
      border-radius: 30px;
      font-size: 18px;
      font-weight: 600;
      cursor: pointer;
      transition: all 0.3s ease;
      display: flex;
      align-items: center;
      gap: 10px;
      box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);
    }

    .btn:disabled {
      opacity: 0.6;
      cursor: not-allowed;
    }

    .btn-start {
      background: var(--primary-color);
      color: white;
    }

    .btn-start:hover:not(:disabled) {
      transform: translateY(-2px);
      box-shadow: 0 6px 20px rgba(102, 126, 234, 0.4);
    }

    .btn-end {
      background: #ff4757;
      color: white;
    }

    .btn-end:hover {
      transform: translateY(-2px);
      box-shadow: 0 6px 20px rgba(255, 71, 87, 0.4);
    }

    .info {
      text-align: left;
      background: #f8f9fa;
      padding: 20px;
      border-radius: 15px;
      font-size: 14px;
      color: #555;
    }

    .info p {
      margin-bottom: 10px;
    }

    .info ul {
      list-style-position: inside;
      padding-left: 10px;
    }

    .info li {
      margin-bottom: 5px;
    }
  </style>
</head>

<body>
  <div class="app">
    <div class="container">
      <div class="header">
        <h1>üéôÔ∏è Voice AI Agent</h1>
        <p class="subtitle">Talk to Riya - Your Admissions Assistant</p>
      </div>

      <div class="status-card">
        <div class="status-indicator">
          <div id="pulse-indicator" class="pulse"></div>
          <span id="status-text" class="status-text">Ready to start</span>
        </div>

        <div id="error-container" style="display: none;" class="error-message"></div>

        <div class="connection-status">
          <span id="conn-dot" class="status-dot disconnected"></span>
          <span id="conn-text">Disconnected</span>
        </div>
      </div>

      <div class="controls">
        <button id="start-btn" class="btn btn-start">
          <span class="btn-icon">üìû</span> Start Call
        </button>
        <button id="end-btn" class="btn btn-end" style="display: none;">
          <span class="btn-icon">üì¥</span> End Call
        </button>
      </div>

      <div class="info">
        <p>üí° <strong>How it works:</strong></p>
        <ul>
          <li>Click "Start Call" to begin</li>
          <li>Speak naturally - the agent will listen</li>
          <li>Wait for Riya to respond</li>
          <li>Click "End Call" when finished</li>
        </ul>
      </div>
    </div>
  </div>

  <script>
    // --- Application State ---
    let state = {
      isConnected: false,
      isCallActive: false, // User clicked Start Call
      status: 'idle', // idle, listening, processing, speaking
      error: null
    };

    // --- DOM Elements ---
    const startBtn = document.getElementById('start-btn');
    const endBtn = document.getElementById('end-btn');
    const statusText = document.getElementById('status-text');
    const pulse = document.getElementById('pulse-indicator');
    const connDot = document.getElementById('conn-dot');
    const connText = document.getElementById('conn-text');
    const errorContainer = document.getElementById('error-container');

    // --- Audio & Logic Variables ---
    let socket = null;
    let recognition = null;
    let audioQueue = [];
    let isPlaying = false;
    let activeAudio = null;
    let currentAudioUrl = null; // Track current URL for retries
    let silenceTimeout = null;
    const SILENCE_THRESHOLD = 2000; // 2 seconds
    let audioRetryCount = 0; // Track retry attempts
    const MAX_AUDIO_RETRIES = 2; // Retry failed audio twice before skipping

    // Echo Cancellation
    let aecStream = null;
    let currentSpokenText = ""; // Text currently being spoken by bot
    let echoTimeout = null; // Timeout to clear echo text after audio finishes
    let lastAudioEndTime = 0; // Timestamp when last audio finished playing








    let vadModel = null;
    let vadContext = null;
    let vadProcessor = null;
    let vadState = null; // VAD internal state
    let vadSampleRate = 16000;
    let consecutiveSilenceFrames = 0;
    const SILENCE_FRAMES_THRESHOLD = 8; // ~256ms of silence (less sensitive)
    const SPEECH_THRESHOLD = 0.3; // Lower = easier to detect speech
    let vadEnabled = false; // Only enable VAD after speech is detected

    // --- UI Update Helper ---
    function updateUI() {
      // connection
      if (state.isConnected) {
        connDot.className = 'status-dot connected';
        connText.innerText = 'Connected';
        startBtn.disabled = false;
      } else {
        connDot.className = 'status-dot disconnected';
        connText.innerText = 'Disconnected';
        startBtn.disabled = true;
      }

      // Error
      if (state.error) {
        errorContainer.style.display = 'block';
        errorContainer.innerText = '‚ö†Ô∏è ' + state.error;
      } else {
        errorContainer.style.display = 'none';
      }

      // Call Buttons
      if (state.isCallActive) {
        startBtn.style.display = 'none';
        endBtn.style.display = 'flex';
      } else {
        startBtn.style.display = 'flex';
        endBtn.style.display = 'none';
      }

      // Status Text & Pulse
      pulse.className = 'pulse'; // reset
      if (!state.isCallActive) {
        statusText.innerText = 'Ready to start';
        state.status = 'idle';
      } else {
        // Mapping internal status to UI
        switch (state.status) {
          case 'listening':
            statusText.innerText = 'üéß Listening...';
            pulse.classList.add('listening');
            break;
          case 'processing':
            statusText.innerText = '‚öôÔ∏è Processing...';
            pulse.classList.add('processing');
            break;
          case 'speaking':
            statusText.innerText = 'üîä Agent speaking...';
            pulse.classList.add('speaking');
            break;
          default:
            statusText.innerText = 'Active';
        }
      }
    }

    // --- WebSocket Logic ---
    function connectWebSocket() {
      const protocol = window.location.protocol === "https:" ? "wss:" : "ws:";
      const host = window.location.host;
      const wsUrl = `${protocol}//${host}`;

      socket = new WebSocket(wsUrl);
      socket.binaryType = "arraybuffer";

      socket.onopen = () => {
        console.log("WS Connected");
        state.isConnected = true;
        state.error = null;
        updateUI();
      };

      socket.onclose = () => {
        console.log("WS Closed");
        state.isConnected = false;
        // Auto-reconnect after 3s
        setTimeout(connectWebSocket, 3000);
        updateUI();
      };

      socket.onerror = (err) => {
        console.error(err);
        state.isConnected = false;
        state.error = "Connection failed";
        updateUI();
      };

      socket.onmessage = (event) => {
        // IGNORE incoming messages if call is ended
        if (!state.isCallActive) return;

        if (typeof event.data === "string") {
          try {
            const parsed = JSON.parse(event.data);
            if (parsed.type === "llm_reply") {
              // Store text for echo detection
              // We append to a temporary buffer or queue if needed, 
              // but for now, simple "last received" is okay or we track per audio.
              // Ideally, we attach text to the audio URL, but let's simplify:
              // We can't easily sync text to audio blobs here without complex queueing.
              // IMPROVEMENT: For now, we will relax the check to just "isSpeaking".
              currentSpokenText = parsed.text;
            } else if (parsed.type === "llm_error" || parsed.type === "tts_error") {
              state.error = parsed.error;
              state.status = 'listening'; // Go back to listening if error
              updateUI();
            }
          } catch (e) { }
        } else {
          // Binary Audio
          const arrayBuffer = event.data;

          // Validate audio data
          if (!arrayBuffer || arrayBuffer.byteLength === 0) {
            console.error("Received empty audio data from server");
            return;
          }

          console.log(`Received audio: ${arrayBuffer.byteLength} bytes`);
          const blob = new Blob([arrayBuffer], { type: "audio/mpeg" });
          const url = URL.createObjectURL(blob);
          queueAudio(url);
        }
      };
    }

    // --- Audio Queue ---
    function queueAudio(url) {
      audioQueue.push(url);
      processQueue();
    }

    function processQueue() {
      if (isPlaying || audioQueue.length === 0) return;

      isPlaying = true;
      state.status = 'speaking';
      updateUI();

      const url = audioQueue.shift();
      currentAudioUrl = url; // Store for correct retry
      activeAudio = new Audio(url);

      activeAudio.onended = () => {
        isPlaying = false;
        lastAudioEndTime = Date.now(); // Mark end time for tail protection
        activeAudio = null;
        // If more audio, keep speaking, else back to listening
        if (audioQueue.length > 0) {
          processQueue();
        } else {
          state.status = 'listening';
          // Clear echo text after 6 seconds (protects against tail-end echoes)
          clearTimeout(echoTimeout);
          echoTimeout = setTimeout(() => {
            currentSpokenText = "";
            console.log("Echo protection cleared");
          }, 6000);
          updateUI();
        }
        audioRetryCount = 0; // Reset retry counter on success
      };

      activeAudio.onerror = (event) => {
        const err = event.error || event;
        // Capture properties safely from the event target if possible, or fall back
        const targetAudio = event.target;

        console.error("Audio playback error:", {
          error: err,
          // Use target if available, or fallbacks
          src: targetAudio?.src || currentAudioUrl || 'unknown',
          readyState: targetAudio?.readyState,
          networkState: targetAudio?.networkState,
          errorCode: targetAudio?.error?.code,
          errorMessage: targetAudio?.error?.message,
          retryCount: audioRetryCount
        });

        // Retry logic: try playing again before giving up
        if (audioRetryCount < MAX_AUDIO_RETRIES && currentAudioUrl) {
          audioRetryCount++;
          console.log(`Retrying audio playback (attempt ${audioRetryCount}/${MAX_AUDIO_RETRIES})...`);

          // Cleanup current failed audio
          if (activeAudio) {
            activeAudio.pause();
            activeAudio = null;
          }
          isPlaying = false;

          // Re-queue the SAME URL at the front of the queue
          audioQueue.unshift(currentAudioUrl);

          // Retry after brief delay
          setTimeout(() => {
            processQueue();
          }, 300);
        } else {
          // Max retries reached, skip to next audio
          console.warn("Max retries reached, skipping to next audio");
          audioRetryCount = 0;
          isPlaying = false;
          activeAudio = null;
          currentAudioUrl = null; // Clear current URL

          state.status = 'listening';
          updateUI();
          // Continue with next audio in queue
          processQueue();
        }
      };

      activeAudio.play().catch(e => {
        if (e.name === 'AbortError') return; // Ignore intentional stops
        console.error("Autoplay failed", e);
        state.error = "Audio blocked. Click page.";
        updateUI();
      });
    }

    function cancelAudio() {
      if (activeAudio) {
        // PREVENT RETRY LOOP: Remove handlers before stopping
        activeAudio.onerror = null;
        activeAudio.onended = null;

        activeAudio.pause();
        activeAudio.currentTime = 0; // Reset to beginning
        activeAudio.src = ''; // Clear source to fully stop
        activeAudio = null;
      }
      audioQueue = [];
      currentAudioUrl = null; // Clear current URL to prevent retry
      isPlaying = false;

      // ENSURE ECHO PROTECTION CLEARS EVENTUALLY
      // Since onended isn't called, we must manually set the cleanup timer here.
      // Otherwise, currentSpokenText remains stuck forever until next audio plays.
      clearTimeout(echoTimeout);
      echoTimeout = setTimeout(() => {
        currentSpokenText = "";
        console.log("Echo protection cleared (after cancel)");
      }, 6000);

      lastAudioEndTime = Date.now(); // Also mark end time on cancel to prevent immediate echo leakage
      state.status = 'listening';
      updateUI();
    }

    // --- Web Speech API Logic ---
    async function startCall() {
      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
      if (!SpeechRecognition) {
        state.error = "Browser not supported (Use Chrome)";
        updateUI();
        return;
      }

      // 1. AEC Hack: Request mic with echoCancellation to force Hardware AEC
      try {
        aecStream = await navigator.mediaDevices.getUserMedia({
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true
          }
        });
        // We don't play this stream, just hold it open.
        console.log("AEC Stream active");

        // Setup VAD audio processing if model is loaded
        if (vadModel) {
          await setupVADAudioProcessing();
        }
      } catch (err) {
        console.warn("Could not get AEC stream", err);
      }

      state.isCallActive = true;
      state.status = 'listening';
      updateUI();

      if (!recognition) {
        recognition = new SpeechRecognition();
        recognition.continuous = true;
        recognition.interimResults = true;
        recognition.lang = "en-US";

        recognition.onstart = () => {
          console.log("Recognition started");
          state.status = 'listening';
          updateUI();
        };

        recognition.onerror = (event) => {
          // SILENCE EXPECTED ERRORS: 'no-speech' is normal when user is thinking
          if (event.error === 'no-speech') {
            // Do NOT restart here. 'onend' will fire next and handle the restart centrally.
            // This prevents "double-start" race conditions.
            return;
          }

          console.warn("Recognition error:", event.error); // Warn for real errors only


          if (event.error === 'network') {
            // Network error - restart recognition after a brief delay
            console.log("Network error detected, restarting recognition...");
            if (state.isCallActive) {
              setTimeout(() => {
                try {
                  recognition.stop(); // Stop first to clean up
                  setTimeout(() => {
                    if (state.isCallActive) {
                      recognition.start();
                    }
                  }, 100);
                } catch (e) {
                  console.error("Failed to restart recognition:", e);
                }
              }, 500);
            }
            return;
          }

          // For other errors (audio-capture, not-allowed, etc.), log but don't crash
          if (event.error === 'aborted') return; // Normal stop

          // Critical errors - show to user
          if (event.error === 'not-allowed' || event.error === 'service-not-allowed') {
            state.error = "Microphone permission denied";
            state.isCallActive = false;
            updateUI();
          }
        };

        recognition.onend = () => {
          console.log("Recognition ended");
          // Auto-restart if call is active
          if (state.isCallActive) {
            setTimeout(() => {
              try { recognition.start(); } catch (e) { }
            }, 200);
          }
        };

        recognition.onresult = (event) => {
          clearTimeout(silenceTimeout);

          let finalTranscript = "";
          let interimTranscript = "";

          for (let i = event.resultIndex; i < event.results.length; ++i) {
            if (event.results[i].isFinal) {
              finalTranscript += event.results[i][0].transcript;
            } else {
              interimTranscript += event.results[i][0].transcript;
            }
          }

          let fullTranscript = (finalTranscript + interimTranscript).trim().toLowerCase();

          // --- ECHO DETECTION (AGGRESSIVE) ---
          // Check if we recently spoke (even if audio finished)
          if (currentSpokenText) {
            const textToMatch = currentSpokenText.toLowerCase().trim();
            const cleanInput = fullTranscript.toLowerCase().trim();

            // 1. Direct Inclusion (State-Aware)
            const inputWordCount = cleanInput.split(' ').length;

            // Tail Protection: Extend strict checks for 800ms after audio ends
            // This catches acoustic echoes that arrive just as the bot finishes.
            const isTailPeriod = (Date.now() - lastAudioEndTime) < 800;

            // If bot is CURRENTLY speaking OR in the echo tail period, be AGGRESSIVE
            // If completely finished (isPlaying=false AND !isTailPeriod), be SMART
            const shouldRelaxCheck = !isPlaying && !isTailPeriod && inputWordCount < 4;

            if (!shouldRelaxCheck) {
              // SUSTAIN PROTECTION: If we are actively blocking echoes, keeps the shield up!
              // This prevents the protection from expiring mid-echo for long latency echoes.
              if (echoTimeout) {
                clearTimeout(echoTimeout);
                echoTimeout = setTimeout(() => {
                  currentSpokenText = "";
                  console.log("Echo protection cleared (sustained)");
                }, 6000);
              }

              // Checking for "Included" Echoes
              if (cleanInput.includes(textToMatch)) {
                // MIXED INPUT DETECTION:
                // If the transcript contains the echo PLUS other text, it's likely user speaking over the bot.
                // We should STRIP the echo and process the rest.
                const strippedInput = cleanInput.replace(textToMatch, "").trim();

                if (strippedInput.length > 2) {
                  console.log(`>> Mixed Input Detected (Echo + User). Stripping echo...`);
                  console.log(`   Original: "${cleanInput}"`);
                  console.log(`   Stripped: "${strippedInput}"`);

                  // CRITICAL: Update the transcript variables so downstream logic uses only the user's part
                  // We can't easily split final vs interim perfectly, so we'll just update the derived fullTranscript 
                  // and assume finalTranscript takes the hit for sending.
                  // A simple heuristic: Replace the echo in the finalTranscript if present.
                  finalTranscript = finalTranscript.toLowerCase().replace(textToMatch, "").trim();
                  interimTranscript = interimTranscript.toLowerCase().replace(textToMatch, "").trim();

                  // Continue processing with the stripped input
                } else {
                  // It's just the echo (or negligible noise) effectively
                  console.log(`>> Echo BLOCKED (Exact match, active=${isPlaying}):`, cleanInput);
                  return;
                }
              } else if (textToMatch.includes(cleanInput)) {
                // The input is fully inside the bot's speech (pure echo)
                console.log(`>> Echo BLOCKED (Substring, active=${isPlaying}):`, cleanInput);
                return;
              }
            }

            // 2. Fuzzy Match (Levenshtein) - DYNAMIC THRESHOLD
            if (cleanInput.length > 3) {
              const distance = levenshtein(cleanInput, textToMatch);
              const maxLen = Math.max(cleanInput.length, textToMatch.length);
              const similarity = 1 - (distance / maxLen);

              // If playing, use stricter threshold (0.6) even for short inputs to prevent self-interruption
              // If not playing, use relaxed threshold (0.8) for short inputs (allow answers)
              let threshold = 0.6; // Standard
              if (!isPlaying && cleanInput.length < 10) {
                threshold = 0.8; // Relaxed for short answers
              }

              if (similarity > threshold) {
                console.log(`>> Echo BLOCKED (Fuzzy ${Math.round(similarity * 100)}% > ${threshold * 100}%):`, cleanInput);
                return;
              }

              // 3. WORD OVERLAP CHECK (Bag-of-Words) - Phase 9
              // Catches "partial" echoes that fail strict substring (e.g. "please tell me your 10 digit" vs "10 digit number")
              // Only apply if input has enough words (> 3) to be statistically significant
              if (cleanInput.split(' ').length > 3) {
                // Remove punctuation for cleaner word matching
                const normalize = (s) => s.replace(/[^\w\s]/g, '').toLowerCase();
                const inputWords = normalize(cleanInput).split(/\s+/).filter(w => w.length > 0);
                const referenceWords = new Set(normalize(textToMatch).split(/\s+/).filter(w => w.length > 0));

                let matchCount = 0;
                for (const word of inputWords) {
                  if (referenceWords.has(word)) matchCount++;
                }

                const overlapRatio = matchCount / inputWords.length;
                // If > 70% of user's words are in the bot's text, it's likely an echo
                if (overlapRatio > 0.7) {
                  console.log(`>> Echo BLOCKED (Word Overlap ${Math.round(overlapRatio * 100)}%):`, cleanInput);
                  return;
                }
              }
            }
          }

          // Enable VAD once we detect speech
          if (interimTranscript && !vadEnabled) {
            vadEnabled = true;
            console.log("VAD: Enabled after speech detection");
          }

          // Barge-in
          if (finalTranscript || interimTranscript) {
            if (isPlaying || audioQueue.length > 0) {
              cancelAudio();
            }
          }

          // Silence Detection: VAD (fast) or Timer (fallback)
          if (finalTranscript || interimTranscript) {
            if (!vadModel) {
              // Fallback: Use 2-second timer if VAD didn't load
              clearTimeout(silenceTimeout);
              silenceTimeout = setTimeout(() => {
                console.log("Fallback timer: Silence detected -> Stop & Send");
                recognition.stop();
                state.status = 'processing';
                updateUI();
              }, SILENCE_THRESHOLD);
            }
            // If VAD is loaded, it handles silence detection automatically
          }

          if (finalTranscript) {
            console.log("Sending:", finalTranscript);
            if (socket && socket.readyState === WebSocket.OPEN) {
              socket.send(JSON.stringify({ type: 'user_text', text: finalTranscript }));
            }
          }
        };
      }

      try { recognition.start(); } catch (e) { }
    }

    function stopCall() {
      state.isCallActive = false;
      // Logic to stop everything
      if (recognition) {
        recognition.stop();
      }
      // Stop AEC Stream
      if (aecStream) {
        aecStream.getTracks().forEach(track => track.stop());
        aecStream = null;
      }
      // Stop VAD Processing
      if (vadProcessor) {
        vadProcessor.disconnect();
        vadProcessor = null;
      }
      if (vadContext) {
        vadContext.close();
        vadContext = null;
      }
      consecutiveSilenceFrames = 0; // Reset VAD state
      vadEnabled = false; // Reset VAD activation
      cancelAudio();
      currentSpokenText = ""; // Clear echo protection on call end
      clearTimeout(echoTimeout); // Clear any pending echo timeout
      state.status = 'idle';
      updateUI();
    }

    // --- Event Listeners ---
    startBtn.addEventListener('click', startCall);
    endBtn.addEventListener('click', stopCall);

    // --- Silero VAD Functions ---
    async function initVAD() {
      try {
        console.log("Loading Silero VAD model...");
        vadModel = await ort.InferenceSession.create(
          "https://huggingface.co/onnx-community/silero-vad/resolve/main/onnx/model.onnx"
        );

        // Initialize VAD state for Silero VAD v5 - shape: (2, 1, 128)
        const h = new ort.Tensor("float32", new Float32Array(2 * 1 * 128).fill(0), [2, 1, 128]);
        vadState = { h };

        console.log("Silero VAD loaded successfully");
      } catch (err) {
        console.warn("Could not load VAD model, falling back to timer:", err);
      }
    }

    async function processAudioForVAD(audioData) {
      if (!vadModel || !state.isCallActive) return;

      try {
        // Prepare input tensor (512 samples at 16kHz)
        const audioTensor = new ort.Tensor("float32", audioData, [1, audioData.length]);
        const sr = new ort.Tensor("int64", BigInt64Array.from([BigInt(vadSampleRate)]));

        // Hugging Face model expects 'state' instead of separate h/c
        const inputs = {
          input: audioTensor,
          state: vadState.h, // Use h as state (model will handle internally)
          sr: sr
        };

        const output = await vadModel.run(inputs);
        const speechProb = output.output.data[0]; // 0.0 to 1.0

        // Update state for next iteration (if model returns stateN)
        if (output.stateN) {
          vadState.h = output.stateN;
        }

        // Detect silence (only if VAD is enabled after speech detection)
        if (speechProb < SPEECH_THRESHOLD) {
          consecutiveSilenceFrames++;
          if (consecutiveSilenceFrames >= SILENCE_FRAMES_THRESHOLD && vadEnabled) {
            // User stopped speaking!
            console.log("VAD: Silence detected, stopping recognition");
            if (recognition && state.isCallActive) {
              recognition.stop();
              state.status = 'processing';
              updateUI();
              vadEnabled = false; // Reset for next utterance
            }
            consecutiveSilenceFrames = 0; // Reset
          }
        } else {
          consecutiveSilenceFrames = 0; // Reset on speech
        }
      } catch (err) {
        console.error("VAD processing error:", err);
      }
    }

    async function setupVADAudioProcessing() {
      if (!aecStream) return;

      try {
        vadContext = new AudioContext({ sampleRate: vadSampleRate });
        const source = vadContext.createMediaStreamSource(aecStream);

        // Create ScriptProcessor for audio chunks
        vadProcessor = vadContext.createScriptProcessor(512, 1, 1);

        vadProcessor.onaudioprocess = (e) => {
          if (!state.isCallActive) return;

          const inputData = e.inputBuffer.getChannelData(0);
          processAudioForVAD(inputData);
        };

        source.connect(vadProcessor);
        vadProcessor.connect(vadContext.destination);

        console.log("VAD audio processing started");
      } catch (err) {
        console.error("Could not setup VAD audio processing:", err);
      }
    }

    // Auto-init
    window.onload = async () => {
      connectWebSocket();
      await initVAD(); // Load VAD model on page load
    };

    // --- Utils ---
    function levenshtein(a, b) {
      const matrix = [];
      for (let i = 0; i <= b.length; i++) { matrix[i] = [i]; }
      for (let j = 0; j <= a.length; j++) { matrix[0][j] = j; }
      for (let i = 1; i <= b.length; i++) {
        for (let j = 1; j <= a.length; j++) {
          if (b.charAt(i - 1) === a.charAt(j - 1)) {
            matrix[i][j] = matrix[i - 1][j - 1];
          } else {
            matrix[i][j] = Math.min(
              matrix[i - 1][j - 1] + 1,
              matrix[i][j - 1] + 1,
              matrix[i - 1][j] + 1
            );
          }
        }
      }
      return matrix[b.length][a.length];
    }
  </script>
</body>

</html>
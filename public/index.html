<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Voice AI Agent</title>
  <link rel="icon" type="image/svg+xml" href="/favicon.svg">
  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.19.0/dist/ort.min.js"></script>
  <style>
    :root {
      --primary-color: #667eea;
      --secondary-color: #764ba2;
      --bg-gradient: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      --glass-bg: rgba(255, 255, 255, 0.95);
      --text-color: #333;
      --error-color: #dc3545;
      --success-color: #28a745;
    }

    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    body {
      font-family: 'Segoe UI', Roboto, Helvetica, Arial, sans-serif;
      background: var(--bg-gradient);
      min-height: 100vh;
      display: flex;
      justify-content: center;
      align-items: center;
      color: var(--text-color);
    }

    .app {
      width: 100%;
      padding: 20px;
      display: flex;
      justify-content: center;
    }

    .container {
      background: var(--glass-bg);
      backdrop-filter: blur(10px);
      border-radius: 20px;
      padding: 40px;
      width: 100%;
      max-width: 500px;
      box-shadow: 0 20px 60px rgba(0, 0, 0, 0.3);
      text-align: center;
    }

    .header {
      margin-bottom: 30px;
    }

    .header h1 {
      font-size: 28px;
      margin-bottom: 10px;
      color: #333;
    }

    .subtitle {
      color: #666;
      font-size: 16px;
    }

    .status-card {
      background: #f8f9fa;
      border-radius: 15px;
      padding: 20px;
      margin-bottom: 30px;
      border: 1px solid #e9ecef;
      display: flex;
      flex-direction: column;
      align-items: center;
      gap: 15px;
    }

    .status-indicator {
      display: flex;
      flex-direction: column;
      align-items: center;
      gap: 10px;
    }

    .pulse {
      width: 15px;
      height: 15px;
      border-radius: 50%;
      background-color: #ccc;
      transition: all 0.3s ease;
    }

    .pulse.listening {
      background-color: var(--error-color);
      box-shadow: 0 0 0 0 rgba(220, 53, 69, 0.7);
      animation: pulse-red 1.5s infinite;
    }

    .pulse.speaking {
      background-color: var(--success-color);
      box-shadow: 0 0 0 0 rgba(40, 167, 69, 0.7);
      animation: pulse-green 1.5s infinite;
    }

    .pulse.processing {
      background-color: var(--primary-color);
      animation: spin 1s infinite linear;
      /* Make it look like a spinner for processing if desired, or just pulse blue */
      animation: pulse-blue 1.5s infinite;
    }

    @keyframes pulse-red {
      0% {
        transform: scale(0.95);
        box-shadow: 0 0 0 0 rgba(220, 53, 69, 0.7);
      }

      70% {
        transform: scale(1);
        box-shadow: 0 0 0 10px rgba(220, 53, 69, 0);
      }

      100% {
        transform: scale(0.95);
        box-shadow: 0 0 0 0 rgba(220, 53, 69, 0);
      }
    }

    @keyframes pulse-green {
      0% {
        transform: scale(0.95);
        box-shadow: 0 0 0 0 rgba(40, 167, 69, 0.7);
      }

      70% {
        transform: scale(1);
        box-shadow: 0 0 0 10px rgba(40, 167, 69, 0);
      }

      100% {
        transform: scale(0.95);
        box-shadow: 0 0 0 0 rgba(40, 167, 69, 0);
      }
    }

    @keyframes pulse-blue {
      0% {
        transform: scale(0.95);
        box-shadow: 0 0 0 0 rgba(102, 126, 234, 0.7);
      }

      70% {
        transform: scale(1);
        box-shadow: 0 0 0 10px rgba(102, 126, 234, 0);
      }

      100% {
        transform: scale(0.95);
        box-shadow: 0 0 0 0 rgba(102, 126, 234, 0);
      }
    }

    .status-text {
      font-weight: 600;
      color: #444;
      font-size: 18px;
    }

    .connection-status {
      font-size: 12px;
      color: #666;
      display: flex;
      align-items: center;
      gap: 5px;
    }

    .status-dot {
      width: 8px;
      height: 8px;
      border-radius: 50%;
      background-color: #ccc;
    }

    .status-dot.connected {
      background-color: var(--success-color);
    }

    .status-dot.disconnected {
      background-color: var(--error-color);
    }

    .error-message {
      color: var(--error-color);
      font-size: 14px;
      background: #fff5f5;
      padding: 8px 12px;
      border-radius: 6px;
      width: 100%;
    }

    .controls {
      display: flex;
      justify-content: center;
      margin-bottom: 30px;
    }

    .btn {
      padding: 15px 40px;
      border: none;
      border-radius: 30px;
      font-size: 18px;
      font-weight: 600;
      cursor: pointer;
      transition: all 0.3s ease;
      display: flex;
      align-items: center;
      gap: 10px;
      box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);
    }

    .btn:disabled {
      opacity: 0.6;
      cursor: not-allowed;
    }

    .btn-start {
      background: var(--primary-color);
      color: white;
    }

    .btn-start:hover:not(:disabled) {
      transform: translateY(-2px);
      box-shadow: 0 6px 20px rgba(102, 126, 234, 0.4);
    }

    .btn-end {
      background: #ff4757;
      color: white;
    }

    .btn-end:hover {
      transform: translateY(-2px);
      box-shadow: 0 6px 20px rgba(255, 71, 87, 0.4);
    }

    .info {
      text-align: left;
      background: #f8f9fa;
      padding: 20px;
      border-radius: 15px;
      font-size: 14px;
      color: #555;
    }

    .info p {
      margin-bottom: 10px;
    }

    .info ul {
      list-style-position: inside;
      padding-left: 10px;
    }

    .info li {
      margin-bottom: 5px;
    }
  </style>
</head>

<body>
  <div class="app">
    <div class="container">
      <div class="header">
        <h1>üéôÔ∏è Voice AI Agent</h1>
        <p class="subtitle">Talk to Riya - Your Admissions Assistant</p>
      </div>

      <div class="status-card">
        <div class="status-indicator">
          <div id="pulse-indicator" class="pulse"></div>
          <span id="status-text" class="status-text">Ready to start</span>
        </div>

        <div id="error-container" style="display: none;" class="error-message"></div>

        <div class="connection-status">
          <span id="conn-dot" class="status-dot disconnected"></span>
          <span id="conn-text">Disconnected</span>
        </div>
      </div>

      <div class="controls">
        <button id="start-btn" class="btn btn-start">
          <span class="btn-icon">üìû</span> Start Call
        </button>
        <button id="end-btn" class="btn btn-end" style="display: none;">
          <span class="btn-icon">üì¥</span> End Call
        </button>
        <button id="reset-speaker-btn" class="btn btn-reset" style="display: none;">
          <span class="btn-icon">üîÑ</span> Reset Speaker
        </button>
      </div>

      <div class="info">
        <p>üí° <strong>How it works:</strong></p>
        <ul>
          <li>Click "Start Call" to begin</li>
          <li>Speak naturally - the agent will listen</li>
          <li>Wait for Riya to respond</li>
          <li>Click "End Call" when finished</li>
        </ul>
      </div>
    </div>
  </div>

  <script>
    // --- Application State ---
    let state = {
      isConnected: false,
      isCallActive: false, // User clicked Start Call
      status: 'idle', // idle, listening, processing, speaking
      error: null,
      currentVadProb: 0, // Real-time VAD probability (0.0 - 1.0)
      peakVadProb: 0, // Peak VAD probability during current speech segment (for wake phrase detection)
      reAuthMode: false, // Re-authentication mode (enables wake phrase detection)
      waitingForWakePhrase: false, // Whether we're waiting for wake phrase (call active but not started)
      wakePhraseDetected: false, // Whether wake phrase was detected
      confirmationStep: false, // Whether we're in confirmation step
      wakePhraseVoiceFeatures: null, // Voice features from wake phrase (for consistency check)
      collectedName: null, // Name collected during confirmation step
      pendingIntentCheck: false, // Whether an intent check is currently in progress (prevents concurrent checks)
      pendingWakePhraseQueue: [], // Queue of wake phrase transcripts blocked by pendingIntentCheck (prevents silent drops)
      mismatchCount: 0, // Consecutive mismatch count (for hysteresis)
      lastMismatchTime: null, // Timestamp of last mismatch (for wrong lock detection)
      wrongLockDetected: false, // Whether wrong lock was detected
      endingPhase: false, // Whether we're in ending phase
      lastAgentSpeechEndTime: 0, // Timestamp when agent speech ended (for echo-aware windows)
      isRestartingRecognition: false, // Flag to prevent stopCall() during recognition restart (race condition protection)
      recognitionIsActive: false, // Manual tracking of recognition state (more reliable than recognition.state)
      lastRecognitionEndTime: 0, // Timestamp when recognition last ended (for health check)
      bargeInVadCount: 0, // Counter for VAD-based barge-in detection (consecutive high-VAD frames)
      // Proactive follow-up tracking
      lastAgentQuestion: null, // { text: "...", timestamp: Date.now(), questionId: "..." }
      followUpStage: 0, // 0 = no follow-up, 1 = asked "Are you there?", 2 = moving on
      followUpAlternate: false, // Alternate between "Are you there?" and "Did you hear me?"
      lastUserResponseTime: null, // Timestamp when user last responded (for follow-up detection and thinking pause detection)
      // Explicit field tracking for accurate summary generation
      collectedFields: {
        name: null,
        phone: null,
        course: null,
        education: null,
        intakeYear: null,
        city: null,
        budget: null
      },
      speakerProfile: {
        baselineVolume: null, // Average RMS volume of primary speaker
        volumeHistory: [],    // Sliding window of volume samples
        calibrationComplete: false,
        frameCount: 0,
        recentMaxRMS: 0.0,    // Peak volume in recent frames (decaying)
        // Voice ID Lite Stats
        pitchHistory: [],     // Fundamental Frequency (Hz)
        timbreHistory: [],    // Spectral Centroid (Hz) - now using proper FFT-based
        baselinePitch: null,
        baselineTimbre: null,
        pitchVariance: null,      // Pitch variance for robust matching
        timbreVariance: null,     // Timbre variance for robust matching
        voiceprint: null,          // Complete voiceprint signature
        // Enhanced features
        spectralCentroidHistory: [], // Proper FFT-based spectral centroid
        mfccHistory: [],             // MFCC coefficients (13 features)
        mfccDeltaHistory: [],         // Delta coefficients (temporal features)
        mfccDeltaDeltaHistory: [],    // Delta-Delta coefficients (acceleration features)
        mfccNormalizedHistory: [],   // CMN/CVN normalized MFCC
        mfccMean: null,              // Cepstral mean for CMN
        mfccVariance: null,          // Cepstral variance for CVN
        speakerId: null,             // Unique speaker identifier
        // Contextual awareness for voice matching
        conversationContext: {
          isAnsweringQuestion: false,  // User is answering agent's question
          lastQuestionTime: 0,         // Timestamp of last agent question
          topicRelevance: 1.0,        // Topic relevance score (0.0-1.0)
          responseTiming: 0,          // Time since last agent speech
          confidenceHistory: [],      // History of confidence scores
          adaptiveThreshold: 0.3       // Current adaptive threshold
        },
        // Multi-speaker support
        knownSpeakers: new Map(),    // Map of speakerId -> voiceprint
        currentSpeakerId: null,      // Currently active speaker
        lastAdaptationTime: null,    // Track when we last logged ID Card (prevent spam)
        // Main speaker identification
        firstSpeakerDetected: false,  // Whether we've detected the first speaker
        firstSpeakerVolume: null,     // Volume of first speaker (for comparison)
        firstSpeakerPitch: null,      // Pitch of first speaker
        firstSpeakerTimbre: null,     // Timbre of first speaker
        callInitiatorDetected: false, // Whether greeting words detected (hello, hi)
        calibrationStartTime: null,   // When calibration started
        // Volume-weighted calibration (prioritize louder speakers)
        weightedVolumeHistory: [],    // Volume-weighted samples
        weightedPitchHistory: [],     // Volume-weighted pitch samples
        weightedTimbreHistory: [],     // Volume-weighted timbre samples
        // Session drift handling
        anchorProfile: null,          // Locked anchor profile (never changes)
        sessionProfile: null          // Adaptive session profile (slowly changes)
      }
    };

    // --- Configuration Flags ---
    // Master feature flag: Set to false to disable all speaker filtering (instant disable)
    // When enabled, provides multi-layer protection: volume filtering, keyword filtering, and voice ID
    const ENABLE_SPEAKER_FILTERING = true;
    
    // Sub-features (only active when ENABLE_SPEAKER_FILTERING is true)
    const ENABLE_VOLUME_FILTERING = true;   // Volume-based outlier detection
    const ENABLE_KEYWORD_FILTERING = true;  // Background noise keyword detection
    const VOLUME_OUTLIER_THRESHOLD_HIGH = 3.0; // Relaxed high threshold (3x)
    const VOLUME_OUTLIER_THRESHOLD_LOW = 0.1; // LESS AGGRESSIVE: More lenient low threshold (0.1x) to prevent false rejections of main speaker speaking quietly
    const BASELINE_CALIBRATION_FRAMES = 100;   // ~3-4 seconds at 30ms/frame

    // --- DOM Elements ---
    const startBtn = document.getElementById('start-btn');
    const endBtn = document.getElementById('end-btn');
    const resetSpeakerBtn = document.getElementById('reset-speaker-btn');
    const statusText = document.getElementById('status-text');
    const pulse = document.getElementById('pulse-indicator');
    const connDot = document.getElementById('conn-dot');
    const connText = document.getElementById('conn-text');
    const errorContainer = document.getElementById('error-container');

    // --- Audio & Logic Variables ---
    let socket = null;
    let recognition = null;
    let audioQueue = [];
    let isPlaying = false;
    let activeAudio = null;
    let currentAudioUrl = null; // Track current URL for retries
    let silenceTimeout = null;
    const SILENCE_THRESHOLD = 2000; // 2 seconds
    let audioRetryCount = 0; // Track retry attempts
    const MAX_AUDIO_RETRIES = 2; // Retry failed audio twice before skipping

    // Echo Cancellation
    let aecStream = null;
    let currentSpokenText = ""; // Text currently being spoken by bot
    let echoTimeout = null; // Timeout to clear echo text after audio finishes
    let lastAudioEndTime = 0; // Timestamp when last audio finished playing
    let lastSpokenTextForPause = ""; // Track last spoken text to detect pause needs








    let vadModel = null;
    let vadContext = null;
    let vadProcessor = null;
    let vadState = null; // VAD internal state
    let vadSampleRate = 16000;
    let consecutiveSilenceFrames = 0;
    const SILENCE_FRAMES_THRESHOLD = 8; // ~256ms of silence (less sensitive)
    const SPEECH_THRESHOLD = 0.3; // Lower = easier to detect speech
    let vadEnabled = false; // Only enable VAD after speech is detected

    // --- UI Update Helper ---
    function updateUI() {
      // connection
      if (state.isConnected) {
        connDot.className = 'status-dot connected';
        connText.innerText = 'Connected';
        startBtn.disabled = false;
      } else {
        connDot.className = 'status-dot disconnected';
        connText.innerText = 'Disconnected';
        startBtn.disabled = true;
      }

      // Error
      if (state.error) {
        errorContainer.style.display = 'block';
        errorContainer.innerText = '‚ö†Ô∏è ' + state.error;
      } else {
        errorContainer.style.display = 'none';
      }

      // Call Buttons
      if (state.isCallActive) {
        startBtn.style.display = 'none';
        endBtn.style.display = 'flex';
        // Show reset speaker button if wrong lock detected
        if (state.wrongLockDetected) {
          resetSpeakerBtn.style.display = 'flex';
        } else {
          resetSpeakerBtn.style.display = 'none';
        }
      } else {
        startBtn.style.display = 'flex';
        endBtn.style.display = 'none';
        resetSpeakerBtn.style.display = 'none';
      }

      // Status Text & Pulse
      pulse.className = 'pulse'; // reset
      if (!state.isCallActive) {
        statusText.innerText = 'Ready to start';
        state.status = 'idle';
      } else {
        // Mapping internal status to UI
        switch (state.status) {
          case 'listening':
            statusText.innerText = 'üéß Listening...';
            pulse.classList.add('listening');
            break;
          case 'processing':
            statusText.innerText = '‚öôÔ∏è Processing...';
            pulse.classList.add('processing');
            break;
          case 'speaking':
            statusText.innerText = 'üîä Agent speaking...';
            pulse.classList.add('speaking');
            break;
          default:
            statusText.innerText = 'Active';
        }
      }
    }

    // --- WebSocket Logic ---
    function connectWebSocket() {
      const protocol = window.location.protocol === "https:" ? "wss:" : "ws:";
      const host = window.location.host;

      // Generate or retrieve persistent Session ID
      let sessionId = localStorage.getItem('voice_agent_session_id');
      if (!sessionId) {
        sessionId = 'sess_' + Math.random().toString(36).substring(2, 15);
        localStorage.setItem('voice_agent_session_id', sessionId);
      }

      const wsUrl = `${protocol}//${host}?sessionId=${sessionId}`;

      socket = new WebSocket(wsUrl);
      socket.binaryType = "arraybuffer";

      socket.onopen = () => {
        console.log("WS Connected");
        state.isConnected = true;
        state.error = null;
        updateUI();
      };

      socket.onclose = () => {
        console.log("WS Closed");
        state.isConnected = false;
        // Auto-reconnect after 3s
        setTimeout(connectWebSocket, 3000);
        updateUI();
      };

      socket.onerror = (err) => {
        console.error(err);
        state.isConnected = false;
        state.error = "Connection failed";
        updateUI();
      };

      socket.onmessage = (event) => {
        // IGNORE incoming messages if call is ended (unless re-auth mode)
        if (!state.isCallActive && !state.reAuthMode) return;

        if (typeof event.data === "string") {
          try {
            const parsed = JSON.parse(event.data);

            // Handle valid text chunks (Accumulate for echo detection)
            if (parsed.type === "text_start" || parsed.type === "text_chunk" || parsed.type === "llm_reply") {
              const text = parsed.text || " ";
              const trimmedText = text.trim();
              
              // FIXED: Don't accumulate follow-up messages or rejection feedback in currentSpokenText
              // Follow-up messages ("Are you there?", "Did you hear me?") should NOT be part of conversation context
              // Rejection feedback ("I didn't catch that...") should also NOT be tracked as questions
              const isFollowUpMessage = trimmedText === "Are you there?" || trimmedText === "Did you hear me?";
              const isRejectionFeedback = trimmedText.includes("I didn't catch that") || 
                                         trimmedText.includes("didn't catch") ||
                                         trimmedText.includes("samajh nahi payi") ||
                                         trimmedText.includes("phir se");
              
              if (!isFollowUpMessage && !isRejectionFeedback) {
                // Only accumulate non-follow-up and non-rejection messages
                currentSpokenText = (currentSpokenText + " " + text).trim();
                // Track last spoken text for pause calculation
                lastSpokenTextForPause = currentSpokenText;
                console.log(`[DEBUG] Accumuated SpokenText: "${currentSpokenText}"`);
              } else {
                // Follow-up or rejection message - don't accumulate, just log
                // FIXED: Still update lastSpokenTextForPause for pause calculation
                // This ensures pause timing is correct even after follow-up/rejection messages
                lastSpokenTextForPause = trimmedText;
                if (isFollowUpMessage) {
                  console.log(`[FOLLOW-UP] Follow-up message detected, NOT accumulating: "${trimmedText}"`);
                } else {
                  console.log(`[REJECTION] Rejection feedback detected, NOT accumulating: "${trimmedText}"`);
                }
              }
              
              // FIXED: Track agent questions for proactive follow-up
              // Detect if this is a question (ends with ?) and set lastAgentQuestion
              // BUT: Skip follow-up messages and rejection feedback - these are not real questions
              if (trimmedText.endsWith('?') && !isFollowUpMessage && !isRejectionFeedback) {
                // This is a real question - start tracking
                state.lastAgentQuestion = {
                  text: currentSpokenText,
                  timestamp: Date.now(),
                  questionId: 'q_' + Date.now()
                };
                state.followUpStage = 0; // Reset follow-up stage
                
                // ENHANCED: Update conversation context for adaptive voice matching
                state.speakerProfile.conversationContext.lastQuestionTime = Date.now();
                state.speakerProfile.conversationContext.isAnsweringQuestion = false; // Reset - user hasn't answered yet
                
                console.log(`[FOLLOW-UP] Agent asked question: "${currentSpokenText}"`);
              }
            }
            // Handle wake phrase intent check result
            // FIXED: Don't set wakePhraseDetected flag here - it should only be set after clear intent verification
            // The wake phrase detection flow is: fuzzy/LLM intent ‚Üí clear intent ‚Üí handleWakePhraseDetected()
            // Setting the flag here causes premature state transition before clear intent verification completes
            else if (parsed.type === "wake_phrase_intent_result") {
              // This result is handled in the promise chain in checkWakePhraseIntent()
              // Do not set wakePhraseDetected flag here - it will be set by handleWakePhraseDetected() after clear intent passes
              if (parsed.isWakePhrase && isWakePhraseDetectionActive()) {
                console.log(`>> Wake phrase intent detected (LLM): "${parsed.text}" - waiting for clear intent verification`);
                // The flag will be set by handleWakePhraseDetected() after clear intent verification completes
              }
            }
            // Handle ending intent check result
            else if (parsed.type === "ending_intent_result") {
              if (parsed.isEndingIntent) {
                state.endingPhase = true;
                console.log(`>> Ending phase detected`);
                // FIXED: Reset follow-up timer when ending phase is detected
                // This prevents follow-ups from triggering during ending
                if (state.lastAgentQuestion) {
                  state.lastAgentQuestion = null;
                  state.followUpStage = 0;
                  console.log(`[ENDING] Reset follow-up timer - ending phase active`);
                }
              } else {
                // FIXED: Reset flag if server rejects ending intent
                // This ensures flag matches server state even if set optimistically
                // Prevents incorrect ending-phase leniency in subsequent conversation
                if (state.endingPhase) {
                  state.endingPhase = false;
                  console.log(`[ENDING] Server rejected ending intent - resetting ending phase flag`);
                }
              }
            }
            // Handle clear intent check result
            else if (parsed.type === "clear_intent_result") {
              // This is handled in the wake phrase detection promise
              // No action needed here, just acknowledge
            }
            // Handle agent speak (for confirmation questions, re-auth messages)
            // FIXED: Process agent_speak messages - they come as llm_reply + audio from server
            else if (parsed.type === "agent_speak") {
              // Server handles agent_speak and sends back as llm_reply + audio
              // This handler is here for completeness, but actual processing happens via llm_reply
              console.log("[AGENT_SPEAK] Received agent speak request");
            }
            // Handle errors
            else if (parsed.type === "llm_error" || parsed.type === "tts_error") {
              state.error = parsed.error;
              state.status = 'listening';
              // CRITICAL: Ensure recognition is active when status becomes 'listening'
              ensureRecognitionActive();
              updateUI();
            }
          } catch (e) {
            console.error("Error parsing JSON:", e);
          }
        } else {
          // Binary Audio
          const arrayBuffer = event.data;

          // Validate audio data
          if (!arrayBuffer || arrayBuffer.byteLength === 0) {
            console.error("Received empty audio data from server");
            return;
          }

          console.log(`Received audio: ${arrayBuffer.byteLength} bytes`);
          const blob = new Blob([arrayBuffer], { type: "audio/mpeg" });
          const url = URL.createObjectURL(blob);
          queueAudio(url);
        }
      };
    }

    // --- Audio Queue ---
    function queueAudio(url) {
      audioQueue.push(url);
      processQueue();
    }

    function processQueue() {
      if (isPlaying || audioQueue.length === 0) return;

      isPlaying = true;
      state.status = 'speaking';
      
      // CRITICAL FIX: Keep recognition active during agent speech to enable barge-in
      // Recognition must stay active so we can detect user interruptions in real-time
      // This makes conversation feel natural and interactive
      if (state.isCallActive && !state.recognitionIsActive) {
        console.log(`[BARGE-IN] Ensuring recognition active during agent speech for barge-in detection`);
        ensureRecognitionActive();
      }
      
      updateUI();

      const url = audioQueue.shift();
      currentAudioUrl = url; // Store for correct retry
      activeAudio = new Audio(url);

      activeAudio.onended = () => {
        isPlaying = false;
        lastAudioEndTime = Date.now(); // Mark end time for tail protection
        // FIXED: Track when agent speech ends (for echo-aware windows)
        // This must be done in the original handler to avoid replacing it
        state.lastAgentSpeechEndTime = Date.now();
        console.log(`[ECHO-AWARE] Agent speech ended at ${new Date(state.lastAgentSpeechEndTime).toISOString()}`);
        activeAudio = null;
        
        // NATURAL PAUSES: Check if we need to add a pause based on the spoken text
        const pauseDelay = calculatePauseDelay(lastSpokenTextForPause);
        if (pauseDelay > 0) {
          console.log(`[PAUSE] Adding ${pauseDelay}ms pause after: "${lastSpokenTextForPause.substring(0, 50)}..."`);
        }
        
        // If more audio, add pause delay if needed, then continue
        if (audioQueue.length > 0) {
          setTimeout(() => {
            processQueue();
          }, pauseDelay);
        } else {
          // Add pause before returning to listening (if text suggests it)
          setTimeout(() => {
            state.status = 'listening';
            // CRITICAL: Ensure recognition is active when status becomes 'listening'
            // This ensures the system is ready to hear the user immediately
            ensureRecognitionActive();
            // Clear echo text after 6 seconds (protects against tail-end echoes)
            clearTimeout(echoTimeout);
            echoTimeout = setTimeout(() => {
              currentSpokenText = "";
              lastSpokenTextForPause = ""; // FIXED: Reset pause text when clearing echo to prevent stale text accumulation
              console.log("Echo protection cleared");
              // SAFETY CHECK: Verify recognition is still active when echo protection clears
              // This catches edge cases where recognition might have stopped during the 6-second window
              // CRITICAL: Always verify, don't trust manual tracking alone - recognition might have stopped
              if (state.isCallActive && !state.endingPhase) {
                // Force check: if recognition ended within last 8 seconds, ensure it's active
                const timeSinceLastEnd = state.lastRecognitionEndTime > 0 
                  ? Date.now() - state.lastRecognitionEndTime 
                  : Infinity;
                
                if (!state.recognitionIsActive || timeSinceLastEnd < 8000) {
                  console.log(`[ECHO CLEARED] Verifying recognition is active (isActive: ${state.recognitionIsActive}, ended ${Math.round(timeSinceLastEnd/1000)}s ago)`);
                  ensureRecognitionActive();
                }
              }
            }, 6000);
            updateUI();
          }, pauseDelay);
        }
        audioRetryCount = 0; // Reset retry counter on success
      };

      activeAudio.onerror = (event) => {
        const err = event.error || event;
        // Capture properties safely from the event target if possible, or fall back
        const targetAudio = event.target;

        console.error("Audio playback error:", {
          error: err,
          // Use target if available, or fallbacks
          src: targetAudio?.src || currentAudioUrl || 'unknown',
          readyState: targetAudio?.readyState,
          networkState: targetAudio?.networkState,
          errorCode: targetAudio?.error?.code,
          errorMessage: targetAudio?.error?.message,
          retryCount: audioRetryCount
        });

        // Retry logic: try playing again before giving up
        if (audioRetryCount < MAX_AUDIO_RETRIES && currentAudioUrl) {
          audioRetryCount++;
          console.log(`Retrying audio playback (attempt ${audioRetryCount}/${MAX_AUDIO_RETRIES})...`);

          // Cleanup current failed audio
          if (activeAudio) {
            activeAudio.pause();
            activeAudio = null;
          }
          isPlaying = false;

          // Re-queue the SAME URL at the front of the queue
          audioQueue.unshift(currentAudioUrl);

          // Retry after brief delay
          setTimeout(() => {
            processQueue();
          }, 300);
        } else {
          // Max retries reached, skip to next audio
          console.warn("Max retries reached, skipping to next audio");
          audioRetryCount = 0;
          isPlaying = false;
          activeAudio = null;
          currentAudioUrl = null; // Clear current URL

          state.status = 'listening';
          // CRITICAL: Ensure recognition is active when status becomes 'listening'
          ensureRecognitionActive();
          updateUI();
          // Continue with next audio in queue
          processQueue();
        }
      };

      activeAudio.play().catch(e => {
        if (e.name === 'AbortError') return; // Ignore intentional stops
        console.error("Autoplay failed", e);
        state.error = "Audio blocked. Click page.";
        updateUI();
      });
      
      // FIXED: Removed duplicate onended handler that was replacing the critical queue processor
      // The original handler at line 644 now includes echo-aware tracking to prevent handler replacement
    }
    
    // NATURAL PAUSES: Calculate pause delay based on text content
    function calculatePauseDelay(text) {
      if (!text || text.trim().length === 0) return 0;
      
      const trimmedText = text.trim();
      const lastChar = trimmedText[trimmedText.length - 1];
      const lowerText = trimmedText.toLowerCase();
      
      // After confirmation questions: longer pause (1-2 seconds)
      if (lowerText.includes('is everything correct') || 
          lowerText.includes('sab sahi hai') || 
          lowerText.includes('theek hai na')) {
        return 1500; // 1.5 second pause
      }
      
      // After question marks: medium pause (0.5-1 second)
      if (lastChar === '?') {
        return 800; // 0.8 second pause
      }
      
      // After periods (end of sentence): medium pause (0.5 second)
      if (lastChar === '.') {
        // Check if it's an acknowledgment followed by question
        const hasAcknowledgment = /^(got it|noted|perfect|great|acha|theek hai)[!.,]?/i.test(trimmedText);
        if (hasAcknowledgment) {
          return 300; // Brief pause after acknowledgment
        }
        return 500; // Standard pause after period
      }
      
      // After commas: brief pause (0.2-0.3 second)
      if (lastChar === ',') {
        return 250; // 0.25 second pause
      }
      
      // After exclamation (acknowledgment): brief pause (0.2-0.3 second)
      if (lastChar === '!') {
        const isAcknowledgment = /^(got it|noted|perfect|great|acha|theek hai)[!]?$/i.test(trimmedText);
        if (isAcknowledgment) {
          return 300; // Brief pause after acknowledgment
        }
        return 200; // Standard pause after exclamation
      }
      
      return 0; // No pause needed
    }

    function cancelAudio() {
      if (activeAudio) {
        // PREVENT RETRY LOOP: Remove handlers before stopping
        activeAudio.onerror = null;
        activeAudio.onended = null;

        activeAudio.pause();
        activeAudio.currentTime = 0; // Reset to beginning
        activeAudio.src = ''; // Clear source to fully stop
        activeAudio = null;
      }
      audioQueue = [];
      currentAudioUrl = null; // Clear current URL to prevent retry
      isPlaying = false;

      // ENSURE ECHO PROTECTION CLEARS EVENTUALLY
      // Since onended isn't called, we must manually set the cleanup timer here.
      // Otherwise, currentSpokenText remains stuck forever until next audio plays.
      clearTimeout(echoTimeout);
      echoTimeout = setTimeout(() => {
        currentSpokenText = "";
        console.log("Echo protection cleared (after cancel)");
        // SAFETY CHECK: Verify recognition is still active when echo protection clears
        // CRITICAL: Always verify, don't trust manual tracking alone
        if (state.isCallActive && !state.endingPhase) {
          const timeSinceLastEnd = state.lastRecognitionEndTime > 0 
            ? Date.now() - state.lastRecognitionEndTime 
            : Infinity;
          
          if (!state.recognitionIsActive || timeSinceLastEnd < 8000) {
            console.log(`[ECHO CLEARED] Verifying recognition is active (isActive: ${state.recognitionIsActive}, ended ${Math.round(timeSinceLastEnd/1000)}s ago)`);
            ensureRecognitionActive();
          }
        }
      }, 6000);

      lastAudioEndTime = Date.now(); // Also mark end time on cancel to prevent immediate echo leakage
      state.status = 'listening';
      // CRITICAL: Ensure recognition is active when status becomes 'listening'
      ensureRecognitionActive();
      updateUI();
    }

    // --- Helper Function: Ensure Recognition is Active ---
    // CRITICAL: When status becomes 'listening', recognition MUST be active for natural conversation
    // This ensures the system is ready to hear the user as soon as it claims to be listening
    function ensureRecognitionActive() {
      if (!recognition) return;
      
      // Only ensure recognition is active if call is active and not ending
      if (state.isCallActive && !state.endingPhase) {
        // FIX: Use manual state tracking as primary, recognition.state as fallback
        // Manual tracking is more reliable since recognition.state is not standardized
        if (state.recognitionIsActive) {
          // Recognition is already active according to our manual tracking
          // Double-check with recognition.state if available (fallback)
          if ('state' in recognition) {
            const recognitionState = recognition.state;
            if (recognitionState === 'starting' || recognitionState === 'started') {
              // Confirmed active, all good
              return; // No need to start
            } else if (recognitionState === 'stopped' || recognitionState === 'inactive') {
              // State mismatch - manual tracking says active but recognition.state says inactive
              // Trust recognition.state and update manual tracking
              console.warn(`[RECOGNITION] State mismatch - manual tracking says active but recognition.state is ${recognitionState}`);
              state.recognitionIsActive = false;
              // Fall through to start recognition
            }
            // If recognitionState is undefined or unknown, trust manual tracking
          } else {
            // recognition.state not available, trust manual tracking
            return; // Already active according to manual tracking
          }
        }
        
        // Recognition is not active (according to manual tracking or recognition.state)
        try {
          recognition.start();
          // Note: recognitionIsActive will be set to true in onstart handler
          console.log(`[RECOGNITION] Started successfully`);
        } catch (e) {
          // If start fails (e.g., already started), handle gracefully
          if (e.message && (e.message.includes('already') || e.message.includes('started'))) {
            // Recognition is actually started, update manual tracking
            state.recognitionIsActive = true;
            console.log(`[RECOGNITION] Already started - updated manual tracking`);
          } else {
            console.warn(`[RECOGNITION] Failed to ensure active, will retry:`, e);
            // Retry after a short delay
            setTimeout(() => {
              if (state.isCallActive && !state.endingPhase) {
                try {
                  recognition.start();
                  // Note: recognitionIsActive will be set to true in onstart handler
                  console.log(`[RECOGNITION] Retry start successful`);
                } catch (e2) {
                  if (e2.message && (e2.message.includes('already') || e2.message.includes('started'))) {
                    // Recognition is actually started, update manual tracking
                    state.recognitionIsActive = true;
                    console.log(`[RECOGNITION] Already started (retry check) - updated manual tracking`);
                  } else {
                    console.warn(`[RECOGNITION] Retry start failed:`, e2);
                  }
                }
              }
            }, 300);
          }
        }
      }
    }

    // --- Web Speech API Logic ---
    async function startCall() {
      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
      if (!SpeechRecognition) {
        state.error = "Browser not supported (Use Chrome)";
        updateUI();
        return;
      }

      // 1. AEC Hack: Request mic with echoCancellation to force Hardware AEC
      try {
        aecStream = await navigator.mediaDevices.getUserMedia({
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true
          }
        });
        // We don't play this stream, just hold it open.
        console.log("AEC Stream active");

        // FIXED: Wait for VAD model to load before setting up processing
        // This ensures VAD is ready when recognition starts
        if (!vadModel) {
          console.log("Waiting for VAD model to load...");
          // Wait up to 5 seconds for VAD to load
          let waitCount = 0;
          while (!vadModel && waitCount < 50) {
            await new Promise(resolve => setTimeout(resolve, 100));
            waitCount++;
          }
        }
        
        if (vadModel) {
          await setupVADAudioProcessing();
        } else {
          console.warn("VAD model not loaded, using fallback timer-based silence detection");
        }
      } 
      catch (err) {
        console.warn("Could not get AEC stream", err);
      }

      // FIXED: Set waitingForWakePhrase BEFORE isCallActive to ensure wake phrase detection is enabled
      // This ensures wake phrase detection works even if recognition starts before isCallActive is set
      state.waitingForWakePhrase = true;
      state.isCallActive = true;
      // FIX: Reset manual recognition state tracking for new call
      state.recognitionIsActive = false;
      state.lastRecognitionEndTime = 0;
      console.log(`[WAKE PHRASE DEBUG] Call started - waitingForWakePhrase: ${state.waitingForWakePhrase}, isCallActive: ${state.isCallActive}`);
      state.status = 'listening';
      // CRITICAL: Ensure recognition is active when status becomes 'listening'
      // Note: Recognition will be started below, but this ensures it's active if it was already created
      ensureRecognitionActive();
      
      // FIXED: Reset LLM context for new call to prevent data from previous calls
      // This ensures each call starts fresh and doesn't show data from previous conversations
      if (socket && socket.readyState === WebSocket.OPEN) {
        socket.send(JSON.stringify({ 
          type: 'new_call'
        }));
        console.log(`[CALL] Sent new_call message to reset context`);
      }
      
      // FIXED: Clean up any existing follow-up timer before creating a new one
      // This prevents timer accumulation if startCall() is called multiple times
      if (followUpTimer) {
        clearInterval(followUpTimer);
        followUpTimer = null;
      }
      
      // FIXED: Create follow-up timer when call starts (not globally)
      // This ensures the timer is recreated on each new call after stopCall() clears it
      followUpTimer = setInterval(checkProactiveFollowUp, 1000);
      
      // FIX #4: Create recognition health check timer when call starts
      // This proactively ensures recognition stays active throughout the conversation
      if (recognitionHealthCheckTimer) {
        clearInterval(recognitionHealthCheckTimer);
        recognitionHealthCheckTimer = null;
      }
      // ENHANCED: More frequent health checks for better reliability
      // Check every 1.5 seconds to catch recognition issues faster
      recognitionHealthCheckTimer = setInterval(checkRecognitionHealth, 1500); // Check every 1.5 seconds
      console.log(`[HEALTH CHECK] Started recognition health check timer`);
      
      // Reset speaker profile for new call
      if (ENABLE_SPEAKER_FILTERING) {
        state.speakerProfile.calibrationComplete = false;
        state.speakerProfile.frameCount = 0;
        state.speakerProfile.volumeHistory = [];
        state.speakerProfile.pitchHistory = [];
        state.speakerProfile.timbreHistory = [];
        state.speakerProfile.spectralCentroidHistory = [];
        state.speakerProfile.mfccHistory = [];
        state.speakerProfile.mfccDeltaHistory = [];
        state.speakerProfile.mfccDeltaDeltaHistory = [];
        state.speakerProfile.mfccNormalizedHistory = [];
        state.speakerProfile.mfccMean = null;
        state.speakerProfile.mfccVariance = null;
        state.speakerProfile.conversationContext = {
          isAnsweringQuestion: false,
          lastQuestionTime: 0,
          topicRelevance: 1.0,
          responseTiming: 0,
          confidenceHistory: [],
          adaptiveThreshold: 0.3
        };
        state.speakerProfile.firstSpeakerDetected = false;
        state.speakerProfile.firstSpeakerVolume = null;
        state.speakerProfile.firstSpeakerPitch = null;
        state.speakerProfile.firstSpeakerTimbre = null;
        state.speakerProfile.callInitiatorDetected = false;
        state.speakerProfile.calibrationStartTime = Date.now();
        state.speakerProfile.weightedVolumeHistory = [];
        state.speakerProfile.weightedPitchHistory = [];
        state.speakerProfile.weightedTimbreHistory = [];
        state.speakerProfile.lastMismatchReason = null;
        // FIXED: Reset mismatch tracking state to prevent stale state from previous call
        // Without this, mismatchCount and lastMismatchTime from previous calls could cause
        // incorrect wrong lock detection in new calls
        state.mismatchCount = 0;
        state.lastMismatchTime = null;
        state.wrongLockDetected = false;
        // FIXED: Reset recentMaxRMS to prevent stale peak volume from previous call
        // Without this, volume-based filtering in new call uses previous speaker's peak volume
        // causing incorrect input filtering before new speaker is properly calibrated
        state.speakerProfile.recentMaxRMS = 0.0;
        // FIXED: Clear knownSpeakers Map to prevent speakers from previous calls persisting
        // Without this, multi-speaker matching could incorrectly trigger across separate calls
        // causing false speaker identification
        state.speakerProfile.knownSpeakers.clear();
        // FIXED: Reload voiceprints from localStorage after clearing to restore persistent speakers
        // This ensures multi-speaker persistence works correctly - speakers from previous calls
        // are restored, and new speakers are merged when saving (not overwritten)
        loadVoiceprintsFromStorage();
        state.speakerProfile.currentSpeakerId = null;
        console.log(">> Speaker profile reset for new call");
      }
      
      updateUI();

      if (!recognition) {
        recognition = new SpeechRecognition();
        recognition.continuous = true;
        recognition.interimResults = true;
        recognition.lang = "en-US";

        recognition.onstart = () => {
          console.log("Recognition started");
          // FIX: Track recognition state manually (more reliable than recognition.state)
          state.recognitionIsActive = true;
          // FIXED: Clear isRestartingRecognition flag when recognition actually starts
          // This ensures the flag is cleared in the success path, not just error paths
          // Without this, the flag remains true after successful restart, preventing subsequent retries
          state.isRestartingRecognition = false;
          state.status = 'listening';
          updateUI();
        };

        recognition.onerror = (event) => {
          // SILENCE EXPECTED ERRORS: 'no-speech' is normal when user is thinking
          if (event.error === 'no-speech') {
            // Do NOT restart here. 'onend' will fire next and handle the restart centrally.
            // This prevents "double-start" race conditions.
            return;
          }

          console.warn("Recognition error:", event.error); // Warn for real errors only


          if (event.error === 'network') {
            // Network error - restart recognition after a brief delay
            console.log("Network error detected, restarting recognition...");
            if (state.isCallActive) {
              setTimeout(() => {
                try {
                  recognition.stop(); // Stop first to clean up
                  setTimeout(() => {
                    if (state.isCallActive) {
                      recognition.start();
                    }
                  }, 100);
                } catch (e) {
                  console.error("Failed to restart recognition:", e);
                }
              }, 500);
            }
            return;
          }

          // For other errors (audio-capture, not-allowed, etc.), log but don't crash
          if (event.error === 'aborted') return; // Normal stop

          // Critical errors - show to user
          if (event.error === 'not-allowed' || event.error === 'service-not-allowed') {
            state.error = "Microphone permission denied";
            // FIX #1: Log when isCallActive is being set to false due to permission error
            console.warn(`[STATE CHANGE] Microphone permission denied - setting isCallActive=false`);
            state.isCallActive = false;
            updateUI();
          }
        };

        recognition.onend = () => {
          console.log("Recognition ended");
          // FIX: Track recognition state manually (more reliable than recognition.state)
          state.recognitionIsActive = false;
          state.lastRecognitionEndTime = Date.now();
          
          // CRITICAL FIX: Always restart if call is active OR if we're in confirmation/wake phrase step
          // ENHANCED: Also restart during agent speech to enable barge-in detection
          // This ensures recognition is always active, making conversation feel natural
          const shouldRestart = (state.isCallActive && !state.endingPhase) || 
                                state.confirmationStep || 
                                state.waitingForWakePhrase ||
                                (isPlaying || audioQueue.length > 0); // Keep active during agent speech for barge-in
          
          if (shouldRestart) {
            console.log(`[RECOGNITION] Auto-restarting (isCallActive: ${state.isCallActive}, waitingForWakePhrase: ${state.waitingForWakePhrase}, confirmationStep: ${state.confirmationStep}, endingPhase: ${state.endingPhase})`);
            // FIX #5: Set flag to prevent stopCall() from interfering during restart
            state.isRestartingRecognition = true;
            setTimeout(() => {
              try { 
                // FIXED: Double-check state before restarting (defensive programming)
                // Include agent speech condition to ensure barge-in detection works during agent speech
                if (state.isCallActive || state.confirmationStep || state.waitingForWakePhrase || (isPlaying || audioQueue.length > 0)) {
                  recognition.start(); 
                  console.log(`[RECOGNITION] Restarted successfully`);
                  // Clear flag after successful restart
                  state.isRestartingRecognition = false;
                } else {
                  console.warn(`[RECOGNITION] Skipping restart - call state changed during delay`);
                  state.isRestartingRecognition = false;
                }
              } catch (e) { 
                console.warn(`[RECOGNITION] Restart failed:`, e);
                // Retry restart after a delay, especially important during confirmation or wake phrase retry
                if (state.isCallActive || state.confirmationStep || state.waitingForWakePhrase) {
                  console.log(`[RECOGNITION] Retrying restart after 500ms`);
                  setTimeout(() => {
                    try {
                      if (state.isCallActive || state.confirmationStep || state.waitingForWakePhrase) {
                        recognition.start();
                        // Note: recognitionIsActive will be set to true in onstart handler
                        console.log(`[RECOGNITION] Retry restart successful`);
                        state.isRestartingRecognition = false;
                      } else {
                        console.warn(`[RECOGNITION] Skipping retry restart - call state changed`);
                        state.isRestartingRecognition = false;
                      }
                    } catch (e2) {
                      console.error(`[RECOGNITION] Retry restart failed:`, e2);
                      state.isRestartingRecognition = false;
                    }
                  }, 500);
                } else {
                  state.isRestartingRecognition = false;
                }
              }
            }, 200);
          } else {
            if (!state.isCallActive && !state.confirmationStep && !state.waitingForWakePhrase) {
              console.warn(`[RECOGNITION] Not restarting - DEBUG INFO: isCallActive=${state.isCallActive}, confirmationStep=${state.confirmationStep}, waitingForWakePhrase=${state.waitingForWakePhrase}, endingPhase=${state.endingPhase}`);
              console.warn(`[RECOGNITION] This might indicate a bug - recognition should restart during active calls`);
              // FIX #1: Log when isCallActive becomes false unexpectedly
              console.warn(`[STATE CHANGE] isCallActive became false - Stack trace:`, new Error().stack);
            } else if (state.endingPhase) {
              console.log(`[RECOGNITION] Not restarting (ending phase)`);
            }
          }
        };

        // --- Wake Phrase Detection Functions ---
        // Keywords for hotel management/admissions context
        const WAKE_KEYWORDS = [
          'hotel management',
          'admission',
          'course',
          'apply',
          'enroll',
          'institute',
          'college',
          'counseling',
          // Hindi/Hinglish variations
          'admission lena',
          'admission chahiye',
          'course lena',
          'course chahiye',
          'apply karna',
          'apply karo'
        ];

        // Base wake phrases (existing - keep all)
        const BASE_WAKE_PHRASES = [
          // English
          'hello riya start my counselling',
          'hi riya begin counselling',
          'riya start my counselling',
          'hello riya i want counselling',
          'riya counselling start',
          'hello riya start counselling',
          'hi riya start counselling',
          // Hindi
          'namaste riya counselling shuru',
          'hello riya counselling start karo',
          'riya counselling shuru karo',
          'namaste counselling chahiye',
          'hello riya counselling shuru karo',
          // Hinglish
          'hello riya counselling start karo',
          'hi riya counselling shuru karo',
          'riya counselling chahiye'
        ];

        // Generate expanded wake phrases with keywords
        function generateExpandedWakePhrases() {
          const expanded = [...BASE_WAKE_PHRASES];
          const prefixes = ['hello riya', 'hi riya', 'riya', 'namaste riya', 'hello', 'hi'];
          
          // Add patterns: prefix + keyword
          for (const prefix of prefixes) {
            for (const keyword of WAKE_KEYWORDS) {
              // Pattern: "prefix keyword" or "prefix ... keyword"
              expanded.push(`${prefix} ${keyword}`);
              expanded.push(`${prefix} i want ${keyword}`);
              expanded.push(`${prefix} tell me about ${keyword}`);
              expanded.push(`${prefix} ${keyword} information`);
              expanded.push(`i want ${keyword} riya`);
              expanded.push(`${keyword} riya`);
            }
          }
          
          return expanded;
        }

        const WAKE_PHRASES = generateExpandedWakePhrases();

        // Normalize text: lowercase, remove punctuation, handle plurals
        function normalizeTextForWakePhrase(text) {
          let normalized = text.toLowerCase()
            .replace(/[.,\/#!$%\^&\*;:{}=\-_`~()]/g, '') // Remove punctuation
            .replace(/\s{2,}/g, ' ') // Collapse whitespace
            .trim();
          
          // Normalize plurals and variants
          normalized = normalized
            .replace(/\bcourses\b/g, 'course')
            .replace(/\badmissions\b/g, 'admission')
            .replace(/\bcounselling\b/g, 'counseling')
            .replace(/\bapplications\b/g, 'apply')
            .replace(/\benrollments\b/g, 'enroll')
            .replace(/\binstitutions\b/g, 'institute');
          
          return normalized;
        }

        // Check if transcript contains Riya + keyword
        function hasRiyaAndKeyword(normalizedText) {
          // NATURAL WAKE PHRASE: Accept phrases with keywords even without "Riya"
          // This makes wake phrases more natural like "Hello, I need counselling"
          const normalizedKeywords = WAKE_KEYWORDS.map(kw => normalizeTextForWakePhrase(kw));
          const hasKeyword = normalizedKeywords.some(keyword => normalizedText.includes(keyword));
          
          // Check for natural wake phrases (greeting + keyword, or just strong intent)
          const hasGreeting = /^(hello|hi|hey|namaste|good morning|good afternoon|good evening)/i.test(normalizedText);
          const hasIntentWords = /(need|want|looking for|interested in|tell me about|information about)/i.test(normalizedText);
          
          // Accept if: (Riya + keyword) OR (greeting + keyword) OR (intent words + keyword)
          const hasRiya = normalizedText.includes('riya');
          if (hasRiya && hasKeyword) return true; // Original: Riya + keyword
          if (hasGreeting && hasKeyword) return true; // Natural: "Hello, I need counselling"
          if (hasIntentWords && hasKeyword) return true; // Natural: "I want counselling"
          
          return false;
        }

        // Fuzzy matching for wake phrases
        function fuzzyMatchWakePhrase(normalizedTranscript) {
          for (const phrase of WAKE_PHRASES) {
            const normalizedPhrase = normalizeTextForWakePhrase(phrase);
            const similarity = calculateSimilarity(normalizedTranscript, normalizedPhrase);
            
            // Different thresholds: 75% for exact phrases, 70% for keyword-based
            const isExactPhrase = BASE_WAKE_PHRASES.includes(phrase);
            const threshold = isExactPhrase ? 0.75 : 0.70;
            
            if (similarity > threshold) {
              console.log(`>> Wake phrase detected (fuzzy match ${(similarity * 100).toFixed(1)}%): "${phrase}"`);
              return true;
            }
          }
          return false;
        }

        // Calculate similarity between two strings (normalized Levenshtein)
        function calculateSimilarity(str1, str2) {
          const distance = levenshtein(str1, str2);
          const maxLen = Math.max(str1.length, str2.length);
          return maxLen > 0 ? 1 - (distance / maxLen) : 0;
        }

        // Check if wake phrase detection should be active
        function isWakePhraseDetectionActive() {
          return !state.isCallActive || state.reAuthMode || state.waitingForWakePhrase;
        }

        // Wrong lock recovery detection
        function checkWrongLockRecovery() {
          if (!state.speakerProfile.calibrationComplete) return;
          
          // Check if we should trigger wrong lock recovery
          // FIXED: Only check lastMismatchTime if there's an active mismatch (mismatchCount > 0)
          // This prevents false triggers from old mismatches that have already resolved
          const hasActiveMismatch = state.mismatchCount > 0;
          const timeSinceLastMismatch = state.lastMismatchTime ? (Date.now() - state.lastMismatchTime) : 0;
          // FIXED: Changed threshold from 3 to 30 frames (~1 second at 30ms per frame)
          // Previous threshold of 3 frames (90ms) was too low and caused frequent false positives
          // from brief background noise, user coughing, or momentary voice variations
          const MISMATCH_FRAME_THRESHOLD = 30; // ~1 second of continuous mismatch
          const shouldTrigger = 
            (state.mismatchCount >= MISMATCH_FRAME_THRESHOLD) || // 30+ consecutive mismatches (~1 second)
            (hasActiveMismatch && state.lastMismatchTime && timeSinceLastMismatch > 10000) || // 10 seconds of continuous ACTIVE mismatch
            false; // User frustration signals can be added here
            
          if (shouldTrigger && !state.wrongLockDetected) {
            state.wrongLockDetected = true;
            state.reAuthMode = true;
            // FIXED: Set waitingForWakePhrase to true to block all transcripts until wake phrase is re-entered
            // Without this, normal speech gets processed during re-auth mode, allowing users to bypass re-identification
            state.waitingForWakePhrase = true;
            console.log(`>> Wrong lock detected! Entering re-auth mode...`);
            
            // Agent says re-auth message
            if (socket && socket.readyState === WebSocket.OPEN) {
              socket.send(JSON.stringify({ 
                type: 'agent_speak', 
                text: "I'm hearing multiple voices. Please say 'Riya start counselling' again to confirm it's you." 
              }));
            }
          }
        }

        // Ending phase detection
        function checkEndingPhase(transcript) {
          if (state.endingPhase) return; // Already in ending phase
          
          const lowerTranscript = transcript.toLowerCase().trim();
          const endingKeywords = ['thank you', 'thanks', 'bye', 'goodbye', "that's all", "i'm done", 'done', 'finish', 'complete'];
          
          // Check if transcript contains ending keywords
          const hasEndingKeyword = endingKeywords.some(keyword => lowerTranscript.includes(keyword));
          
          if (hasEndingKeyword) {
            // Send to server for LLM intent check
            if (socket && socket.readyState === WebSocket.OPEN) {
              socket.send(JSON.stringify({ 
                type: 'ending_intent_check', 
                text: transcript 
              }));
            }
          }
        }

        // LLM intent check for wake phrase (via server)
        // FIXED: Use message listener pattern instead of replacing socket.onmessage
        // This prevents message loss and handler collisions
        async function checkWakePhraseIntent(transcript) {
          try {
            if (!socket || socket.readyState !== WebSocket.OPEN) return false;
            
            // Send intent check request to server
            socket.send(JSON.stringify({ 
              type: 'wake_phrase_intent_check', 
              text: transcript 
            }));
            
            // Wait for response (with timeout)
            return new Promise((resolve) => {
              // FIXED: Chain handlers instead of replacing to prevent orphaned handlers
              // When multiple checks run concurrently, we chain to the existing handler
              // instead of replacing it, preventing handler loss
              const currentHandler = socket.onmessage;
              
              // FIXED: Track if timeout already fired to prevent race condition
              // If timeout fires first, ignore late server responses
              let timeoutFired = false;
              let resolved = false;
              
              // Helper function to safely resolve only once
              const safeResolve = (value) => {
                if (resolved) {
                  console.warn(`[Wake Phrase Intent] Ignoring duplicate resolve attempt (timeout already fired: ${timeoutFired})`);
                  return;
                }
                resolved = true;
                removeFromChain();
                resolve(value);
              };
              
              // Helper function to remove this handler from the chain
              const removeFromChain = () => {
                if (!socket) return;
                
                // FIXED: If we're the active handler, restore to the handler we chained to
                // This handles the case where we're still the active handler
                if (socket.onmessage === wrapperHandler) {
                  socket.onmessage = currentHandler;
                  return;
                }
                
                // FIXED: Search the entire chain to find any handler that points to us
                // This handles the case where a new handler was added after us
                // We need to traverse the entire chain from the root, not just from the active handler
                let current = socket.onmessage;
                let iterations = 0;
                const MAX_ITERATIONS = 100; // Safety limit to prevent infinite loops
                const visited = new Set(); // Track visited handlers to prevent infinite loops
                
                while (current && iterations < MAX_ITERATIONS) {
                  iterations++;
                  
                  // FIXED: Add defensive check for handler properties
                  // If handler doesn't have expected properties, break to prevent errors
                  if (typeof current !== 'function') {
                    console.warn('[Handler Chain] Invalid handler type detected, breaking chain traversal');
                    break;
                  }
                  
                  // FIXED: Check if current handler points to our wrapper
                  // This finds us even if we're not the active handler
                  if (current._isWrapper && current._nextHandler === wrapperHandler) {
                    // Found the handler that points to us - update it to skip us
                    current._nextHandler = currentHandler;
                    return;
                  }
                  
                  // FIXED: Also check if current handler IS our wrapper (defensive check)
                  // This handles edge cases where wrapper might be found directly
                  if (current === wrapperHandler) {
                    // We found ourselves - this shouldn't happen if we're not the active handler,
                    // but if it does, we need to remove ourselves from the chain
                    // Find the handler that points to us by searching from the beginning again
                    let prev = socket.onmessage;
                    let prevIterations = 0;
                    while (prev && prevIterations < MAX_ITERATIONS) {
                      prevIterations++;
                      if (prev._isWrapper && prev._nextHandler === wrapperHandler) {
                        prev._nextHandler = currentHandler;
                        return;
                      }
                      if (prev._isWrapper && prev._nextHandler) {
                        prev = prev._nextHandler;
                      } else {
                        prev = null;
                      }
                      if (prev === socket.onmessage) break;
                    }
                    // If we can't find the previous handler, force remove by setting socket.onmessage
                    console.warn('[Handler Chain] Found wrapper in chain but could not find previous handler - forcing removal');
                    socket.onmessage = currentHandler;
                    return;
                  }
                  
                  // Move to next handler in chain
                  // FIXED: Add defensive check before accessing _nextHandler
                  if (current._isWrapper && current._nextHandler) {
                    // Check for circular references before moving
                    if (visited.has(current)) {
                      console.warn('[Handler Chain] Circular reference detected (visited set), breaking');
                      break;
                    }
                    visited.add(current);
                    current = current._nextHandler;
                  } else {
                    current = null;
                  }
                  
                  // FIXED: Safety check to prevent circular references
                  if (current === socket.onmessage) {
                    console.warn('[Handler Chain] Circular reference detected, breaking');
                    break;
                  }
                }
                
                if (iterations >= MAX_ITERATIONS) {
                  console.warn('[Handler Chain] Maximum iterations reached, possible infinite loop');
                  // FIXED: Force remove wrapper even if not found in chain to prevent memory leaks
                  // This ensures the wrapper is removed even if chain traversal fails
                  // Note: wrapperHandler is defined later, but since removeFromChain is only called
                  // after wrapperHandler is defined, the reference works via closure
                  try {
                    if (socket && socket.onmessage === wrapperHandler) {
                      socket.onmessage = currentHandler;
                      console.warn('[Handler Chain] Force removed wrapper by replacing socket.onmessage');
                    } else {
                      // Force remove by setting to currentHandler as last resort
                      // This prevents memory leaks even if wrapper is not found in chain
                      console.warn('[Handler Chain] Could not find wrapper in chain - forcing removal to prevent memory leak');
                      socket.onmessage = currentHandler;
                    }
                  } catch (e) {
                    // If wrapperHandler is not defined yet (shouldn't happen, but defensive)
                    console.warn('[Handler Chain] Error during force removal, forcing socket.onmessage reset:', e);
                    socket.onmessage = currentHandler;
                  }
                }
              };
              
              // Create wrapper that handles both our message and forwards others
              const wrapperHandler = (event) => {
                // FIXED: Check if it's our message FIRST, before forwarding
                // This prevents duplicate processing of other messages (text_chunk, llm_reply, etc.)
                // which would cause duplicate currentSpokenText accumulation and lastAgentQuestion updates
                if (typeof event.data === 'string') {
                  try {
                    const parsed = JSON.parse(event.data);
                    if (parsed.type === 'wake_phrase_intent_result') {
                      // This is our message - handle it and don't forward to prevent duplicate processing
                      clearTimeout(timeout);
                      // FIXED: Only resolve if timeout hasn't fired yet
                      if (!timeoutFired) {
                        console.log(`[Wake Phrase Intent] Server response received before timeout for: "${transcript}"`);
                        safeResolve(parsed.isWakePhrase || false);
                      } else {
                        console.warn(`[Wake Phrase Intent] Server response received after timeout, ignoring for: "${transcript}"`);
                      }
                      return; // Don't forward this message to prevent duplicate processing
                    }
                  } catch (e) {
                    // Not JSON or parse error, continue to forward
                  }
                }
                
                // Forward other messages to current handler (only if not our message)
                if (currentHandler) {
                  currentHandler(event);
                }
              };
              
              // Mark wrapper for identification and chain to current handler
              wrapperHandler._isWrapper = true;
              wrapperHandler._nextHandler = currentHandler;
              
              const timeout = setTimeout(() => {
                timeoutFired = true;
                // FIXED: Ensure handler is removed before resolving to prevent message channel errors
                try {
                  removeFromChain();
                } catch (e) {
                  console.warn('Error removing handler from chain:', e);
                }
                console.warn(`[Wake Phrase Intent] Timeout after 2s for: "${transcript}" - treating as rejection`);
                // FIXED: On timeout, treat as rejection (not acceptance) to be safe
                // This prevents accepting unclear wake phrases due to slow LLM
                safeResolve(false);
              }, 2000);
              
              // Chain to existing handler instead of replacing
              socket.onmessage = wrapperHandler;
            });
          } catch (e) {
            console.warn('Wake phrase intent check failed:', e);
            return false;
          }
        }

        // Clear intent verification (always run)
        // FIXED: Use message listener pattern instead of replacing socket.onmessage
        // This prevents message loss and handler collisions
        async function checkClearIntent(transcript) {
          try {
            if (!socket || socket.readyState !== WebSocket.OPEN) return false;
            
            // Send clear intent check request to server
            socket.send(JSON.stringify({ 
              type: 'clear_intent_check', 
              text: transcript 
            }));
            
            // Wait for response (with timeout)
            return new Promise((resolve) => {
              // FIXED: Chain handlers instead of replacing to prevent orphaned handlers
              // When multiple checks run concurrently, we chain to the existing handler
              // instead of replacing it, preventing handler loss
              const currentHandler = socket.onmessage;
              
              // FIXED: Track if timeout already fired to prevent race condition
              // If timeout fires first, ignore late server responses
              let timeoutFired = false;
              let resolved = false;
              
              // Helper function to safely resolve only once
              const safeResolve = (value) => {
                if (resolved) {
                  console.warn(`[Clear Intent] Ignoring duplicate resolve attempt (timeout already fired: ${timeoutFired})`);
                  return;
                }
                resolved = true;
                removeFromChain();
                resolve(value);
              };
              
              // Helper function to remove this handler from the chain
              const removeFromChain = () => {
                if (!socket) return;
                
                // FIXED: If we're the active handler, restore to the handler we chained to
                // This handles the case where we're still the active handler
                if (socket.onmessage === wrapperHandler) {
                  socket.onmessage = currentHandler;
                  return;
                }
                
                // FIXED: Search the entire chain to find any handler that points to us
                // This handles the case where a new handler was added after us
                // We need to traverse the entire chain from the root, not just from the active handler
                let current = socket.onmessage;
                let iterations = 0;
                const MAX_ITERATIONS = 100; // Safety limit to prevent infinite loops
                const visited = new Set(); // Track visited handlers to prevent infinite loops
                
                while (current && iterations < MAX_ITERATIONS) {
                  iterations++;
                  
                  // FIXED: Add defensive check for handler properties
                  // If handler doesn't have expected properties, break to prevent errors
                  if (typeof current !== 'function') {
                    console.warn('[Handler Chain] Invalid handler type detected, breaking chain traversal');
                    break;
                  }
                  
                  // FIXED: Check if current handler points to our wrapper
                  // This finds us even if we're not the active handler
                  if (current._isWrapper && current._nextHandler === wrapperHandler) {
                    // Found the handler that points to us - update it to skip us
                    current._nextHandler = currentHandler;
                    return;
                  }
                  
                  // FIXED: Also check if current handler IS our wrapper (defensive check)
                  // This handles edge cases where wrapper might be found directly
                  if (current === wrapperHandler) {
                    // We found ourselves - this shouldn't happen if we're not the active handler,
                    // but if it does, we need to remove ourselves from the chain
                    // Find the handler that points to us by searching from the beginning again
                    let prev = socket.onmessage;
                    let prevIterations = 0;
                    while (prev && prevIterations < MAX_ITERATIONS) {
                      prevIterations++;
                      if (prev._isWrapper && prev._nextHandler === wrapperHandler) {
                        prev._nextHandler = currentHandler;
                        return;
                      }
                      if (prev._isWrapper && prev._nextHandler) {
                        prev = prev._nextHandler;
                      } else {
                        prev = null;
                      }
                      if (prev === socket.onmessage) break;
                    }
                    // If we can't find the previous handler, force remove by setting socket.onmessage
                    console.warn('[Handler Chain] Found wrapper in chain but could not find previous handler - forcing removal');
                    socket.onmessage = currentHandler;
                    return;
                  }
                  
                  // Move to next handler in chain
                  // FIXED: Add defensive check before accessing _nextHandler
                  if (current._isWrapper && current._nextHandler) {
                    // Check for circular references before moving
                    if (visited.has(current)) {
                      console.warn('[Handler Chain] Circular reference detected (visited set), breaking');
                      break;
                    }
                    visited.add(current);
                    current = current._nextHandler;
                  } else {
                    current = null;
                  }
                  
                  // FIXED: Safety check to prevent circular references
                  if (current === socket.onmessage) {
                    console.warn('[Handler Chain] Circular reference detected, breaking');
                    break;
                  }
                }
                
                if (iterations >= MAX_ITERATIONS) {
                  console.warn('[Handler Chain] Maximum iterations reached, possible infinite loop');
                  // FIXED: Force remove wrapper even if not found in chain to prevent memory leaks
                  // This ensures the wrapper is removed even if chain traversal fails
                  // Note: wrapperHandler is defined later, but since removeFromChain is only called
                  // after wrapperHandler is defined, the reference works via closure
                  try {
                    if (socket && socket.onmessage === wrapperHandler) {
                      socket.onmessage = currentHandler;
                      console.warn('[Handler Chain] Force removed wrapper by replacing socket.onmessage');
                    } else {
                      // Force remove by setting to currentHandler as last resort
                      // This prevents memory leaks even if wrapper is not found in chain
                      console.warn('[Handler Chain] Could not find wrapper in chain - forcing removal to prevent memory leak');
                      socket.onmessage = currentHandler;
                    }
                  } catch (e) {
                    // If wrapperHandler is not defined yet (shouldn't happen, but defensive)
                    console.warn('[Handler Chain] Error during force removal, forcing socket.onmessage reset:', e);
                    socket.onmessage = currentHandler;
                  }
                }
              };
              
              // Create wrapper that handles both our message and forwards others
              const wrapperHandler = (event) => {
                // FIXED: Check if it's our message FIRST, before forwarding
                // This prevents duplicate processing of other messages (text_chunk, llm_reply, etc.)
                // which would cause duplicate currentSpokenText accumulation and lastAgentQuestion updates
                if (typeof event.data === 'string') {
                  try {
                    const parsed = JSON.parse(event.data);
                    if (parsed.type === 'clear_intent_result') {
                      // This is our message - handle it and don't forward to prevent duplicate processing
                      clearTimeout(timeout);
                      // FIXED: Only resolve if timeout hasn't fired yet
                      if (!timeoutFired) {
                        console.log(`[Clear Intent] Server response received before timeout for: "${transcript}"`);
                        safeResolve(parsed.hasClearIntent || false);
                      } else {
                        console.warn(`[Clear Intent] Server response received after timeout, ignoring for: "${transcript}"`);
                      }
                      return; // Don't forward this message to prevent duplicate processing
                    }
                  } catch (e) {
                    // Not JSON or parse error, continue to forward
                  }
                }
                
                // Forward other messages to current handler (only if not our message)
                if (currentHandler) {
                  currentHandler(event);
                }
              };
              
              // Mark wrapper for identification and chain to current handler
              wrapperHandler._isWrapper = true;
              wrapperHandler._nextHandler = currentHandler;
              
              const timeout = setTimeout(() => {
                timeoutFired = true;
                // FIXED: Ensure handler is removed before resolving to prevent message channel errors
                try { removeFromChain(); } 
                catch (e) { console.warn('Error removing handler from chain:', e); }
                console.warn(`[Clear Intent] Timeout after 2.5s for: "${transcript}" - treating as rejection`);
                // FIXED: On timeout, treat as rejection (not acceptance) to be safe
                // This prevents accepting unclear wake phrases due to slow LLM
                safeResolve(false);
              }, 2500); // INCREASED: 2s ‚Üí 2.5s to account for network latency (server responses arriving at ~2s)
              
              // Chain to existing handler instead of replacing
              socket.onmessage = wrapperHandler;
            });
          } 
          catch (e) {
            console.warn('Clear intent check failed:', e);
            return false;
          }
        }

        // Voice consistency check between wake phrase and confirmation
        // FIXED: Validate MFCC arrays before passing to calculateMFCCSimilarity
        function checkVoiceConsistency(wakeFeatures, confirmFeatures) {
          if (!wakeFeatures || !confirmFeatures) return false;
          
          // Track which checks are valid (have data) and which pass
          let hasValidCheck = false;
          let validChecksPass = 0;
          let totalValidChecks = 0;
          
          // Check pitch (within ¬±30%)
          // FIXED: Handle null/undefined pitch values - if either is missing, skip pitch check
          // Coercing null to 0 causes ratio to be 0, which fails the >=0.7 check incorrectly
          if (wakeFeatures.pitch != null && confirmFeatures.pitch != null && 
              wakeFeatures.pitch > 0 && confirmFeatures.pitch > 0) {
            hasValidCheck = true;
            totalValidChecks++;
            const pitchRatio = confirmFeatures.pitch / wakeFeatures.pitch;
            const pitchMatch = pitchRatio >= 0.7 && pitchRatio <= 1.3;
            if (pitchMatch) validChecksPass++;
            console.log(`[Voice Consistency] Pitch: ${pitchMatch ? 'MATCH' : 'MISMATCH'} (ratio: ${pitchRatio.toFixed(2)})`);
          } else {
            console.log('[Voice Consistency] Pitch: SKIPPED (data unavailable)');
          }
          
          // Check volume (within ¬±50%)
          // FIXED: Handle null/undefined volume values - if either is missing, skip volume check
          // Coercing null to 0 causes ratio to be 0, which fails the >=0.5 check incorrectly
          if (wakeFeatures.volume != null && confirmFeatures.volume != null && 
              wakeFeatures.volume > 0 && confirmFeatures.volume > 0) {
            hasValidCheck = true;
            totalValidChecks++;
            const volumeRatio = confirmFeatures.volume / wakeFeatures.volume;
            const volumeMatch = volumeRatio >= 0.5 && volumeRatio <= 1.5;
            if (volumeMatch) validChecksPass++;
            console.log(`[Voice Consistency] Volume: ${volumeMatch ? 'MATCH' : 'MISMATCH'} (ratio: ${volumeRatio.toFixed(2)})`);
          } else {
            console.log('[Voice Consistency] Volume: SKIPPED (data unavailable)');
          }
          
          // ENHANCED: Check MFCC similarity with Delta/Delta-Delta if available
          // FIXED: Validate that MFCC properties are arrays with valid length before calling calculateMFCCSimilarity
          if (wakeFeatures.mfcc && confirmFeatures.mfcc) {
            // Validate that both are arrays with length > 0
            const wakeMFCC = wakeFeatures.mfcc;
            const confirmMFCC = confirmFeatures.mfcc;
            
            if (Array.isArray(wakeMFCC) && Array.isArray(confirmMFCC) && 
                wakeMFCC.length > 0 && confirmMFCC.length > 0) {
              hasValidCheck = true;
              totalValidChecks++;
              try {
                // ENHANCED: Use enhanced MFCC similarity (supports Delta/Delta-Delta if available)
                // For wake phrase consistency, we may not have Delta/Delta-Delta yet, so use basic similarity
                const mfccSim = calculateMFCCSimilarity(wakeMFCC, confirmMFCC);
                const mfccMatch = mfccSim > 0.6; // 60% threshold for wake phrase consistency
                if (mfccMatch) validChecksPass++;
                console.log(`[Voice Consistency] MFCC: ${mfccMatch ? 'MATCH' : 'MISMATCH'} (similarity: ${mfccSim.toFixed(2)})`);
              } catch (e) {
                console.warn('[Voice Consistency] MFCC calculation failed:', e);
                // If MFCC calculation fails, don't count it as a valid check
                console.log('[Voice Consistency] MFCC: SKIPPED (calculation failed)');
              }
            } else {
              console.log('[Voice Consistency] MFCC: SKIPPED (data unavailable or invalid)');
            }
          } else {
            console.log('[Voice Consistency] MFCC: SKIPPED (data unavailable)');
          }
          
          // FIXED: Require at least one valid check to pass, and fail if no features are available
          // This prevents the security bypass where all checks default to true when features are unavailable
          if (!hasValidCheck) {
            console.log(`[Voice Consistency] FAILED: No valid voice features available for comparison`);
            return false;
          }
          
          // Require at least 50% of valid checks to pass (or all if only 1 check)
          const requiredPasses = totalValidChecks === 1 ? 1 : Math.ceil(totalValidChecks * 0.5);
          const isConsistent = validChecksPass >= requiredPasses;
          console.log(`[Voice Consistency] Result: ${isConsistent ? 'MATCH' : 'MISMATCH'} (${validChecksPass}/${totalValidChecks} checks passed, required: ${requiredPasses})`);
          return isConsistent;
        }

        recognition.onresult = (event) => {
          clearTimeout(silenceTimeout);
          
          // FIXED: Reset peak VAD at start of new speech segment
          // This ensures we track peak confidence for the current speech segment
          // Use peak instead of current to handle cases where VAD drops after speech ends
          // FIXED: Capture peak synchronously before reset to ensure we get the value
          // even if async VAD processing is still updating it (defensive programming)
          const peakVadForThisSegment = state.peakVadProb;
          // Reset immediately after capture to ensure clean state for next segment
          // Note: onresult fires after segment is finalized, so VAD processing should be complete
          state.peakVadProb = 0; // Reset for next segment
          
          // FIXED: Use peak VAD for better speech quality assessment
          // Peak VAD represents the highest confidence during the speech segment,
          // which is more reliable than current VAD that may drop after speech ends
          // Log it for debugging and potential future use in adaptive confidence thresholds
          if (peakVadForThisSegment > 0) {
            console.log(`[VAD] Peak VAD for segment: ${peakVadForThisSegment.toFixed(2)}`);
          }

          let finalTranscript = "";
          let interimTranscript = "";

          for (let i = event.resultIndex; i < event.results.length; ++i) {
            // --- 1. STRICT GARBAGE FILTERING (Confidence) ---
            // FIXED: Be more lenient - only ignore if confidence is very low AND volume is also very low
            const confidence = event.results[i][0].confidence;
            const volume = state.speakerProfile.recentMaxRMS || 0;
            const isVeryLowVolume = volume < 0.01; // Extremely low volume
            
            // Only ignore if BOTH confidence is very low (< 0.1) AND volume is very low
            // This prevents blocking valid speech that just has low confidence
            if (confidence < 0.1 && isVeryLowVolume) {
              console.log(`>> Low confidence result (${confidence.toFixed(2)}) with very low volume (${volume.toFixed(4)}), ignoring...`);
              continue;
            }
            
            // If confidence is low but volume is reasonable, still process it (might be valid speech)
            if (confidence < 0.6 && confidence >= 0.1 && !isVeryLowVolume) {
              console.log(`>> Moderate confidence (${confidence.toFixed(2)}) but reasonable volume (${volume.toFixed(4)}), processing...`);
            }

            if (event.results[i].isFinal) {
              finalTranscript += event.results[i][0].transcript;
            } else {
              interimTranscript += event.results[i][0].transcript;
            }
          }

          let fullTranscript = (finalTranscript + interimTranscript).trim().toLowerCase();

          // DEBUG: Log all transcripts for debugging
          if (finalTranscript) {
            console.log(`[WAKE PHRASE DEBUG] Final transcript: "${finalTranscript}" | isWakePhraseDetectionActive: ${isWakePhraseDetectionActive()} | waitingForWakePhrase: ${state.waitingForWakePhrase} | isCallActive: ${state.isCallActive}`);
          }

          // --- 1.5. WAKE PHRASE DETECTION (Only when call not active or re-auth mode) ---
          // FIXED: Check wake phrase detection FIRST, before blocking
          // This ensures wake phrases are detected even if waitingForWakePhrase is true
          if (isWakePhraseDetectionActive() && finalTranscript) {
            console.log(`[WAKE PHRASE DEBUG] Checking wake phrase for: "${fullTranscript}"`);
            // FIXED: Prevent concurrent intent checks from interfering with each other
            // If an intent check is already in progress, queue this transcript instead of dropping it
            // This prevents silent drops when multiple speech segments arrive in rapid succession
            if (state.pendingIntentCheck) {
              const MAX_QUEUE_SIZE = 5; // Limit queue size to prevent memory issues
              if (state.pendingWakePhraseQueue.length >= MAX_QUEUE_SIZE) {
                // Queue is full, remove oldest and add newest (keep most recent)
                state.pendingWakePhraseQueue.shift();
                console.log(`[WAKE PHRASE QUEUE] Queue full, removing oldest transcript`);
              }
              state.pendingWakePhraseQueue.push(fullTranscript);
              console.log(`[WAKE PHRASE QUEUE] Queued transcript (${state.pendingWakePhraseQueue.length} in queue): "${fullTranscript}"`);
              return;
            }
            
            // Step 1: Normalize transcript
            const normalizedTranscript = normalizeTextForWakePhrase(fullTranscript);
            console.log(`[WAKE PHRASE DEBUG] Normalized: "${normalizedTranscript}"`);
            
            // Step 2: Check keyword presence (Riya + keyword required)
            const hasRiyaKeyword = hasRiyaAndKeyword(normalizedTranscript);
            console.log(`[WAKE PHRASE DEBUG] Has Riya + keyword: ${hasRiyaKeyword}`);
            
            if (!hasRiyaKeyword) {
              // Missing Riya or keyword - not a wake phrase
              console.log(`[WAKE PHRASE DEBUG] Missing Riya or keyword, not a wake phrase`);
              // Continue with normal processing (don't return, let it fall through)
            } else {
              // Step 3: Fuzzy matching (fast path)
              const fuzzyMatch = fuzzyMatchWakePhrase(normalizedTranscript);
              console.log(`[WAKE PHRASE DEBUG] Fuzzy match result: ${fuzzyMatch}`);
              
              if (fuzzyMatch) {
                // Step 4: Clear intent verification (always run) - async, handle with promise
                // FIXED: Removed VAD confidence check for wake phrases
                // VAD is designed for continuous speech, not short wake phrases
                // Wake phrases are often spoken differently (louder/clearer) but VAD may not capture that
                // We already have fuzzy matching (80%+) and clear intent verification (LLM) as safeguards
                // VAD confidence check was blocking valid wake phrases unnecessarily
                state.pendingIntentCheck = true;
                const clearIntentStartTime = Date.now();
                checkClearIntent(fullTranscript).then(hasClearIntent => {
                  state.pendingIntentCheck = false;
                  const latency = Date.now() - clearIntentStartTime;
                  console.log(`[CLEAR INTENT] Latency: ${latency}ms for: "${fullTranscript}"`);
                  
                  if (hasClearIntent) {
                    console.log(`>> Clear intent verified for: "${fullTranscript}"`);
                    // FIXED: Clear queue when wake phrase is detected (no need to process queued transcripts)
                    state.pendingWakePhraseQueue = [];
                    handleWakePhraseDetected(fullTranscript);
                  } else {
                    console.log(`>> Wake phrase fuzzy match but no clear intent, rejecting: "${fullTranscript}"`);
                    // FIXED: Provide user feedback and keep recognition active for retry
                    handleWakePhraseRejection(fullTranscript);
                    // FIXED: Process queue after rejection (user might have queued better attempt)
                    processWakePhraseQueue();
                  }
                }).catch(err => {
                  state.pendingIntentCheck = false;
                  const latency = Date.now() - clearIntentStartTime;
                  console.warn(`[CLEAR INTENT] Check failed after ${latency}ms:`, err);
                  console.log(`>> Falling back to accepting wake phrase due to check error`);
                  // FIXED: If clear intent check fails (timeout/error), still accept the wake phrase
                  // since fuzzy match already passed
                  // FIXED: Clear queue when wake phrase is detected
                  state.pendingWakePhraseQueue = [];
                  handleWakePhraseDetected(fullTranscript);
                });
                return; // Don't process this transcript further, wait for async result
              } else {
                // Step 6: LLM intent check (fallback if no fuzzy match) - async
                // FIXED: Removed VAD confidence check here too (same reason as fuzzy match path)
                state.pendingIntentCheck = true;
                checkWakePhraseIntent(fullTranscript).then(llmIntent => {
                  if (llmIntent) {
                    // Clear intent verification (always run)
                    return checkClearIntent(fullTranscript);
                  } else {
                    // Not a wake phrase
                    return Promise.resolve(false);
                  }
                }).then(hasClearIntent => {
                  state.pendingIntentCheck = false;
                  // FIXED: hasClearIntent is the resolved boolean value from checkClearIntent()
                  // Explicitly check for true to avoid truthy issues with other values
                  if (hasClearIntent === true) {
                    console.log(`>> Clear intent verified for: "${fullTranscript}"`);
                    // FIXED: Clear queue when wake phrase is detected (no need to process queued transcripts)
                    state.pendingWakePhraseQueue = [];
                    handleWakePhraseDetected(fullTranscript);
                  } else {
                    console.log(`>> Wake phrase LLM intent but no clear intent, rejecting: "${fullTranscript}"`);
                    // FIXED: Provide user feedback and keep recognition active for retry
                    handleWakePhraseRejection(fullTranscript);
                    // FIXED: Process queue after rejection (user might have queued better attempt)
                    processWakePhraseQueue();
                  }
                }).catch(err => {
                  state.pendingIntentCheck = false;
                  console.warn('Wake phrase intent check failed:', err);
                  console.log(`>> Falling back to accepting wake phrase due to check error`);
                  // FIXED: If checks fail, still accept since LLM already detected intent
                  // FIXED: Clear queue when wake phrase is detected
                  state.pendingWakePhraseQueue = [];
                  handleWakePhraseDetected(fullTranscript);
                });
                return; // Don't process this transcript further, wait for async result
              }
            }
          }
          
          // Helper function to process queued wake phrase transcripts
          // FIXED: Process queue when pendingIntentCheck flag clears
          // This ensures transcripts that were blocked are not silently dropped
          function processWakePhraseQueue() {
            if (state.pendingWakePhraseQueue.length === 0) return;
            if (state.pendingIntentCheck) return; // Still processing, don't process queue yet
            if (!isWakePhraseDetectionActive()) {
              // Wake phrase detection no longer active, clear queue
              console.log(`[WAKE PHRASE QUEUE] Wake phrase detection no longer active, clearing queue`);
              state.pendingWakePhraseQueue = [];
              return;
            }
            
            // Process the queue (FIFO - first in, first out)
            // Process one at a time to prevent concurrent intent checks
            const queuedTranscript = state.pendingWakePhraseQueue.shift();
            if (queuedTranscript) {
              console.log(`[WAKE PHRASE QUEUE] Processing queued transcript: "${queuedTranscript}"`);
              
              // Re-trigger wake phrase detection for the queued transcript
              const normalizedTranscript = normalizeTextForWakePhrase(queuedTranscript);
              const hasRiyaKeyword = hasRiyaAndKeyword(normalizedTranscript);
              
              if (hasRiyaKeyword) {
                const fuzzyMatch = fuzzyMatchWakePhrase(normalizedTranscript);
                
                if (fuzzyMatch) {
                  // Process fuzzy match path
                  state.pendingIntentCheck = true;
                  const clearIntentStartTime = Date.now();
                  checkClearIntent(queuedTranscript).then(hasClearIntent => {
                    state.pendingIntentCheck = false;
                    const latency = Date.now() - clearIntentStartTime;
                    console.log(`[CLEAR INTENT] Latency: ${latency}ms for queued: "${queuedTranscript}"`);
                    
                    if (hasClearIntent) {
                      console.log(`>> Clear intent verified for queued: "${queuedTranscript}"`);
                      // FIXED: Clear queue when wake phrase is detected (no need to process remaining queued transcripts)
                      state.pendingWakePhraseQueue = [];
                      handleWakePhraseDetected(queuedTranscript);
                    } else {
                      console.log(`>> Wake phrase fuzzy match but no clear intent, rejecting queued: "${queuedTranscript}"`);
                      handleWakePhraseRejection(queuedTranscript);
                      // Process next item in queue if any
                      processWakePhraseQueue();
                    }
                  }).catch(err => {
                    state.pendingIntentCheck = false;
                    const latency = Date.now() - clearIntentStartTime;
                    console.warn(`[CLEAR INTENT] Check failed after ${latency}ms for queued:`, err);
                    console.log(`>> Falling back to accepting queued wake phrase due to check error`);
                    // FIXED: Clear queue when wake phrase is detected
                    state.pendingWakePhraseQueue = [];
                    handleWakePhraseDetected(queuedTranscript);
                  });
                } else {
                  // Process LLM intent check path
                  state.pendingIntentCheck = true;
                  checkWakePhraseIntent(queuedTranscript).then(llmIntent => {
                    if (llmIntent) {
                      return checkClearIntent(queuedTranscript);
                    } else {
                      return Promise.resolve(false);
                    }
                  }).then(hasClearIntent => {
                    state.pendingIntentCheck = false;
                    if (hasClearIntent === true) {
                      console.log(`>> Clear intent verified for queued: "${queuedTranscript}"`);
                      // FIXED: Clear queue when wake phrase is detected (no need to process remaining queued transcripts)
                      state.pendingWakePhraseQueue = [];
                      handleWakePhraseDetected(queuedTranscript);
                    } else {
                      console.log(`>> Wake phrase LLM intent but no clear intent, rejecting queued: "${queuedTranscript}"`);
                      handleWakePhraseRejection(queuedTranscript);
                      // Process next item in queue if any
                      processWakePhraseQueue();
                    }
                  }).catch(err => {
                    state.pendingIntentCheck = false;
                    console.warn('Wake phrase intent check failed for queued:', err);
                    console.log(`>> Falling back to accepting queued wake phrase due to check error`);
                    // FIXED: Clear queue when wake phrase is detected
                    state.pendingWakePhraseQueue = [];
                    handleWakePhraseDetected(queuedTranscript);
                  });
                }
              } else {
                // Not a wake phrase, skip and process next
                console.log(`[WAKE PHRASE QUEUE] Queued transcript is not a wake phrase, skipping: "${queuedTranscript}"`);
                processWakePhraseQueue();
              }
            }
          }
          
          // Helper function to handle wake phrase rejection (provide feedback and allow retry)
          function handleWakePhraseRejection(transcript) {
            console.log(`[WAKE PHRASE REJECTION] Providing feedback for: "${transcript}"`);
            
            // Determine language for feedback
            const isHindi = /namaste|kripya|apna|bataiye|karo|shuru|chahiye/i.test(transcript);
            const isHinglish = /counselling|start|hello|hi/i.test(transcript) && /karo|shuru|apna/i.test(transcript);
            
            let feedbackMessage = "I didn't catch that. Could you say 'I need counselling' again?";
            if (isHindi) {
              feedbackMessage = "Main samajh nahi payi. Kya aap phir se 'mujhe counselling chahiye' bol sakte hain?";
            } else if (isHinglish) {
              feedbackMessage = "Main samajh nahi payi. Kya aap phir se 'I need counselling' bol sakte hain?";
            }
            
            // Send feedback as agent_speak
            if (socket && socket.readyState === WebSocket.OPEN) {
              socket.send(JSON.stringify({ 
                type: 'agent_speak', 
                text: feedbackMessage
              }));
            }
            
            // FIXED: Keep recognition active after rejection - don't stop, allow immediate retry
            // Recognition should stay active so user can retry immediately
            console.log(`[WAKE PHRASE REJECTION] Keeping recognition active for retry`);
            
            // FIXED: Ensure recognition is active - better state checking
            // Recognition might already be started, so we just check and log
            // FIXED: Prevent race condition with recognition.onend by checking isRestartingRecognition flag
            setTimeout(() => {
              if (state.isCallActive && state.waitingForWakePhrase) {
                try {
                    // Check recognition state - use manual tracking as primary
                    if (recognition) {
                      // FIXED: Check if recognition is already restarting (prevents race with recognition.onend)
                      if (state.isRestartingRecognition) {
                        console.log(`[WAKE PHRASE REJECTION] Recognition restart already in progress, skipping duplicate start`);
                        return;
                      }
                      
                      if (state.recognitionIsActive) {
                        // Already active according to manual tracking
                        console.log(`[WAKE PHRASE REJECTION] Recognition already active (manual tracking), no restart needed`);
                      } else {
                        // Set flag to prevent duplicate starts from recognition.onend
                        state.isRestartingRecognition = true;
                        recognition.start();
                        // Note: recognitionIsActive will be set to true in onstart handler
                        // Note: isRestartingRecognition will be cleared in onstart handler
                        console.log(`[WAKE PHRASE REJECTION] Restarted recognition for retry`);
                      }
                    }
                } catch (e) {
                  // FIXED: Ignore "already started" errors gracefully - they're not real errors
                  if (e.message && e.message.includes('already started')) {
                    console.log(`[WAKE PHRASE REJECTION] Recognition already started, ignoring error`);
                    state.isRestartingRecognition = false; // Clear flag on error
                  } else {
                    console.warn(`[WAKE PHRASE REJECTION] Failed to restart recognition:`, e);
                    state.isRestartingRecognition = false; // Clear flag on error
                  }
                }
              }
            }, 500); // Wait 500ms after feedback to restart recognition
          }
          
          // Helper function to handle wake phrase detection
          function handleWakePhraseDetected(transcript) {
              // FIXED: Clear queue when wake phrase is detected (no need to process queued transcripts)
              // Wake phrase was successfully detected, so queued transcripts are no longer needed
              state.pendingWakePhraseQueue = [];
              
              // Store voice features from wake phrase
              const currentRMS = state.speakerProfile.recentMaxRMS || 0.01;
              const currentPitch = state.speakerProfile.pitchHistory.length > 0 
                ? state.speakerProfile.pitchHistory[state.speakerProfile.pitchHistory.length - 1] 
                : null;
              const currentTimbre = state.speakerProfile.timbreHistory.length > 0
                ? state.speakerProfile.timbreHistory[state.speakerProfile.timbreHistory.length - 1]
                : null;
              const currentMFCC = state.speakerProfile.mfccHistory.length > 0
                ? state.speakerProfile.mfccHistory[state.speakerProfile.mfccHistory.length - 1]
                : null;
              
              state.wakePhraseVoiceFeatures = {
                volume: currentRMS,
                pitch: currentPitch,
                timbre: currentTimbre,
                mfcc: currentMFCC
              };
              
              state.wakePhraseDetected = true;
              state.confirmationStep = true;
              
              // Determine language and ask confirmation question
              const isHindi = /namaste|kripya|apna|bataiye|karo|shuru|chahiye/i.test(transcript);
              const isHinglish = /counselling|start|hello|hi/i.test(transcript) && /karo|shuru|apna/i.test(transcript);
              
              // CONVERSATIONAL CONFIRMATION: Make it natural, not technical
              let confirmationQuestion = "Hi! What is your name?";
              if (isHindi) {
                confirmationQuestion = "Namaste! Aapka naam kya hai?";
              } else if (isHinglish) {
                confirmationQuestion = "Hello! Aapka naam kya hai?";
              }
              
              // FIXED: Send confirmation question as agent_speak (not user_text) so it's synthesized and played
              if (socket && socket.readyState === WebSocket.OPEN) {
                socket.send(JSON.stringify({ 
                  type: 'agent_speak', 
                  text: confirmationQuestion
                }));
              }
              
              console.log(`>> Wake phrase detected. Waiting for confirmation...`);
              
              // FIXED: Ensure recognition is active during confirmation step
              // Sometimes recognition stops after agent speech, so we need to restart it
              setTimeout(() => {
                if (state.confirmationStep && state.isCallActive) {
                  try {
                    // FIX: Check manual tracking first, then try to start if needed
                    if (state.recognitionIsActive) {
                      console.log(`[CONFIRMATION] Recognition already active (manual tracking)`);
                    } else {
                      recognition.start();
                      // Note: recognitionIsActive will be set to true in onstart handler
                      console.log(`[CONFIRMATION] Restarted recognition for confirmation step`);
                    }
                  } catch (e) {
                    // If already started, that's fine - update manual tracking
                    if (e.message && e.message.includes('already')) {
                      state.recognitionIsActive = true;
                      console.log(`[CONFIRMATION] Recognition already active - updated manual tracking`);
                    } else {
                      console.warn(`[CONFIRMATION] Failed to restart recognition:`, e);
                      // Retry once more after a delay
                      setTimeout(() => {
                        try {
                          recognition.start();
                          // Note: recognitionIsActive will be set to true in onstart handler
                          console.log(`[CONFIRMATION] Retry restart successful`);
                        } catch (e2) {
                          if (e2.message && e2.message.includes('already')) {
                            state.recognitionIsActive = true;
                            console.log(`[CONFIRMATION] Recognition already active (retry) - updated manual tracking`);
                          } else {
                            console.error(`[CONFIRMATION] Retry restart failed:`, e2);
                          }
                        }
                      }, 500);
                    }
                  }
                }
              }, 500); // Wait 500ms after agent speech to restart recognition
            }
          
          // --- 1.6. CONFIRMATION STEP HANDLING ---
          // FIXED: Process confirmation step BEFORE the blocking guard
          // This ensures confirmation responses are captured even if waitingForWakePhrase is true
          // FIXED: Block both final and interim transcripts during confirmation
          // Interim transcripts should not reach echo detection to prevent interference
          if (state.confirmationStep) {
            if (finalTranscript) {
              // Process final transcripts for confirmation
              console.log(`[CONFIRMATION DEBUG] Processing confirmation response: "${finalTranscript}"`);
            
            // Collect current voice features
            const currentRMS = state.speakerProfile.recentMaxRMS || 0.01;
            const currentPitch = state.speakerProfile.pitchHistory.length > 0 
              ? state.speakerProfile.pitchHistory[state.speakerProfile.pitchHistory.length - 1] 
              : null;
            const currentTimbre = state.speakerProfile.timbreHistory.length > 0
              ? state.speakerProfile.timbreHistory[state.speakerProfile.timbreHistory.length - 1]
              : null;
            const currentMFCC = state.speakerProfile.mfccHistory.length > 0
              ? state.speakerProfile.mfccHistory[state.speakerProfile.mfccHistory.length - 1]
              : null;
            
            const confirmFeatures = {
              volume: currentRMS,
              pitch: currentPitch,
              timbre: currentTimbre,
              mfcc: currentMFCC
            };
            
            console.log(`[CONFIRMATION DEBUG] Wake phrase features:`, state.wakePhraseVoiceFeatures);
            console.log(`[CONFIRMATION DEBUG] Confirmation features:`, confirmFeatures);
            
            // Check voice consistency
            const voiceMatch = checkVoiceConsistency(state.wakePhraseVoiceFeatures, confirmFeatures);
            console.log(`[CONFIRMATION DEBUG] Voice consistency check: ${voiceMatch ? 'MATCH' : 'MISMATCH'}`);
            
            if (voiceMatch) {
              // Lock initiator
              state.speakerProfile.callInitiatorDetected = true;
              state.speakerProfile.firstSpeakerDetected = true;
              // FIX #1: Log state change when confirmation step completes
              if (state.confirmationStep) {
                console.log(`[STATE CHANGE] Confirmation step passed - setting confirmationStep=false`);
              }
              state.confirmationStep = false;
              state.wakePhraseDetected = false;
              state.reAuthMode = false;
              state.waitingForWakePhrase = false; // Clear waiting state, truly start conversation
              
              // FIXED: Reset wrong lock detection flag when voice consistency check passes after re-authentication
              // This ensures the reset speaker button is hidden after successful re-identification
              state.wrongLockDetected = false;
              
              // FIXED: Reset mismatch tracking state when transitioning from wake phrase to active conversation
              // Any mismatches detected during wake phrase waiting should not persist into calibration phase
              state.mismatchCount = 0;
              state.lastMismatchTime = null;
              
              // CRITICAL FIX: Set initial baseline volume from wake phrase/confirmation volumes
              // This ensures baseline reflects the real caller's actual speaking volume
              // Use the higher of the two volumes (confirmation is usually more representative of normal speech)
              const wakeVolume = state.wakePhraseVoiceFeatures?.volume || 0.01;
              const confirmVolume = confirmFeatures.volume || 0.01;
              // FIXED: Use the higher of the two directly (removed incorrect * 0.5 multiplication)
              // Confirmation volume is usually more representative of normal speech, so we use it if higher
              const initialBaseline = Math.max(wakeVolume, confirmVolume);
              
              // Set initial baseline BEFORE calibration starts
              // This prevents calibration from using silence/background noise as baseline
              if (!state.speakerProfile.baselineVolume || state.speakerProfile.baselineVolume < initialBaseline * 0.5) {
                state.speakerProfile.baselineVolume = initialBaseline;
                console.log(`[BASELINE INIT] Set initial baseline from wake phrase/confirmation: ${initialBaseline.toFixed(4)} (wake: ${wakeVolume.toFixed(4)}, confirm: ${confirmVolume.toFixed(4)})`);
              }
              
              // Store collected name
              state.collectedName = finalTranscript.trim();
              
              // OPENING GREETING FLOW: After confirmation, send name and let LLM introduce naturally
              // The LLM prompt already instructs to introduce immediately, so we just send the name
              // and the LLM will respond with: "Hi! I'm Ayesha from the Admissions team. Got it, [name]! What is your phone number?"
              if (socket && socket.readyState === WebSocket.OPEN) {
                socket.send(JSON.stringify({ 
                  type: 'user_text', 
                  text: `My name is ${state.collectedName}`,
                  isSystemMessage: true 
                }));
                // Note: LLM will automatically introduce itself per prompt instructions
                // Prompt says: "Immediately introduce yourself and ask for their name" but since name is already collected,
                // LLM should acknowledge name and move to next field (phone number)
              }
              
              // Start calibration
              state.speakerProfile.calibrationStartTime = Date.now();
              state.speakerProfile.calibrationComplete = false;
              
              console.log(`>> Initiator locked! Name collected: ${state.collectedName}. Starting calibration with baseline: ${state.speakerProfile.baselineVolume.toFixed(4)}...`);
              
              // Don't process this transcript further (name already sent to LLM)
              return;
            } else {
              // Voice mismatch - ask to repeat wake phrase
              console.log(`>> Voice mismatch in confirmation. Asking to repeat...`);
              if (socket && socket.readyState === WebSocket.OPEN) {
                socket.send(JSON.stringify({ 
                  type: 'agent_speak', 
                  text: "I'm hearing multiple voices. Please say 'Riya start counselling' again to confirm it's you." 
                }));
              }
              // FIX #1: Log state change when confirmation fails
              if (state.confirmationStep) {
                console.log(`[STATE CHANGE] Confirmation failed - setting confirmationStep=false, waitingForWakePhrase=true`);
              }
              state.confirmationStep = false;
              state.wakePhraseDetected = false;
              // FIXED: Re-enable waiting for wake phrase so subsequent speech waits for re-entry
              state.waitingForWakePhrase = true;
              // FIXED: Reset mismatch tracking state when re-enabling wake phrase detection
              // Mismatches accumulated during confirmation should not persist into the next wake phrase attempt
              state.mismatchCount = 0;
              state.lastMismatchTime = null;
              return; // Reject this transcript
            }
            } else if (interimTranscript) {
              // FIXED: Block interim transcripts during confirmation to prevent echo detection interference
              // Interim transcripts should not be processed until they become final
              console.log(`[CONFIRMATION] Blocking interim transcript during confirmation: "${interimTranscript}"`);
              return; // Block interim transcripts
            }
          }
          
          // If waiting for wake phrase or in confirmation step, ignore all other transcripts
          // FIXED: Also block transcripts during confirmation step to prevent interim transcripts
          // and non-confirmation speech from being processed through echo detection and speaker filtering
          // The confirmation step handler at line 1469 will process the final confirmation response
          // and return, so final transcripts won't reach this guard
          if (state.waitingForWakePhrase || state.confirmationStep) {
            return; // Block all transcripts during wake phrase waiting or confirmation step
            // Note: Final transcripts during confirmation step are handled at line 1469 and return before reaching here
          }

          // --- 2. ECHO DETECTION (PRIORITY #1) ---
          // derived from user request: "Agent is hearing itself".
          // This must run BEFORE Voice ID, because Voice ID might accidentally accept the agent's own voice
          // if the volume is loud enough.
          
          // FIXED: During ending phase, allow "thank you" and similar ending phrases even if they match echo
          // These are legitimate user responses, not echo, and should trigger ending detection
          // Check this BEFORE echo detection blocks the transcript
          if (state.endingPhase && finalTranscript) {
            const lowerTranscript = finalTranscript.toLowerCase().trim();
            const endingPhrases = ['thank you', 'thanks', 'yes', 'correct', 'sahi hai', 'theek hai', 'okay', 'ok', 'sab sahi', 'everything correct', 'all correct'];
            const isEndingResponse = endingPhrases.some(phrase => lowerTranscript.includes(phrase));
            if (isEndingResponse) {
              // Allow ending responses even if they might match echo - these are legitimate user responses
              console.log(`[ENDING] Allowing ending response: "${finalTranscript}" (bypassing echo check during ending phase)`);
              // Continue to process this transcript normally - don't return here
            }
          }

          // FIXED: Store original values before any modifications for fallback checks
          // These are needed for checks at lines 974 and 1013 that occur after stripping
          const originalFullTranscript = fullTranscript;
          const originalFinalTranscript = finalTranscript;

          if (currentSpokenText) {
            // Helper to normalize text for comparison (remove punctuation, handle contractions)
            const normalizeText = (str) => {
              return str.toLowerCase()
                .replace(/[.,\/#!$%\^&\*;:{}=\-_`~()]/g, "") // Remove punctuation
                .replace(/\s{2,}/g, " ") // Collapse whitespace
                .replace(/\bi'm\b/g, "i am")
                .replace(/\byou're\b/g, "you are")
                .replace(/\bwe're\b/g, "we are")
                .replace(/\bthey're\b/g, "they are")
                .replace(/\bhe's\b/g, "he is")
                .replace(/\bshe's\b/g, "she is")
                .replace(/\bit's\b/g, "it is")
                .replace(/\bthat's\b/g, "that is")
                .replace(/\bcan't\b/g, "cannot")
                .replace(/\bwon't\b/g, "will not")
                .replace(/\bdon't\b/g, "do not")
                .trim();
            };

            const textToMatch = normalizeText(currentSpokenText);
            // FIXED: Use let instead of const so cleanInput can be updated after echo stripping
            let cleanInput = normalizeText(fullTranscript);
            const inputWordCount = cleanInput.split(' ').length;

            // FIXED: Always clear existing timeout before setting new one (prevents race conditions)
            if (echoTimeout) {
              clearTimeout(echoTimeout);
              echoTimeout = null;
            }
            // Set new timeout with proper cleanup
            echoTimeout = setTimeout(() => { 
              currentSpokenText = ""; 
              echoTimeout = null; // Clear reference
              console.log("Echo protection cleared (sustained)");
              // SAFETY CHECK: Verify recognition is still active when echo protection clears
              // CRITICAL: Always verify, don't trust manual tracking alone
              if (state.isCallActive && !state.endingPhase) {
                const timeSinceLastEnd = state.lastRecognitionEndTime > 0 
                  ? Date.now() - state.lastRecognitionEndTime 
                  : Infinity;
                
                if (!state.recognitionIsActive || timeSinceLastEnd < 8000) {
                  console.log(`[ECHO CLEARED] Verifying recognition is active (isActive: ${state.recognitionIsActive}, ended ${Math.round(timeSinceLastEnd/1000)}s ago)`);
                  ensureRecognitionActive();
                }
              }
            }, 6000);

            console.log(`[DEBUG] Checking Echo: Input="${cleanInput}" | Bot="${textToMatch}"`);

            // 1. STRICT SUBSTRING CHECKS (Aggressive - Always run this if we have active text)
            if (cleanInput.length > 0) {
              if (cleanInput.includes(textToMatch)) {
                const strippedInput = cleanInput.replace(textToMatch, "").trim();
                // Residue must be meaningful (> 4 chars like "hello", not just "hi" or noise)
                // If the user truly spoke over the bot, it would be a sentence.
                if (strippedInput.length > 4) {
                  console.log(`>> Mixed Input Detected. Stripping echo...`);
                  // FIXED: Preserve original formatting while stripping echo
                  // Issue: Previous approach normalized transcripts, losing capitalization, punctuation, and contractions
                  // Solution: Find echo in original transcripts using smart matching, then remove it while preserving formatting
                  
                  // Helper function to find and remove echo from original text while preserving formatting
                  const stripEchoPreservingFormat = (originalText, normalizedEcho) => {
                    if (!originalText || !normalizedEcho) return originalText;
                    
                    // Normalize original for comparison
                    const normalizedOriginal = normalizeText(originalText);
                    
                    // Find where echo appears in normalized text
                    const echoIndex = normalizedOriginal.indexOf(normalizedEcho);
                    if (echoIndex === -1) {
                      // Echo not found - return original unchanged
                      return originalText;
                    }
                    
                    // Split original text into words
                    const originalWordsOnly = originalText.split(/\s+/).filter(w => w.length > 0);
                    
                    // Build normalized version word-by-word to map positions correctly
                    // This handles contractions that expand to multiple words
                    const normalizedWords = [];
                    const wordMapping = []; // Maps normalized word index to original word index
                    
                    for (let i = 0; i < originalWordsOnly.length; i++) {
                      const normalizedWord = normalizeText(originalWordsOnly[i]);
                      const normalizedWordParts = normalizedWord.split(/\s+/).filter(w => w.length > 0);
                      
                      for (let j = 0; j < normalizedWordParts.length; j++) {
                        normalizedWords.push(normalizedWordParts[j]);
                        wordMapping.push(i); // Map back to original word index
                      }
                    }
                    
                    // Find echo in normalized word sequence
                    const echoWords = normalizedEcho.split(/\s+/).filter(w => w.length > 0);
                    let matchStart = -1;
                    
                    for (let i = 0; i <= normalizedWords.length - echoWords.length; i++) {
                      const slice = normalizedWords.slice(i, i + echoWords.length);
                      if (slice.join(' ') === normalizedEcho) {
                        matchStart = i;
                        break;
                      }
                    }
                    
                    if (matchStart === -1) {
                      return originalText; // No match found
                    }
                    
                    // Find which original words to remove
                    const firstEchoWordIndex = wordMapping[matchStart];
                    const lastEchoWordIndex = wordMapping[matchStart + echoWords.length - 1];
                    
                    // Reconstruct text without echo words
                    const beforeWords = originalWordsOnly.slice(0, firstEchoWordIndex);
                    const afterWords = originalWordsOnly.slice(lastEchoWordIndex + 1);
                    
                    return (beforeWords.join(' ') + ' ' + afterWords.join(' ')).trim().replace(/\s{2,}/g, ' ');
                  };
                  
                  // Strip echo from original transcripts while preserving formatting
                  finalTranscript = stripEchoPreservingFormat(finalTranscript, textToMatch);
                  interimTranscript = stripEchoPreservingFormat(interimTranscript, textToMatch);
                  
                  // Update fullTranscript with stripped version (normalized for downstream use)
                  fullTranscript = strippedInput;
                  
                  // FIXED: Recalculate cleanInput after stripping to reflect the stripped transcript
                  // Previous cleanInput was calculated before stripping and contains the echo
                  // Fuzzy match checks need to use the stripped version, not the stale pre-stripping value
                  cleanInput = strippedInput; // strippedInput is already normalized
                  
                  console.log(`[ECHO STRIP] Removed: "${textToMatch}" | Final: "${finalTranscript}" | Interim: "${interimTranscript}"`);
                } else {
                  console.log(`>> Echo BLOCKED (Exact match + Residue too short): "${cleanInput}"`);
                  return;
                }
              } else if (textToMatch.includes(cleanInput)) {
                // Check for very short common words that might be false positives if just "I" or "a"
                // FIXED: Be more lenient after agent speech ends (echo-aware window)
                const timeSinceAgentSpeech = Date.now() - state.lastAgentSpeechEndTime;
                const isInEchoWindow = timeSinceAgentSpeech < 3000; // 3 seconds after agent speech
                
                // If we're in echo window, be more lenient (require 4+ words instead of 2+)
                const wordThreshold = isInEchoWindow ? 4 : 2;

                if (inputWordCount > wordThreshold) {
                  console.log(`>> Echo BLOCKED (Substring match > ${wordThreshold} words): "${cleanInput}"`);
                  return;
                } else {
                  // CRITICAL FIX: If it matches the bot text, it MIGHT be an echo starting ("You...").
                  // If we let it pass now, the server will stop TTS.
                  // So, if it is INTERIM, we must WAIT. Only allow if FINAL.
                  // Because if it's real ("Completed 12th"), the user will finish and it'll become Final.
                  // If it's an echo ("You are welcome"), it will grow and eventually be Blocked > 2 words.

                  // We need to check if ANY result is final. 
                  // Logic: if 'finalTranscript' is empty, it means we are in interim state.
                  // FIXED: Use originalFinalTranscript before stripping, not modified finalTranscript
                  // After stripping, finalTranscript might be empty even if original had content
                  if (!originalFinalTranscript) {
                    console.log(`>> Echo Suspect (Interim): "${cleanInput}" -> Waiting...`);
                    return;
                  } else {
                    console.log(`>> Echo ALLOWED (Short Answer FINAL): "${cleanInput}"`);
                  }
                }
              }
            }

            // Fuzzy Match - Improved for Partial/Substring Echoes
            // FIXED: Use normalized cleanInput instead of non-normalized fullTranscript
            // Both arguments to levenshtein() must use the same normalization format
            if (cleanInput.length > 3) {
              const len = cleanInput.length;
              
              // 1. Standard Full Match (Good for full echoes)
              // FIXED: Use cleanInput (normalized) instead of fullTranscript (non-normalized)
              const distance = levenshtein(cleanInput, textToMatch);
              const maxLen = Math.max(cleanInput.length, textToMatch.length);
              const similarity = 1 - (distance / maxLen);

              // 2. Prefix Match (Good if echo captures start of bot speech)
              // Compare input vs Start of Bot Text
              // FIXED: Only extract prefix if bot text is long enough
              let prefixSim = 0;
              if (textToMatch.length >= len) {
                const prefixBot = textToMatch.substring(0, len);
                const prefixDist = levenshtein(cleanInput, prefixBot);
                prefixSim = 1 - (prefixDist / Math.max(len, prefixBot.length));
              }

              // 3. Suffix Match (Good if echo captures end of bot speech)
              // FIXED: Only extract suffix when bot text is long enough
              // When user input is longer than bot text, extracting suffix would return entire bot text
              // Example: bot="yes" (3), user="yes, I would like to" (20) -> would extract all of "yes" incorrectly
              let suffixSim = 0;
              if (textToMatch.length >= len) {
                const suffixBot = textToMatch.substring(textToMatch.length - len);
                const suffixDist = levenshtein(cleanInput, suffixBot);
                suffixSim = 1 - (suffixDist / Math.max(len, suffixBot.length));
              }

              // If ANY similarity is high (> 75%), block it.
              // FIXED: During ending phase, allow "thank you" and similar ending phrases even if they match echo
              // These are legitimate user responses during ending, not echo
              if (similarity > 0.6 || prefixSim > 0.75 || suffixSim > 0.75) {
                 const bestSim = Math.max(similarity, prefixSim, suffixSim);
                 
                 // Check if this is an ending response during ending phase
                 if (state.endingPhase) {
                   const lowerInput = cleanInput.toLowerCase().trim();
                   const endingPhrases = ['thank you', 'thanks', 'yes', 'correct', 'sahi hai', 'theek hai', 'okay', 'ok', 'sab sahi', 'everything correct'];
                   const isEndingResponse = endingPhrases.some(phrase => lowerInput.includes(phrase));
                   if (isEndingResponse) {
                     console.log(`>> Echo ALLOWED (Ending response during ending phase): "${cleanInput}"`);
                     // Don't return - allow this to proceed
                   } else {
                     console.log(`>> Echo BLOCKED (Fuzzy ${Math.round(bestSim * 100)}%): "${cleanInput}"`);
                     return;
                   }
                 } else {
                   console.log(`>> Echo BLOCKED (Fuzzy ${Math.round(bestSim * 100)}%): "${cleanInput}"`);
                   return;
                 }
              }
            } else if (originalFullTranscript.length > 3) {
              // Fallback: If cleanInput is too short but fullTranscript exists, normalize it properly
              // FIXED: Use originalFullTranscript before stripping, not modified fullTranscript
              // After stripping, fullTranscript might be too short even if original had enough content
              // FIXED: Use normalizeText() instead of just .toLowerCase() to match textToMatch format
              // (This should rarely happen, but provides a safety net)
              const normalizedFallback = normalizeText(originalFullTranscript);
              const distance = levenshtein(normalizedFallback, textToMatch);
              const maxLen = Math.max(normalizedFallback.length, textToMatch.length);
              const similarity = 1 - (distance / maxLen);
              
              if (similarity > 0.6) {
                console.log(`>> Echo BLOCKED (Fuzzy fallback ${Math.round(similarity * 100)}%): "${normalizedFallback}"`);
                return;
              }
            }
          }

          // --- 3. FILTER MICRO-HALLUCINATIONS & FALSE TRANSCRIPTS ---
          // CRITICAL FIX: Accept ALL responses from real callers, regardless of length
          // Let LLM decide relevance instead of blocking short answers
          // This ensures caller never feels stuck and can give natural short responses
          
          // Only filter if:
          // 1. Not a real caller (no voice match) AND
          // 2. Very low confidence AND
          // 3. Extremely short (1-2 chars) AND
          // 4. Not in allowed list
          const isRealCaller = state.speakerProfile.callInitiatorDetected && 
                              (!state.speakerProfile.lastMismatchReason || 
                               state.speakerProfile.lastMismatchReason.includes('None'));
          
          // Calculate average confidence
          let avgConfidence = 0;
          if (event.results && event.results.length > 0) {
            let totalConf = 0;
            let count = 0;
            for (let i = 0; i < event.results.length; i++) {
              if (event.results[i] && event.results[i][0] && event.results[i][0].confidence !== undefined) {
                totalConf += event.results[i][0].confidence;
                count++;
              }
            }
            avgConfidence = count > 0 ? totalConf / count : 0;
          }
          
          // CRITICAL: Real callers can say ANYTHING - accept all their responses
          // Only block obvious hallucinations from non-callers
          if (!isRealCaller && fullTranscript.length > 0 && fullTranscript.length <= 2) {
            // For non-callers, only block 1-2 char noise (unless it's a common word)
            const allowedShortWords = ['ok', 'no', 'hi', 'ya', 'yo', 'ye', 'ah', 'oh', 'um', 'uh', 'yes', 'yeah'];
            if (!allowedShortWords.includes(fullTranscript.toLowerCase())) {
              console.log(`>> Micro-hallucination blocked (non-caller): "${fullTranscript}"`);
              return;
            }
          }
          
          // For real callers: Accept ALL responses, even very short ones
          // The LLM will decide if it's relevant to the conversation
          // This ensures natural conversation flow - caller never feels stuck
          if (isRealCaller) {
            // Real caller - accept any response, regardless of length
            // Log short responses but don't block them
            if (fullTranscript.length <= 3) {
              console.log(`[ACCEPT] Short response from real caller: "${fullTranscript}" (confidence: ${avgConfidence.toFixed(2)}) - letting LLM decide relevance`);
            }
          } else if (fullTranscript.length > 0 && fullTranscript.length <= 5 && avgConfidence < 0.15) {
            // For non-callers: Only block extremely low confidence short transcripts
            // Increased threshold from 0.2 to 0.15 to be less aggressive
            const allowedShortLowConf = ['ok', 'no', 'yes', 'hi', 'ya', 'yeah', 'sure', 'fine'];
            if (!allowedShortLowConf.includes(fullTranscript.toLowerCase())) {
              console.log(`>> Low-confidence hallucination blocked (non-caller): "${fullTranscript}" (confidence: ${avgConfidence.toFixed(2)})`);
              return;
            }
          }

          // --- 3.5. MAIN SPEAKER IDENTIFICATION (Greeting Detection) ---
          // Detect greeting words to identify the call initiator/main speaker
          if (ENABLE_SPEAKER_FILTERING && !state.speakerProfile.callInitiatorDetected) {
            const greetingWords = ['hello', 'hi', 'hey', 'good morning', 'good afternoon', 'good evening', 'greetings'];
            const transcriptLower = fullTranscript.toLowerCase().trim();
            
            // Check if transcript contains greeting words
            for (const greeting of greetingWords) {
              if (transcriptLower.includes(greeting) || transcriptLower === greeting) {
                state.speakerProfile.callInitiatorDetected = true;
                console.log(`>> CALL INITIATOR DETECTED: "${fullTranscript}" (Greeting: "${greeting}")`);
                // Mark this speaker as the main speaker
                // The calibration will prioritize this speaker's voice features
                break;
              }
            }
          }

          // --- 4. SPEAKER SEPARATION & NOISE FILTERING (PHASE 3) ---
          // IMPROVED: Use recentMaxRMS as primary source (updated in real-time by processAudioForVAD)
          // If recentMaxRMS is suspiciously low, estimate from confidence as fallback
          let currentUtteranceRMS = state.speakerProfile.recentMaxRMS || 0;
          
          // Calculate max confidence for fallback estimation
          let maxConfidence = 0;
          for (let i = event.resultIndex; i < event.results.length; ++i) {
            if (event.results[i][0].confidence > maxConfidence) {
              maxConfidence = event.results[i][0].confidence;
            }
          }
          
          // IMPROVED: Better volume estimation strategy
          // If recentMaxRMS is too low (likely stale or not updated), estimate from confidence
          const baseline = state.speakerProfile.baselineVolume || 0.01;
          if (currentUtteranceRMS < baseline * 0.1 && maxConfidence > 0.5) {
            // Estimate volume from confidence: higher confidence = closer to baseline
            // Map confidence (0.5-1.0) to volume (0.3x-1.2x baseline)
            const confidenceFactor = 0.3 + (maxConfidence - 0.5) * 1.8; // 0.5->0.3x, 1.0->1.2x
            currentUtteranceRMS = baseline * confidenceFactor;
            console.log(`[VOLUME] Estimated from confidence: ${currentUtteranceRMS.toFixed(4)} (conf: ${maxConfidence.toFixed(2)})`);
          } else if (currentUtteranceRMS > 0) {
            // Use actual recentMaxRMS if available
            console.log(`[VOLUME] Using recentMaxRMS: ${currentUtteranceRMS.toFixed(4)}`);
          } else {
            // Last resort: assume moderate volume if we have no data
            currentUtteranceRMS = baseline * 0.7;
            console.log(`[VOLUME] Fallback to baseline*0.7: ${currentUtteranceRMS.toFixed(4)}`);
          }

          // --- SPEAKER FILTERING (Fast Client-Side Check) ---
          if (ENABLE_SPEAKER_FILTERING) {
            // SPECIAL CHECK: If Volume is LOW but VAD Confidence is HIGH (>0.85), 
            // the user is likely speaking quietly or calibration was too loud.
            // We should ACCEPT this and ADAPT the baseline down.
            let forceAcceptVolume = false;
            
            // NOTE: Only adapt on FINAL transcript to avoid adapting to interim echoes.
            // IMPROVED: More aggressive adaptation for main speaker
            if (finalTranscript && fullTranscript.split(' ').length > 1 && 
                state.speakerProfile.calibrationComplete) {
              // If volume is low but we have valid speech, adapt baseline down
              if (currentUtteranceRMS < (state.speakerProfile.baselineVolume * 0.2)) {
                console.log(">> Low Volume but Valid Speech detected. Force Adapting Baseline...");
                state.speakerProfile.baselineVolume = (state.speakerProfile.baselineVolume * 0.7) + (currentUtteranceRMS * 0.3); // rapid drop
                forceAcceptVolume = true;
              }
              // Also adapt if volume is moderately low (0.2x-0.4x) with high confidence
              else if (currentUtteranceRMS < (state.speakerProfile.baselineVolume * 0.4) && maxConfidence > 0.85) {
                console.log(">> Moderate Volume with High Confidence. Adapting Baseline...");
                state.speakerProfile.baselineVolume = (state.speakerProfile.baselineVolume * 0.9) + (currentUtteranceRMS * 0.1); // gradual drop
              }
            }

            // Fast multi-layer filtering (<5ms target)
            // FIXED: Use currentUtteranceRMS instead of potentially stale recentMaxRMS
            // FIXED: Use originalFullTranscript for biometric check to match audio analysis
            // Biometric analysis (processAudioForVAD) was done on original audio (before echo stripping),
            // so we should check the original transcript, not the stripped version
            const transcriptForBiometricCheck = originalFullTranscript || fullTranscript;
            if (!forceAcceptVolume && !shouldAcceptInput(transcriptForBiometricCheck, currentUtteranceRMS, currentSpokenText)) {
              console.log(`[TRANSCRIPT BLOCKED] Rejected: "${fullTranscript}" (Volume: ${currentUtteranceRMS.toFixed(4)}, RMS Baseline: ${state.speakerProfile.baselineVolume?.toFixed(4) || 'N/A'}, Confidence: ${maxConfidence.toFixed(2)})`);
              return; // Reject input
            }
            
            console.log(`[TRANSCRIPT ACCEPTED] "${fullTranscript}" (Volume: ${currentUtteranceRMS.toFixed(4)}, Confidence: ${maxConfidence.toFixed(2)})`);
          }

          // (Echo Detection moved to top)

          // Enable VAD once we detect speech
          if (interimTranscript && !vadEnabled) {
            vadEnabled = true;
            console.log("VAD: Enabled after speech detection");
          }

          // CRITICAL FIX: Enhanced barge-in detection
          // 1. Transcript-based barge-in (existing)
          // 2. VAD-based proactive barge-in (new - detects speech before transcript)
          if (finalTranscript || interimTranscript) {
            if (isPlaying || audioQueue.length > 0) {
              console.log(`[BARGE-IN] User speech detected via transcript - canceling agent audio`);
              cancelAudio();
            }
          }
          
          // PROACTIVE BARGE-IN: Use VAD to detect user speech during agent speech
          // This enables natural interruption before transcript is available
          // Check VAD probability in real-time (updated by processAudioForVAD)
          if (state.currentVadProb > 0.5 && (isPlaying || audioQueue.length > 0)) {
            // High VAD probability + agent speaking = user is interrupting
            // Only trigger if VAD is consistently high (not just a spike)
            if (!state.bargeInVadCount) state.bargeInVadCount = 0;
            state.bargeInVadCount++;
            
            // CRITICAL FIX: Update UI to listening mode IMMEDIATELY when barge-in detected
            // This makes the caller feel heard instantly, before transcript is available
            if (state.bargeInVadCount >= 2) {
              // Update UI early (at 2 frames) to show instant response
              if (state.status !== 'listening') {
                state.status = 'listening';
                updateUI(); // Instant UI update - caller sees they're being heard
                console.log(`[BARGE-IN] UI updated to listening mode (VAD prob: ${state.currentVadProb.toFixed(2)})`);
              }
            }
            
            // Require 3 consecutive high-VAD frames (~90ms) to confirm barge-in
            // This prevents false triggers from brief noise
            if (state.bargeInVadCount >= 3) {
              console.log(`[BARGE-IN] User speech detected via VAD (prob: ${state.currentVadProb.toFixed(2)}) - canceling agent audio`);
              cancelAudio();
              state.bargeInVadCount = 0; // Reset counter
            }
          } else {
            // Reset counter if VAD drops or agent stops speaking
            state.bargeInVadCount = 0;
          }

          // Silence Detection: VAD (fast) or Timer (fallback)
          if (finalTranscript || interimTranscript) {
            if (!vadModel) {
              // Fallback: Use 2-second timer if VAD didn't load
              clearTimeout(silenceTimeout);
              silenceTimeout = setTimeout(() => {
                console.log("Fallback timer: Silence detected -> Stop & Send");
                recognition.stop();
                state.status = 'processing';
                updateUI();
              }, SILENCE_THRESHOLD);
            }
            // If VAD is loaded, it handles silence detection automatically
          }

          if (finalTranscript) {
            // FIXED: Check for ending phase FIRST, before other checks
            // This ensures ending responses like "thank you" are detected even if they might match echo
            // During ending phase, echo detection will be more lenient for ending phrases
            // FIXED: Check ending keywords locally first to set ending phase immediately
            // This prevents race condition where server response arrives after we check the flag
            const lowerTranscript = finalTranscript.toLowerCase().trim();
            const endingKeywords = ['thank you', 'thanks', 'bye', 'goodbye', "that's all", "i'm done", 'done', 'finish', 'complete'];
            const hasEndingKeyword = endingKeywords.some(keyword => lowerTranscript.includes(keyword));
            
            if (hasEndingKeyword && !state.endingPhase) {
              // Optimistically set ending phase (will be verified by server)
              // This allows ending phase handling to work immediately without waiting for server response
              state.endingPhase = true;
              console.log(`[ENDING] Ending phase detected locally (will be verified by server): "${finalTranscript}"`);
            }
            
            // Still send to server for verification (async verification)
            checkEndingPhase(finalTranscript);
            
            // Check for wrong lock recovery triggers
            checkWrongLockRecovery();
            
            console.log(`[TRANSCRIPT] Sending to LLM: "${finalTranscript}"`);
            console.log(`[TRANSCRIPT DEBUG] Length: ${finalTranscript.length}, Confidence: ${maxConfidence?.toFixed(2) || 'N/A'}, Volume: ${currentUtteranceRMS?.toFixed(4) || 'N/A'}`);
            
            // FIXED: Reset follow-up timer when user responds
            // This prevents follow-ups from triggering when user is actively speaking
            // BUT: During ending phase, check if user confirmed before resetting
            if (state.lastAgentQuestion) {
              if (state.endingPhase) {
                // During ending phase, check if user confirmed
                const lowerTranscript = finalTranscript.toLowerCase().trim();
                const confirmationPhrases = ['yes', 'correct', 'sahi hai', 'theek hai', 'okay', 'ok', 'sab sahi', 'everything correct', 'all correct', 'thank you', 'thanks'];
                const isConfirmation = confirmationPhrases.some(phrase => lowerTranscript.includes(phrase));
                if (isConfirmation) {
                  console.log(`[ENDING] User confirmed: "${finalTranscript}" - resetting follow-up and preparing to end`);
                  state.lastAgentQuestion = null;
                  state.followUpStage = 0;
                  // Auto-end call after a short delay (3 seconds) to allow agent's closing message
                  setTimeout(() => {
                    if (state.endingPhase && state.isCallActive) {
                      console.log(`[ENDING] Auto-ending call after user confirmation`);
                      stopCall();
                    }
                  }, 3000); // 3 second grace period for agent's closing message
                }
              } else {
                // Normal conversation - reset follow-up timer
                state.lastAgentQuestion = null;
                state.followUpStage = 0;
                console.log(`[FOLLOW-UP] User responded, resetting follow-up timer`);
              }
            }
            
            // Track when user last responded (for follow-up detection)
            state.lastUserResponseTime = Date.now();
            
            // ENHANCED: Update conversation context for adaptive voice matching
            // Check if user is answering a question (within 5 seconds of last question)
            const timeSinceLastQuestion = state.lastAgentQuestion 
              ? Date.now() - state.lastAgentQuestion.timestamp 
              : Infinity;
            state.speakerProfile.conversationContext.isAnsweringQuestion = timeSinceLastQuestion < 5000; // 5 seconds
            state.speakerProfile.conversationContext.responseTiming = timeSinceLastQuestion;
            state.speakerProfile.conversationContext.lastQuestionTime = state.lastAgentQuestion?.timestamp || 0;
            
            // Calculate topic relevance (simple heuristic: if transcript contains question words, it's likely relevant)
            const questionWords = ['what', 'where', 'when', 'who', 'why', 'how', 'which', 'phone', 'number', 'name', 'course', 'budget', 'city', 'education'];
            const transcriptLower = finalTranscript.toLowerCase();
            const relevanceScore = questionWords.filter(word => transcriptLower.includes(word)).length / questionWords.length;
            state.speakerProfile.conversationContext.topicRelevance = Math.min(1.0, 0.5 + relevanceScore); // 0.5-1.0 range
            
            if (socket && socket.readyState === WebSocket.OPEN) {
              socket.send(JSON.stringify({ type: 'user_text', text: finalTranscript }));
            }
          }
        };
      }

      try { 
        recognition.start(); 
        // Note: recognitionIsActive will be set to true in onstart handler
      } catch (e) { 
        // If start fails, that's okay - onstart will set the flag when it actually starts
      }
    }

    function stopCall() {
      // FIX #5: Prevent stopCall() from running during recognition restart (race condition protection)
      if (state.isRestartingRecognition) {
        console.warn(`[STOP CALL] Blocked - recognition restart in progress. Will retry after restart completes.`);
        // Retry stopCall after a short delay
        setTimeout(() => {
          if (!state.isRestartingRecognition) {
            stopCall();
          } else {
            // Force stop if restart is taking too long (safety mechanism)
            console.warn(`[STOP CALL] Force stopping after restart delay`);
            state.isRestartingRecognition = false;
            stopCall();
          }
        }, 1000); // Wait 1 second for restart to complete
        return;
      }
      
      // FIX #1: Log when isCallActive is being set to false
      if (state.isCallActive) {
        console.log(`[STATE CHANGE] stopCall() called - setting isCallActive=false. Stack trace:`, new Error().stack);
      }
      state.isCallActive = false;
      // Logic to stop everything
      if (recognition) { recognition.stop(); }
      // Stop AEC Stream
      if (aecStream) { aecStream.getTracks().forEach(track => track.stop()); aecStream = null; }
      // Stop VAD Processing
      if (vadProcessor) { vadProcessor.disconnect(); vadProcessor = null; }
      if (vadContext) { vadContext.close(); vadContext = null; }
      consecutiveSilenceFrames = 0; // Reset VAD state
      vadEnabled = false; // Reset VAD activation
      cancelAudio();
      currentSpokenText = ""; // Clear echo protection on call end
      clearTimeout(echoTimeout); // Clear any pending echo timeout
      // FIXED: Reset lastSpokenTextForPause to prevent stale text from affecting pause timing in next call
      lastSpokenTextForPause = "";
      // FIXED: Clean up follow-up timer to prevent resource leaks
      if (followUpTimer) {
        clearInterval(followUpTimer);
        followUpTimer = null;
      }
      // FIX #4: Clean up recognition health check timer
      if (recognitionHealthCheckTimer) {
        clearInterval(recognitionHealthCheckTimer);
        recognitionHealthCheckTimer = null;
        console.log(`[HEALTH CHECK] Stopped recognition health check timer`);
      }
      // Reset follow-up state
      state.lastAgentQuestion = null;
      state.followUpStage = 0;
      state.lastUserResponseTime = null;
      // FIXED: Reset authentication state to prevent state pollution across calls
      // If a user ends a call during re-authentication or confirmation, these flags persist
      // and cause incorrect behavior in the next call
      state.reAuthMode = false;
      state.confirmationStep = false;
      state.wakePhraseDetected = false;
      state.wakePhraseVoiceFeatures = null;
      state.pendingIntentCheck = false; // Reset intent check flag
      state.pendingWakePhraseQueue = []; // Clear wake phrase queue on call end
      // FIXED: Reset waitingForWakePhrase to prevent blocking transcripts in the next call
      // If a user manually resets the speaker and ends the call without completing re-authentication,
      // waitingForWakePhrase remains true and blocks all transcripts in the next call
      state.waitingForWakePhrase = false;
      // FIXED: Reset ending phase flag to prevent ending detection from failing in subsequent calls
      // If ending phase is detected in one call, the flag persists and prevents ending detection in later calls
      state.endingPhase = false;
      // FIX #5: Reset restart flag when call ends
      state.isRestartingRecognition = false;
      // FIX: Reset manual recognition state tracking when call ends
      state.recognitionIsActive = false;
      state.lastRecognitionEndTime = 0;
      state.status = 'idle';
      updateUI();
    }

    // --- Proactive Follow-up Timer ---
    // FIXED: Store interval ID so we can clean it up
    // Timer is now created in startCall() instead of globally to ensure it's recreated on each call
    let followUpTimer = null;
    
      // --- Recognition Health Check Timer ---
      // FIX #4: Periodic health check to ensure recognition stays active during calls
      // ENHANCED: Also check during agent speech to ensure barge-in works
      // This proactively detects and fixes cases where recognition stops unexpectedly
      let recognitionHealthCheckTimer = null;
      
      function checkRecognitionHealth() {
        // Check if call is active OR if agent is speaking (need recognition for barge-in)
        const shouldBeActiveForHealthCheck = (state.isCallActive && !state.endingPhase) || 
                              (isPlaying || audioQueue.length > 0) ||
                              state.confirmationStep ||
                              state.waitingForWakePhrase;
        
        if (!shouldBeActiveForHealthCheck) {
          return; // Not in active call, agent not speaking, and not in special states
        }
      
      if (!recognition) {
        console.warn(`[HEALTH CHECK] Recognition object is null`);
        return;
      }
      
      // ENHANCED: Use the same check for health check logic
      // This ensures we check during agent speech too (for barge-in)
      
      if (shouldBeActiveForHealthCheck) {
        // Check if recognition ended recently (within last 8 seconds) but wasn't restarted
        // Increased from 5 to 8 seconds to cover the 6-second echo protection window + buffer
        const timeSinceLastEnd = state.lastRecognitionEndTime > 0 
          ? Date.now() - state.lastRecognitionEndTime 
          : Infinity;
        
        // ENHANCED: More aggressive health check during agent speech (for barge-in)
        // During agent speech, check more frequently and restart immediately if needed
        const isAgentSpeaking = isPlaying || audioQueue.length > 0;
        const healthCheckWindow = isAgentSpeaking ? 3000 : 8000; // 3s during agent speech, 8s otherwise
        
        // Only check if recognition is not active AND it ended recently
        // This prevents checking when recognition is already active or when it ended long ago
        if (!state.recognitionIsActive && timeSinceLastEnd < healthCheckWindow) {
          const reason = isAgentSpeaking ? "agent speaking (barge-in needed)" : "call active";
          console.warn(`[HEALTH CHECK] Recognition ended ${Math.round(timeSinceLastEnd/1000)}s ago but not restarted - ensuring active (${reason})`);
          ensureRecognitionActive();
        } else if (state.recognitionIsActive) {
          // Recognition is active according to manual tracking
          // Optionally verify with recognition.state if available (fallback check)
          if ('state' in recognition) {
            const recognitionState = recognition.state;
            if (recognitionState === 'stopped' || recognitionState === 'inactive') {
              // State mismatch - manual tracking says active but recognition.state says inactive
              console.warn(`[HEALTH CHECK] State mismatch - manual tracking says active but recognition.state is ${recognitionState}`);
              state.recognitionIsActive = false;
              ensureRecognitionActive();
            }
            // If recognitionState is 'starting' or 'started', all good
            // If recognitionState is undefined, trust manual tracking
          }
          // If recognition.state is not available, trust manual tracking (no check needed)
        }
        // If recognition ended more than 5 seconds ago and is not active, don't check
        // This prevents unnecessary checks when recognition was intentionally stopped
      }
    }
    
    function checkProactiveFollowUp() {
      if (!state.isCallActive || state.waitingForWakePhrase || state.endingPhase) {
        return; // Not in active conversation or already ending
      }
      
      if (!state.lastAgentQuestion) {
        return; // No question to follow up on
      }
      
      const timeSinceQuestion = Date.now() - state.lastAgentQuestion.timestamp;
      
      // Only trigger follow-up if user hasn't responded recently (at least 3 seconds since last response)
      const timeSinceLastResponse = state.lastUserResponseTime ? (Date.now() - state.lastUserResponseTime) : Infinity;
      
      // THINKING PAUSE DETECTION: Distinguish thinking (3-5s) vs genuine silence (10s+)
      // If user responded recently (within 5s), they might be thinking - don't interrupt
      const isThinkingPause = timeSinceLastResponse < 5000 && timeSinceQuestion >= 3000 && timeSinceQuestion < 10000;
      if (isThinkingPause) {
        // User is likely thinking - don't interrupt yet
        return;
      }
      
      if (state.followUpStage === 0 && timeSinceQuestion >= 10000 && timeSinceLastResponse >= 3000) {
        // Stage 1: 10 seconds - Ask "Are you there?" or "Did you hear me?" (more natural wait time)
        // Only if user hasn't responded in the last 3 seconds (to avoid interrupting them)
        state.followUpAlternate = !state.followUpAlternate;
        const followUpMessage = state.followUpAlternate ? "Are you there?" : "Did you hear me?";
        
        console.log(`[FOLLOW-UP] Stage 1 (10s): ${followUpMessage} (timeSinceLastResponse: ${timeSinceLastResponse}ms)`);
        
        // FIXED: Send follow-up as agent_speak, not user_text
        // Sending as user_text makes LLM think user said it, causing agent to respond incorrectly
        // Also, do NOT update lastAgentQuestion - this is a follow-up, not a new question
        if (socket && socket.readyState === WebSocket.OPEN) {
          socket.send(JSON.stringify({ 
            type: 'agent_speak', 
            text: followUpMessage
          }));
        }
        
        state.followUpStage = 1;
        // FIXED: Reset timestamp for Stage 2, but keep the original question text
        // This ensures we track time since the original question, not the follow-up
        if (state.lastAgentQuestion) {
          state.lastAgentQuestion.timestamp = Date.now();
        }
        
      } else if (state.followUpStage === 1 && timeSinceQuestion >= 5000) {
        // Stage 2: Another 5 seconds - Move to next question (more natural wait after follow-up)
        // FIXED: Don't move to next question if we're in ending phase - just end the call
        if (state.endingPhase) {
          console.log(`[FOLLOW-UP] Stage 2 (5s): Ending phase detected, not moving to next question - ending call`);
          // Reset follow-up tracking and let ending phase handle it
          state.lastAgentQuestion = null;
          state.followUpStage = 0;
          return;
        }
        
        console.log(`[FOLLOW-UP] Stage 2 (5s): Moving to next question`);
        
        // FIXED: Send as system message to tell LLM to move on, not as user message
        // This prevents the LLM from thinking the user said something
        if (socket && socket.readyState === WebSocket.OPEN) {
          socket.send(JSON.stringify({ 
            type: 'user_text', 
            text: "User did not respond. Please move to the next question naturally.",
            isSystemMessage: true 
          }));
        }
        
        // Reset follow-up tracking
        state.lastAgentQuestion = null;
        state.followUpStage = 0;
      }
    }
    
    // FIXED: Timer is now created in startCall() instead of globally
    // This ensures it's recreated on each new call after stopCall() clears it
    // Removed global initialization to prevent timer from running before call starts

    // --- Event Listeners ---
    startBtn.addEventListener('click', startCall);
    endBtn.addEventListener('click', stopCall);
    resetSpeakerBtn.addEventListener('click', () => {
      // FIXED: Only allow reset during active call to prevent state pollution
      // If call is not active, state modifications persist into next call, causing incorrect behavior
      // like blocking all transcripts (waitingForWakePhrase) or preventing normal conversation (reAuthMode)
      if (!state.isCallActive) {
        console.warn(`[RESET SPEAKER] Call is not active - ignoring reset request to prevent state pollution`);
        return;
      }
      
      // Manual override: reset speaker lock
      state.wrongLockDetected = false;
      state.reAuthMode = true;
      state.speakerProfile.calibrationComplete = false;
      state.speakerProfile.callInitiatorDetected = false;
      state.mismatchCount = 0;
      state.lastMismatchTime = null;
      // FIXED: Set waitingForWakePhrase to true to block all non-wake-phrase transcripts
      // Without this, transcripts will be processed normally after reset, allowing unauthorized
      // speakers to continue the conversation and defeating the re-identification flow
      state.waitingForWakePhrase = true;
      state.wakePhraseDetected = false;
      state.confirmationStep = false;
      console.log(`>> Manual speaker reset triggered`);
      
      // Ask user to repeat wake phrase
      if (socket && socket.readyState === WebSocket.OPEN) {
        socket.send(JSON.stringify({ 
          type: 'agent_speak', 
          text: "Please say 'Riya start counselling' again to re-identify yourself." 
        }));
      }
      
      updateUI();
    });

    // --- Silero VAD Functions ---
    async function initVAD() {
      try {
        console.log("Loading Silero VAD model...");
        // Use a more reliable CDN for the model
        vadModel = await ort.InferenceSession.create(
          "https://cdn.jsdelivr.net/npm/@ricky0123/vad-web@0.0.7/dist/silero_vad.onnx"
        );

        console.log("VAD Model Input Names:", vadModel.inputNames);

        // Initialize State based on model version
        if (vadModel.inputNames.includes('state')) {
          // V5 model
          const h = new ort.Tensor("float32", new Float32Array(2 * 1 * 128).fill(0), [2, 1, 128]);
          vadState = { version: 5, h };
          console.log("Silero VAD V5 loaded successfully");
        } else {
          // V4 model (uses h and c separately, shape [2, 1, 64])
          const h = new ort.Tensor("float32", new Float32Array(2 * 1 * 64).fill(0), [2, 1, 64]);
          const c = new ort.Tensor("float32", new Float32Array(2 * 1 * 64).fill(0), [2, 1, 64]);
          vadState = { version: 4, h, c };
          console.log("Silero VAD V4 loaded successfully");
        }
      } catch (err) {
        console.warn("Could not load VAD model, falling back to timer:", err);
      }
    }

    async function processAudioForVAD(audioData) {
      if (!vadModel || !state.isCallActive) return;

      // FIXED: Clear lastMismatchReason at start of each frame to prevent stale values
      // The mismatch reason is only set inside the biometric check block when certain conditions are met
      // If those conditions aren't met, it would retain the previous value, causing incorrect rejections
      // Clearing it here ensures shouldAcceptInput only uses current-frame mismatch data
      if (ENABLE_SPEAKER_FILTERING) {
        state.speakerProfile.lastMismatchReason = null;
      }

      try {
        if (!processAudioForVAD.analyser) {
          // Setup AnalyserNode once if not already available (requires access to AudioContext)
          // Note: We need the original AudioContext from setupVADAudioProcessing to create an Analyser
          // Since we are inside a function without direct access to the context variable 'vadContext',
          // we will rely on a global or attached property. 
          // IMPROVEMENT: We'll modify setupVADAudioProcessing to attach the analyser to 'state'.
        }

        // --- VAD PROCESSING (Run first to get speechProb) ---
        const audioTensor = new ort.Tensor("float32", audioData, [1, audioData.length]);
        const sr = new ort.Tensor("int64", BigInt64Array.from([BigInt(vadSampleRate)]));

        let inputs = {};
        if (vadState.version === 5) {
          inputs = { input: audioTensor, state: vadState.h, sr: sr };
        }
        else {
          inputs = { input: audioTensor, h: vadState.h, c: vadState.c, sr: sr };
        }

        const output = await vadModel.run(inputs);
        const speechProb = output.output.data[0];
        //----------------------------------------

        // --- FREQUENCY ANALYSIS (Voice ID Lite) - OPTIMIZED FOR LATENCY ---
        // Skip expensive calculations if not needed (frame skipping for non-critical frames)
        let currentPitch = 0;
        let currentCentroid = 0;

        // Only calculate features every N frames or if speech is detected (reduces CPU by ~70%)
        // Now we can safely use speechProb since VAD has run
        const shouldCalculateFeatures = ENABLE_SPEAKER_FILTERING && state.isCallActive && 
          (speechProb > 0.3 || !state.speakerProfile.calibrationComplete || (state.speakerProfile.frameCount % 3 === 0));

        if (shouldCalculateFeatures) {
          // Fast path: Skip pitch if RMS is too low (saves ~2ms per frame)
          const quickRMS = calculateRMSFast(audioData);
          if (quickRMS > 0.01) {
            currentPitch = calculatePitchFast(audioData, vadSampleRate);
          }
          
          // Enhanced: Use proper spectral centroid (FFT-based) instead of just ZCR
          // Fallback to ZCR for very low latency, but calculate proper centroid periodically
          if (state.speakerProfile.frameCount % 2 === 0) {
            // Calculate proper spectral centroid every other frame (balances accuracy vs latency)
            currentCentroid = calculateSpectralCentroid(audioData, vadSampleRate);
          } else {
            // Fast ZCR fallback for intermediate frames
            currentCentroid = calculateZeroCrossingRate(audioData);
          }
        }
        //----------------------------------------

        // --- VOLUME ANALYSIS & SPEAKER PROFILING (Low Latency) ---
        if (ENABLE_SPEAKER_FILTERING && state.isCallActive) {
          // Use fast RMS for real-time path
          const currentRMS = calculateRMSFast(audioData);

          // Calibration Phase (First ~3-4 seconds)
          // Only calibrate if speaker filtering is enabled
          if (ENABLE_SPEAKER_FILTERING && !state.speakerProfile.calibrationComplete) {
            // Only calibrate on speech (VAD > 0.5) to avoid silence skewing baseline
            // CRITICAL FIX: Only calibrate if the AGENT IS NOT SPEAKING (Prevent Echo Calibration)
            if (speechProb > 0.5 && state.status !== 'speaking' && !isPlaying) {
              
              // FIXED: Don't mark first speaker detected until AFTER confirmation step passes
              // This prevents premature calibration and profile creation
              // During wake phrase waiting, we're just collecting voice features, not calibrating
              if (!state.waitingForWakePhrase && !state.confirmationStep) {
                // Only detect first speaker if we're past wake phrase and confirmation
                if (!state.speakerProfile.firstSpeakerDetected) {
                  state.speakerProfile.firstSpeakerDetected = true;
                  state.speakerProfile.firstSpeakerVolume = currentRMS;
                  state.speakerProfile.firstSpeakerPitch = currentPitch > 50 && currentPitch < 400 ? currentPitch : null;
                  state.speakerProfile.firstSpeakerTimbre = currentCentroid > 0 ? currentCentroid : null;
                  console.log(`>> FIRST SPEAKER DETECTED: Volume=${currentRMS.toFixed(4)}, Pitch=${currentPitch.toFixed(0)}Hz, Timbre=${currentCentroid.toFixed(3)}`);
                }
              } else if (state.waitingForWakePhrase || state.confirmationStep) {
                // During wake phrase/confirmation, just track voice features but don't mark as "first speaker"
                // This prevents premature calibration
                if (!state.speakerProfile.firstSpeakerVolume) {
                  state.speakerProfile.firstSpeakerVolume = currentRMS;
                  state.speakerProfile.firstSpeakerPitch = currentPitch > 50 && currentPitch < 400 ? currentPitch : null;
                  state.speakerProfile.firstSpeakerTimbre = currentCentroid > 0 ? currentCentroid : null;
                  // Don't log "FIRST SPEAKER DETECTED" - we're just collecting features
                }
              }
              
              // CRITICAL FIX: Collect voice features BEFORE early return checks
              // Features (pitch, timbre, MFCC) are needed for voice consistency check during wake phrase/confirmation
              // Even if we skip calibration, we MUST collect features so they're available when wake phrase is detected
              
              // Increment frame count first (needed for MFCC collection timing)
              state.speakerProfile.frameCount++;
              
              // Collect voice features for fingerprinting (pre-filtered during collection)
              // These features are needed for voice consistency check, so collect them even during wake phrase/confirmation
              if (currentPitch > 50 && currentPitch < 400) {
                state.speakerProfile.pitchHistory.push(currentPitch);
              }
              if (currentCentroid > 0) {
                state.speakerProfile.timbreHistory.push(currentCentroid);
                // Also collect proper spectral centroid if calculated
                if (state.speakerProfile.frameCount % 2 === 0) {
                  state.speakerProfile.spectralCentroidHistory.push(currentCentroid);
                }
              }
              
              // ENHANCED: Collect MFCC features more frequently (every 2-3 frames for better availability)
              // Changed from every 5 frames to every 2-3 frames to increase availability from 20% to ~40-50%
              // CRITICAL: Collect MFCC even during wake phrase/confirmation - needed for voice consistency check
              const mfccCollectionInterval = 3; // Every 3 frames (~90ms) - balance between accuracy and performance
              if (state.speakerProfile.frameCount % mfccCollectionInterval === 0 && speechProb > 0.5) {
                const mfccFeatures = calculateMFCC(audioData, vadSampleRate);
                if (mfccFeatures && mfccFeatures.length > 0) {
                  // Apply CMN (Cepstral Mean Normalization) and CVN (Cepstral Variance Normalization)
                  const normalizedMFCC = normalizeMFCC(mfccFeatures, state.speakerProfile.mfccHistory);
                  
                  // Store normalized MFCC
                  state.speakerProfile.mfccHistory.push(mfccFeatures); // Keep original for baseline calculation
                  state.speakerProfile.mfccNormalizedHistory.push(normalizedMFCC);
                  
                  // Calculate Delta coefficients (first derivative - temporal dynamics)
                  if (state.speakerProfile.mfccHistory.length >= 2) {
                    const delta = calculateDeltaCoefficients(
                      state.speakerProfile.mfccHistory.slice(-2)
                    );
                    if (delta) {
                      state.speakerProfile.mfccDeltaHistory.push(delta);
                    }
                  }
                  
                  // Calculate Delta-Delta coefficients (second derivative - acceleration)
                  if (state.speakerProfile.mfccDeltaHistory.length >= 2) {
                    const deltaDelta = calculateDeltaCoefficients(
                      state.speakerProfile.mfccDeltaHistory.slice(-2)
                    );
                    if (deltaDelta) {
                      state.speakerProfile.mfccDeltaDeltaHistory.push(deltaDelta);
                    }
                  }
                  
                  // ENHANCED: Temporal analysis for noise detection
                  // Speech has temporal patterns (phonemes, formants), noise is more random
                  if (state.speakerProfile.mfccHistory.length > 5) {
                    const isSpeechLike = analyzeTemporalPatterns(state.speakerProfile.mfccHistory);
                    if (!isSpeechLike) {
                      console.log(`[TEMPORAL ANALYSIS] Possible noise detected (low temporal pattern consistency)`);
                    }
                  }
                }
              }
              
              // FIXED: Don't calibrate during wake phrase waiting or confirmation step
              // Calibration should only happen AFTER confirmation step passes
              // During wake phrase/confirmation, we're just collecting voice features for comparison
              // BUT: We still collect features above so they're available for voice consistency check
              if (state.waitingForWakePhrase || state.confirmationStep) {
                // Skip calibration during wake phrase/confirmation - just collect features for voice consistency check
                // Don't add to calibration history, don't calculate baselines, don't save voiceprint
                // Features are already collected above, so we can return now
                return; // Exit early, don't process calibration
              }
              
              // --- VOLUME-WEIGHTED CALIBRATION ---
              // Prioritize louder speakers (likely closer/main speaker) over background speakers
              // Weight = (currentRMS / maxRMS) ^ 2, so louder samples have exponentially more weight
              const maxRMS = Math.max(...state.speakerProfile.volumeHistory, currentRMS, 0.01);
              const volumeWeight = Math.pow(currentRMS / maxRMS, 2);
              
              // --- PRIORITIZE FIRST SPEAKER OR CALL INITIATOR ---
              // If we detected a greeting (call initiator) or this is the first speaker,
              // give their samples higher weight during calibration
              let speakerWeight = 1.0;
              // FIXED: Consistent threshold calculation - use firstSpeakerVolume if available and > 0,
              // otherwise use currentRMS as fallback. Previous implementation used 1 as fallback,
              // causing threshold to jump from 0.0005 (for 0.001 volume) to 0.5 (for null/0),
              // leading to inconsistent classification of the same voice at different volumes.
              const firstSpeakerVol = state.speakerProfile.firstSpeakerVolume;
              const volumeThreshold = (firstSpeakerVol && firstSpeakerVol > 0) 
                ? firstSpeakerVol * 0.5 
                : currentRMS * 0.5;
              const isFirstSpeaker = state.speakerProfile.firstSpeakerDetected && 
                Math.abs(currentRMS - (firstSpeakerVol || 0)) < volumeThreshold &&
                (state.speakerProfile.firstSpeakerPitch === null || Math.abs(currentPitch - (state.speakerProfile.firstSpeakerPitch || 0)) < 50);
              
              if (state.speakerProfile.callInitiatorDetected || isFirstSpeaker) {
                // Boost weight for main speaker (call initiator or first speaker)
                speakerWeight = 2.0;
                console.log(`[CALIBRATION] Main speaker sample (weight: ${speakerWeight.toFixed(2)})`);
              } else if (currentRMS < (state.speakerProfile.firstSpeakerVolume || currentRMS) * 0.3) {
                // Reduce weight for very quiet speakers (likely background)
                speakerWeight = 0.3;
                console.log(`[CALIBRATION] Background speaker sample (weight: ${speakerWeight.toFixed(2)})`);
              }
              
              const finalWeight = volumeWeight * speakerWeight;
              
              // Store weighted samples (only during calibration, not during wake phrase/confirmation)
              state.speakerProfile.volumeHistory.push(currentRMS);
              state.speakerProfile.weightedVolumeHistory.push({ rms: currentRMS, weight: finalWeight });
              
              // Add to weighted history for pitch and timbre (only during calibration)
              // Note: Features are already in pitchHistory/timbreHistory from above
              if (currentPitch > 50 && currentPitch < 400) {
                state.speakerProfile.weightedPitchHistory.push({ pitch: currentPitch, weight: finalWeight });
              }
              if (currentCentroid > 0) {
                state.speakerProfile.weightedTimbreHistory.push({ timbre: currentCentroid, weight: finalWeight });
              }

              if (state.speakerProfile.frameCount >= BASELINE_CALIBRATION_FRAMES) {
                // IMPROVED: Volume-weighted calibration calculation
                // Use weighted average to prioritize main speaker (first speaker or call initiator)
                const volLen = state.speakerProfile.weightedVolumeHistory.length;
                let volSum = 0;
                let weightSum = 0;
                
                if (volLen > 0) {
                  // Weighted average: sum(volume * weight) / sum(weight)
                  for (let i = 0; i < volLen; i++) {
                    const sample = state.speakerProfile.weightedVolumeHistory[i];
                    volSum += sample.rms * sample.weight;
                    weightSum += sample.weight;
                  }
                  const calculatedBaseline = weightSum > 0 ? volSum / weightSum : 
                    state.speakerProfile.volumeHistory.reduce((a, b) => a + b, 0) / state.speakerProfile.volumeHistory.length;
                  
                  // CRITICAL FIX: Don't let calibration reduce baseline below initial baseline from wake phrase/confirmation
                  // This prevents calibration from using silence/background noise to lower the baseline
                  // If we have an initial baseline (from wake phrase/confirmation), use the higher of the two
                  const currentBaseline = state.speakerProfile.baselineVolume || calculatedBaseline;
                  const initialBaseline = state.wakePhraseVoiceFeatures?.volume || 0;
                  
                  // Use the higher of calculated baseline or initial baseline (but don't go too low)
                  // This ensures baseline reflects actual speech volume, not silence
                  state.speakerProfile.baselineVolume = Math.max(
                    calculatedBaseline,
                    currentBaseline * 0.7, // Don't drop more than 30% from current
                    initialBaseline * 0.5   // Don't go below 50% of initial wake phrase volume
                  );
                  
                  console.log(`[CALIBRATION] Baseline volume calculated: ${state.speakerProfile.baselineVolume.toFixed(4)} (calculated: ${calculatedBaseline.toFixed(4)}, initial: ${initialBaseline.toFixed(4)}, weighted: ${volLen > 0})`);
                } else {
                  // Fallback to simple average if no weighted samples
                  volSum = 0;
                  for (let i = 0; i < state.speakerProfile.volumeHistory.length; i++) {
                    volSum += state.speakerProfile.volumeHistory[i];
                  }
                  const calculatedBaseline = volSum / state.speakerProfile.volumeHistory.length;
                  
                  // Same protection: don't let baseline drop too low
                  const currentBaseline = state.speakerProfile.baselineVolume || calculatedBaseline;
                  const initialBaseline = state.wakePhraseVoiceFeatures?.volume || 0;
                  
                  state.speakerProfile.baselineVolume = Math.max(
                    calculatedBaseline,
                    currentBaseline * 0.7,
                    initialBaseline * 0.5
                  );
                  
                  console.log(`[CALIBRATION] Baseline volume calculated: ${state.speakerProfile.baselineVolume.toFixed(4)} (calculated: ${calculatedBaseline.toFixed(4)}, initial: ${initialBaseline.toFixed(4)})`);
                }

                // IMPROVED: Weighted pitch calculation (prioritize main speaker)
                const validPitches = state.speakerProfile.pitchHistory;
                const weightedPitches = state.speakerProfile.weightedPitchHistory;
                
                if (weightedPitches.length > 0) {
                  // Weighted median: sort by pitch, then find weighted median
                  const sorted = weightedPitches.slice().sort((a, b) => a.pitch - b.pitch);
                  let weightSum = 0;
                  const totalWeight = sorted.reduce((sum, s) => sum + s.weight, 0);
                  const medianWeight = totalWeight / 2;
                  
                  let weightedMedian = sorted[0].pitch;
                  for (let i = 0; i < sorted.length; i++) {
                    weightSum += sorted[i].weight;
                    if (weightSum >= medianWeight) {
                      weightedMedian = sorted[i].pitch;
                      break;
                    }
                  }
                  
                  state.speakerProfile.baselinePitch = weightedMedian;
                  
                  // Weighted variance calculation
                  let pitchWeightSum = 0;
                  let pitchWeightedSum = 0;
                  for (let i = 0; i < weightedPitches.length; i++) {
                    pitchWeightedSum += weightedPitches[i].pitch * weightedPitches[i].weight;
                    pitchWeightSum += weightedPitches[i].weight;
                  }
                  const pitchMean = pitchWeightSum > 0 ? pitchWeightedSum / pitchWeightSum : weightedMedian;
                  
                  let pitchVarSum = 0;
                  for (let i = 0; i < weightedPitches.length; i++) {
                    const diff = weightedPitches[i].pitch - pitchMean;
                    pitchVarSum += diff * diff * weightedPitches[i].weight;
                  }
                  state.speakerProfile.pitchVariance = pitchWeightSum > 0 ? pitchVarSum / pitchWeightSum : 100;
                } else if (validPitches.length > 0) {
                  // Fallback to simple median if no weighted samples
                  const sorted = validPitches.slice().sort((a, b) => a - b);
                  const mid = Math.floor(sorted.length / 2);
                  state.speakerProfile.baselinePitch = sorted.length % 2 === 0 
                    ? (sorted[mid - 1] + sorted[mid]) / 2 
                    : sorted[mid];
                  
                  const pitchMean = state.speakerProfile.baselinePitch;
                  let pitchVarSum = 0;
                  for (let i = 0; i < validPitches.length; i++) {
                    const diff = validPitches[i] - pitchMean;
                    pitchVarSum += diff * diff;
                  }
                  state.speakerProfile.pitchVariance = pitchVarSum / validPitches.length;
                } else {
                  state.speakerProfile.baselinePitch = 150;
                  state.speakerProfile.pitchVariance = 100;
                }

                // IMPROVED: Weighted timbre calculation (prioritize main speaker)
                const validTimbres = state.speakerProfile.timbreHistory;
                const weightedTimbres = state.speakerProfile.weightedTimbreHistory;
                
                if (weightedTimbres.length > 0) {
                  // Weighted mean
                  let timbreWeightSum = 0;
                  let timbreWeightedSum = 0;
                  for (let i = 0; i < weightedTimbres.length; i++) {
                    timbreWeightedSum += weightedTimbres[i].timbre * weightedTimbres[i].weight;
                    timbreWeightSum += weightedTimbres[i].weight;
                  }
                  const timbreMean = timbreWeightSum > 0 ? timbreWeightedSum / timbreWeightSum : 0.1;
                  state.speakerProfile.baselineTimbre = timbreMean;
                  
                  // Weighted variance
                  let timbreVarSum = 0;
                  for (let i = 0; i < weightedTimbres.length; i++) {
                    const diff = weightedTimbres[i].timbre - timbreMean;
                    timbreVarSum += diff * diff * weightedTimbres[i].weight;
                  }
                  state.speakerProfile.timbreVariance = timbreWeightSum > 0 ? timbreVarSum / timbreWeightSum : 0.01;
                } else if (validTimbres.length > 0) {
                  // Fallback to simple mean if no weighted samples
                  let timbreSum = 0;
                  for (let i = 0; i < validTimbres.length; i++) {
                    timbreSum += validTimbres[i];
                  }
                  const timbreMean = timbreSum / validTimbres.length;
                  state.speakerProfile.baselineTimbre = timbreMean;
                  
                  let timbreVarSum = 0;
                  for (let i = 0; i < validTimbres.length; i++) {
                    const diff = validTimbres[i] - timbreMean;
                    timbreVarSum += diff * diff;
                  }
                  state.speakerProfile.timbreVariance = timbreVarSum / validTimbres.length;
                } else {
                  state.speakerProfile.baselineTimbre = 0.1;
                  state.speakerProfile.timbreVariance = 0.01;
                }

                // ENHANCED: Calculate MFCC baseline using weighted average with outlier removal
                let mfccBaseline = null;
                let mfccDeltaBaseline = null;
                let mfccDeltaDeltaBaseline = null;
                
                if (state.speakerProfile.mfccHistory.length > 0) {
                  // Use weighted baseline calculation (prioritizes recent samples, removes outliers)
                  mfccBaseline = calculateWeightedMFCCBaseline(state.speakerProfile.mfccHistory);
                  
                  // Calculate Delta baseline if available
                  if (state.speakerProfile.mfccDeltaHistory.length > 0) {
                    mfccDeltaBaseline = calculateWeightedMFCCBaseline(state.speakerProfile.mfccDeltaHistory);
                  }
                  
                  // Calculate Delta-Delta baseline if available
                  if (state.speakerProfile.mfccDeltaDeltaHistory.length > 0) {
                    mfccDeltaDeltaBaseline = calculateWeightedMFCCBaseline(state.speakerProfile.mfccDeltaDeltaHistory);
                  }
                  
                  // Store CMN/CVN statistics for future normalization
                  if (state.speakerProfile.mfccHistory.length > 0) {
                    const numCoeffs = state.speakerProfile.mfccHistory[0].length;
                    state.speakerProfile.mfccMean = new Array(numCoeffs).fill(0);
                    state.speakerProfile.mfccVariance = new Array(numCoeffs).fill(0);
                    
                    // Calculate mean
                    for (let i = 0; i < state.speakerProfile.mfccHistory.length; i++) {
                      for (let j = 0; j < numCoeffs; j++) {
                        state.speakerProfile.mfccMean[j] += state.speakerProfile.mfccHistory[i][j];
                      }
                    }
                    for (let j = 0; j < numCoeffs; j++) {
                      state.speakerProfile.mfccMean[j] /= state.speakerProfile.mfccHistory.length;
                    }
                    
                    // Calculate variance
                    for (let i = 0; i < state.speakerProfile.mfccHistory.length; i++) {
                      for (let j = 0; j < numCoeffs; j++) {
                        const diff = state.speakerProfile.mfccHistory[i][j] - state.speakerProfile.mfccMean[j];
                        state.speakerProfile.mfccVariance[j] += diff * diff;
                      }
                    }
                    for (let j = 0; j < numCoeffs; j++) {
                      state.speakerProfile.mfccVariance[j] = Math.sqrt(state.speakerProfile.mfccVariance[j] / state.speakerProfile.mfccHistory.length) || 1.0;
                    }
                  }
                }
                
                // Generate enhanced voiceprint signature (composite fingerprint)
                const speakerId = 'speaker_' + Date.now() + '_' + Math.random().toString(36).substring(2, 9);
                state.speakerProfile.speakerId = speakerId;
                state.speakerProfile.currentSpeakerId = speakerId;
                
                state.speakerProfile.voiceprint = {
                  speakerId: speakerId,
                  volume: state.speakerProfile.baselineVolume,
                  pitch: state.speakerProfile.baselinePitch,
                  pitchVariance: state.speakerProfile.pitchVariance,
                  timbre: state.speakerProfile.baselineTimbre,
                  timbreVariance: state.speakerProfile.timbreVariance,
                  mfccBaseline: mfccBaseline,  // Enhanced: Weighted MFCC baseline
                  mfccDeltaBaseline: mfccDeltaBaseline,  // Enhanced: Delta coefficients baseline
                  mfccDeltaDeltaBaseline: mfccDeltaDeltaBaseline,  // Enhanced: Delta-Delta coefficients baseline
                  mfccMean: state.speakerProfile.mfccMean,  // CMN mean
                  mfccVariance: state.speakerProfile.mfccVariance,  // CVN variance
                  spectralCentroid: state.speakerProfile.spectralCentroidHistory.length > 0 
                    ? state.speakerProfile.spectralCentroidHistory.reduce((a, b) => a + b, 0) / state.speakerProfile.spectralCentroidHistory.length
                    : state.speakerProfile.baselineTimbre,
                  sampleCount: state.speakerProfile.frameCount,
                  timestamp: Date.now(),
                  version: 3  // Version 3: Enhanced with Delta/Delta-Delta and CMN/CVN
                };
                
                // Store in known speakers map (multi-speaker support)
                state.speakerProfile.knownSpeakers.set(speakerId, state.speakerProfile.voiceprint);
                
                // Persist to localStorage (persistent storage)
                // FIXED: Merge with existing localStorage data instead of overwriting
                // Previous implementation overwrote all speakers with only current call's speakers,
                // causing permanent loss of speakers from previous calls
                try {
                  // Load existing voiceprints from localStorage
                  const existing = localStorage.getItem('voice_agent_voiceprints');
                  const existingVoiceprints = existing ? JSON.parse(existing) : [];
                  
                  // Create a map of existing voiceprints for quick lookup
                  // FIXED: Extract voiceprint from nested structure when loading existing
                  // localStorage stores: [{ speakerId: "id1", voiceprint: {...} }, ...]
                  // The map should store: speakerId -> nested format { speakerId, voiceprint } for saving
                  // But we need to ensure we're working with the actual voiceprint data, not nested nesting
                  // Store in nested format for consistent saving
                  const existingMap = new Map();
                  for (const vp of existingVoiceprints) {
                    if (vp.speakerId && vp.voiceprint) {
                      // FIXED: Check if voiceprint is already nested (from previous bug)
                      // If it is, extract the actual voiceprint to prevent nested nesting
                      const actualVoiceprint = (vp.voiceprint && typeof vp.voiceprint === 'object' && vp.voiceprint.voiceprint) 
                        ? vp.voiceprint.voiceprint  // Already nested, extract inner voiceprint
                        : vp.voiceprint;            // Not nested, use as-is
                      existingMap.set(vp.speakerId, { speakerId: vp.speakerId, voiceprint: actualVoiceprint });
                    }
                  }
                  
                  // Add/update current call's speakers
                  // knownSpeakers stores actual voiceprint data (not nested), so wrap it for saving
                  for (const [id, vp] of state.speakerProfile.knownSpeakers.entries()) {
                    existingMap.set(id, { speakerId: id, voiceprint: vp });
                  }
                  
                  // Save merged voiceprints back to localStorage
                  // All values are now in consistent nested format: { speakerId, voiceprint }
                  const voiceprintsToSave = Array.from(existingMap.values());
                  localStorage.setItem('voice_agent_voiceprints', JSON.stringify(voiceprintsToSave));
                  console.log(`>> Voiceprint saved to localStorage (${voiceprintsToSave.length} speakers, merged with existing)`);
                } catch (e) {
                  console.warn("Could not save voiceprint to localStorage:", e);
                }

                state.speakerProfile.calibrationComplete = true;
                // FIXED: Reset mismatch tracking state when calibration completes
                // Mismatches detected during calibration phase should not carry into active conversation
                // This prevents false wrong lock detection immediately after calibration completes
                state.mismatchCount = 0;
                state.lastMismatchTime = null;
                const mainSpeakerInfo = state.speakerProfile.callInitiatorDetected 
                  ? " (Call Initiator - Greeting Detected)" 
                  : state.speakerProfile.firstSpeakerDetected 
                    ? " (First Speaker)" 
                    : "";
                console.log(` >> Speaker Profile Calibrated${mainSpeakerInfo} (Production Voiceprint): 
                   RMS=${state.speakerProfile.baselineVolume.toFixed(4)} 
                   Pitch=${state.speakerProfile.baselinePitch.toFixed(0)}Hz (œÉ¬≤=${state.speakerProfile.pitchVariance.toFixed(0)})
                   Timbre(ZCR)=${state.speakerProfile.baselineTimbre.toFixed(4)} (œÉ¬≤=${state.speakerProfile.timbreVariance.toFixed(6)})
                   Voiceprint: ${JSON.stringify(state.speakerProfile.voiceprint)}`);

                // Keep history small (rolling window for adaptive updates)
                state.speakerProfile.volumeHistory = state.speakerProfile.volumeHistory.slice(-50);
                state.speakerProfile.pitchHistory = state.speakerProfile.pitchHistory.slice(-100);
                state.speakerProfile.timbreHistory = state.speakerProfile.timbreHistory.slice(-100);
              }
            }
          }
          
          // Real-time Voice ID enforcement (after calibration)
          if (ENABLE_SPEAKER_FILTERING && state.speakerProfile.calibrationComplete && speechProb > 0.6) {
            // --- ENHANCED: MULTI-SPEAKER MATCHING ---
            // FIXED: Only match if we have sufficient data (avoid using stale MFCC)
            // Try to match against known speakers first (if multi-speaker enabled)
            let matchedSpeaker = null;
            if (state.speakerProfile.knownSpeakers.size > 1) {
              // Get current MFCC if available (calculated periodically)
              // FIXED: MFCC is calculated every 5 frames in processAudioForVAD
              // Since processAudioForVAD runs continuously, last MFCC is at most 5 frames old (recent enough)
              const mfccHistoryLen = state.speakerProfile.mfccHistory.length;
              const currentMFCC = mfccHistoryLen > 0
                ? state.speakerProfile.mfccHistory[mfccHistoryLen - 1]
                : null;
              
              // FIXED: matchSpeaker can work without MFCC (uses pitch, timbre, volume)
              // Previous condition blocked matching when MFCC was null and calibration was complete
              // Since MFCC is calculated every 5 frames, most calls won't have it available
              // This caused multi-speaker identification to fail silently for most frames
              matchedSpeaker = matchSpeaker(currentPitch, currentCentroid, currentRMS, currentMFCC);
              
              if (matchedSpeaker && matchedSpeaker.similarity > 0.7) {
                // Strong match found - update current speaker
                state.speakerProfile.currentSpeakerId = matchedSpeaker.speakerId;
                console.log(`[Multi-Speaker] Matched: ${matchedSpeaker.speakerId} (${(matchedSpeaker.similarity * 100).toFixed(1)}% similarity)`);
              }
            }
            
            // --- REAL-TIME VOICE ID ENFORCEMENT ---
            // Only check if speech is confident (to avoid mismatching noise)

              const basePitch = state.speakerProfile.baselinePitch;
              const baseTimbre = state.speakerProfile.baselineTimbre;

              let mismatch = null;

              // --- PRODUCTION-LEVEL VOICE FINGERPRINT MATCHING ---
              // Uses statistical matching with variance-aware thresholds
              
              // 1. SIMPLIFIED Pitch Check (Natural Conversation Approach)
              // Real counselors don't do complex math - they just remember the voice range
              // Use simple range check: pitch should be within ¬±50% of baseline (very lenient)
              if (currentPitch > 50 && basePitch > 0) {
                const pitchRatio = currentPitch / basePitch;
                // Accept if pitch is within 0.5x to 1.5x baseline (covers natural variation)
                if (pitchRatio < 0.5 || pitchRatio > 1.5) {
                  // Only reject if volume is also very different (likely different speaker)
                  const volumeRatio = currentRMS / (state.speakerProfile.baselineVolume || 0.01);
                  if (volumeRatio < 0.2 || volumeRatio > 3.0) {
                    mismatch = `Pitch+Volume Mismatch (Pitch: ${currentPitch.toFixed(0)}Hz vs ${basePitch.toFixed(0)}Hz, Vol: ${(volumeRatio * 100).toFixed(0)}%)`;
                  }
                  // Otherwise, allow it (main speaker might be excited/calm)
                }
              }

              // 2. SIMPLIFIED Timbre Check (Skip complex checks - timbre varies a lot)
              // Real counselors don't analyze timbre precisely - they just remember the voice
              // Only check timbre if pitch already mismatched (double-check)
              if (mismatch && currentCentroid > 0 && baseTimbre > 0) {
                const timbreRatio = currentCentroid / baseTimbre;
                // Only add timbre mismatch if it's WAY off (>2x or <0.5x) AND volume is also off
                if ((timbreRatio < 0.3 || timbreRatio > 3.0)) {
                  const volumeRatio = currentRMS / (state.speakerProfile.baselineVolume || 0.01);
                  if (volumeRatio < 0.2 || volumeRatio > 3.0) {
                    mismatch += " + Timbre";
                  }
                }
              }
              
              // 3. ENHANCED Voiceprint Check with Adaptive Thresholds and Confidence Scoring
              // Uses Delta/Delta-Delta coefficients, CMN/CVN normalization, and contextual awareness
              if (state.speakerProfile.voiceprint && !mismatch) {
                const mfccHistoryLen = state.speakerProfile.mfccHistory.length;
                const voiceprintToCheck = state.speakerProfile.voiceprint;
                
                // Only check if we have recent MFCC data
                if (mfccHistoryLen > 0 && voiceprintToCheck.mfccBaseline) {
                  const currentMFCC = state.speakerProfile.mfccHistory[mfccHistoryLen - 1];
                  const currentDelta = state.speakerProfile.mfccDeltaHistory.length > 0 
                    ? state.speakerProfile.mfccDeltaHistory[state.speakerProfile.mfccDeltaHistory.length - 1] 
                    : null;
                  const currentDeltaDelta = state.speakerProfile.mfccDeltaDeltaHistory.length > 0 
                    ? state.speakerProfile.mfccDeltaDeltaHistory[state.speakerProfile.mfccDeltaDeltaHistory.length - 1] 
                    : null;
                  
                  // ENHANCED: Use Delta/Delta-Delta coefficients for better matching
                  const mfccSimilarity = calculateMFCCSimilarity(
                    currentMFCC, 
                    voiceprintToCheck.mfccBaseline,
                    currentDelta,
                    voiceprintToCheck.mfccDeltaBaseline,
                    currentDeltaDelta,
                    voiceprintToCheck.mfccDeltaDeltaBaseline
                  );
                  
                  // Update conversation context for adaptive threshold
                  const timeSinceLastQuestion = state.lastAgentQuestion 
                    ? Date.now() - state.lastAgentQuestion.timestamp 
                    : Infinity;
                  state.speakerProfile.conversationContext.isAnsweringQuestion = timeSinceLastQuestion < 5000; // 5 seconds
                  state.speakerProfile.conversationContext.responseTiming = timeSinceLastQuestion;
                  
                  // Calculate adaptive threshold based on context
                  const baseThreshold = 0.3;
                  const adaptiveThreshold = calculateAdaptiveThreshold(
                    baseThreshold, 
                    state.speakerProfile.conversationContext
                  );
                  state.speakerProfile.conversationContext.adaptiveThreshold = adaptiveThreshold;
                  
                  // Calculate confidence score
                  const pitchMatch = currentPitch && state.speakerProfile.baselinePitch 
                    ? Math.abs(currentPitch - state.speakerProfile.baselinePitch) / state.speakerProfile.baselinePitch < 0.5
                    : true;
                  const timbreMatch = currentCentroid && state.speakerProfile.baselineTimbre
                    ? Math.abs(currentCentroid - state.speakerProfile.baselineTimbre) / state.speakerProfile.baselineTimbre < 2.0
                    : true;
                  const volumeMatch = Math.abs(currentRMS - state.speakerProfile.baselineVolume) / state.speakerProfile.baselineVolume < 3.0;
                  
                  // FIXED: Calculate combined acoustic score from all features (not just MFCC)
                  // This provides a more accurate acoustic confidence that considers all voice characteristics
                  const acousticScore = (
                    (mfccSimilarity * 0.4) +  // MFCC is most reliable (40% weight)
                    (pitchMatch ? 0.2 : 0) +  // Pitch match contributes (20% weight)
                    (timbreMatch ? 0.2 : 0) + // Timbre match contributes (20% weight)
                    (volumeMatch ? 0.2 : 0)   // Volume match contributes (20% weight)
                  );
                  
                  const confidence = calculateConfidenceScore(
                    acousticScore,  // Combined acoustic score from all features
                    state.speakerProfile.conversationContext,
                    mfccSimilarity,  // Keep MFCC separate for feature consistency check (10% weight)
                    pitchMatch,
                    timbreMatch,
                    volumeMatch
                  );
                  
                  // Store confidence in history
                  state.speakerProfile.conversationContext.confidenceHistory.push(confidence);
                  if (state.speakerProfile.conversationContext.confidenceHistory.length > 20) {
                    state.speakerProfile.conversationContext.confidenceHistory.shift(); // Keep last 20
                  }
                  
                  // ENHANCED: Use adaptive threshold with confidence scoring
                  // Only reject if similarity is below adaptive threshold AND confidence is low
                  if (mfccSimilarity < adaptiveThreshold && confidence < 0.5) {
                    const volumeRatio = currentRMS / (state.speakerProfile.baselineVolume || 0.01);
                    // Only reject if BOTH MFCC similarity is low AND volume is way off AND confidence is low
                    if (volumeRatio < 0.15 || volumeRatio > 4.0) {
                      mismatch = `Voiceprint Mismatch (MFCC: ${(mfccSimilarity * 100).toFixed(1)}%, Threshold: ${(adaptiveThreshold * 100).toFixed(1)}%, Confidence: ${(confidence * 100).toFixed(1)}%, Volume: ${(volumeRatio * 100).toFixed(0)}%)`;
                    } else {
                      // Low similarity but reasonable volume and context - allow it (context override)
                      console.log(`[CONTEXT OVERRIDE] Low MFCC similarity (${(mfccSimilarity * 100).toFixed(1)}%) but context confirms (confidence: ${(confidence * 100).toFixed(1)}%) - allowing`);
                    }
                  } else if (mfccSimilarity < adaptiveThreshold && confidence >= 0.5) {
                    // Low similarity but high confidence (context confirms) - allow it
                    console.log(`[CONTEXT OVERRIDE] Low MFCC similarity (${(mfccSimilarity * 100).toFixed(1)}%) but high confidence (${(confidence * 100).toFixed(1)}%) - allowing`);
                  }
                }
                // If no MFCC, skip voiceprint check entirely (trust pitch/volume checks above)
              }

              // Set or Clear Mismatch Flag
              state.speakerProfile.lastMismatchReason = mismatch;
              
              // Track consecutive mismatch count (for hysteresis)
              // FIXED: Only track mismatches after calibration is complete
              // During calibration and wake phrase waiting, mismatches are expected and should not be counted
              // This prevents stale mismatch counts from carrying into active conversation
              if (mismatch && state.speakerProfile.calibrationComplete && !state.waitingForWakePhrase) {
                state.mismatchCount++;
                state.lastMismatchTime = Date.now();
              } else if (!mismatch) {
                // FIXED: Reset counter whenever mismatch is falsy, regardless of state
                // This prevents false wrong lock detection after voice features match again
                // Previously, mismatchCount could remain elevated during active conversation,
                // causing false wrong lock detection even when the real caller is speaking
                state.mismatchCount = 0;
                state.lastMismatchTime = null;
              }

              // --- SIMPLIFIED ADAPTIVE PROFILING (Natural Conversation) with SESSION DRIFT ---
              // Only adapt when we're confident it's the main speaker speaking
              // Adapt slowly to maintain stable baseline (like a real counselor remembering the client)
              
              // Check if we're in echo window (don't adapt during echo)
              const timeSinceAgentSpeech = Date.now() - state.lastAgentSpeechEndTime;
              const isInEchoWindow = state.status === 'speaking' || isPlaying || timeSinceAgentSpeech < 2000;
              
              if (!mismatch && speechProb > 0.85 && currentRMS > state.speakerProfile.baselineVolume * 0.3 && !isInEchoWindow) {
                // Adaptive learning rate based on confidence
                let alpha = 0.01; // Default: slow (1% per frame)
                
                // High confidence: faster adaptation (2-3%)
                if (speechProb > 0.85) {
                  const mfccHistoryLen = state.speakerProfile.mfccHistory.length;
                  if (mfccHistoryLen > 0) {
                    const currentMFCC = state.speakerProfile.mfccHistory[mfccHistoryLen - 1];
                    if (state.speakerProfile.voiceprint && state.speakerProfile.voiceprint.mfccBaseline) {
                      const mfccSim = calculateMFCCSimilarity(currentMFCC, state.speakerProfile.voiceprint.mfccBaseline);
                      if (mfccSim > 0.7) {
                        alpha = 0.025; // High confidence: 2.5% per frame
                      } else if (mfccSim > 0.5) {
                        alpha = 0.01; // Medium confidence: 1% per frame
                      } else {
                        alpha = 0.005; // Low confidence: 0.5% per frame
                      }
                    }
                  }
                } else if (speechProb > 0.6) {
                  alpha = 0.01; // Medium confidence: 1% per frame
                } else {
                  alpha = 0.005; // Low confidence: 0.5% per frame
                }

                // FIXED: Only create anchor/session profiles AFTER confirmation step passes
                // These should NOT be created during wake phrase waiting or confirmation
                // They should only be created after the user is locked (confirmation passed)
                if (!state.waitingForWakePhrase && !state.confirmationStep && state.speakerProfile.callInitiatorDetected) {
                  // Initialize anchor profile on first adaptation (locked, never changes)
                  if (!state.speakerProfile.anchorProfile) {
                    state.speakerProfile.anchorProfile = {
                      volume: state.speakerProfile.baselineVolume,
                      pitch: state.speakerProfile.baselinePitch,
                      timbre: state.speakerProfile.baselineTimbre,
                      mfccBaseline: state.speakerProfile.voiceprint?.mfccBaseline || null
                    };
                    console.log(`[ANCHOR] Anchor profile created (locked) - after confirmation passed`);
                  }
                  
                  // Initialize session profile (adaptive, can change)
                  if (!state.speakerProfile.sessionProfile) {
                    state.speakerProfile.sessionProfile = {
                      volume: state.speakerProfile.baselineVolume,
                      pitch: state.speakerProfile.baselinePitch,
                      timbre: state.speakerProfile.baselineTimbre,
                      mfccBaseline: state.speakerProfile.voiceprint?.mfccBaseline || null
                    };
                    console.log(`[SESSION] Session profile initialized - after confirmation passed`);
                  }
                }
                
                // Adapt session profile (not anchor - anchor stays locked)
                if (currentPitch > 50) {
                  state.speakerProfile.sessionProfile.pitch =
                    (state.speakerProfile.sessionProfile.pitch * (1 - alpha)) + (currentPitch * alpha);
                  // Also update baseline for backward compatibility
                  state.speakerProfile.baselinePitch =
                    (state.speakerProfile.baselinePitch * (1 - alpha)) + (currentPitch * alpha);
                }
                if (currentCentroid > 0) {
                  state.speakerProfile.sessionProfile.timbre =
                    (state.speakerProfile.sessionProfile.timbre * (1 - alpha)) + (currentCentroid * alpha);
                  // Also update baseline for backward compatibility
                  state.speakerProfile.baselineTimbre =
                    (state.speakerProfile.baselineTimbre * (1 - alpha)) + (currentCentroid * alpha);
                }

                // Very slow volume adaptation (0.5% per frame) - prevents drift
                if (currentRMS > 0 && currentRMS < state.speakerProfile.baselineVolume * 2.0) {
                  // Only adapt if volume is reasonable (not a spike)
                  state.speakerProfile.sessionProfile.volume =
                    (state.speakerProfile.sessionProfile.volume * 0.995) + (currentRMS * 0.005); // Very slow (0.5%)
                  // Also update baseline for backward compatibility
                  state.speakerProfile.baselineVolume =
                    (state.speakerProfile.baselineVolume * 0.995) + (currentRMS * 0.005); // Very slow (0.5%)
                }

                // SIMPLIFIED: Only log ID Card when there's a significant change (>5%)
                // This prevents constant logging that clutters the console
                if (!state.speakerProfile.lastAdaptationTime || 
                    (Date.now() - state.speakerProfile.lastAdaptationTime) > 5000) {
                  // Log at most once every 5 seconds
                  const pLow = state.speakerProfile.baselinePitch * (1 - 0.45);
                  const pHigh = state.speakerProfile.baselinePitch * (1 + 0.45);
                  const vLow = state.speakerProfile.baselineVolume * 0.2;
                  
                  const currentLogString = `[ID CARD] Main Speaker: Pitch=${state.speakerProfile.baselinePitch.toFixed(0)}Hz (Range: ${pLow.toFixed(0)}-${pHigh.toFixed(0)}Hz) | VolBase=${state.speakerProfile.baselineVolume.toFixed(4)} (> ${vLow.toFixed(4)})`;
                  
                  if (state.speakerProfile.lastLogString !== currentLogString) {
                    console.log(currentLogString);
                    state.speakerProfile.lastLogString = currentLogString;
                    state.speakerProfile.lastAdaptationTime = Date.now();
                  }
                }
              }
          }

          if (currentRMS > state.speakerProfile.recentMaxRMS) {
            state.speakerProfile.recentMaxRMS = currentRMS;
          }
          else {
            state.speakerProfile.recentMaxRMS *= 0.95;
          }
        }

        state.currentVadProb = speechProb;
        // Track peak VAD probability for wake phrase detection
        // This ensures we use the highest confidence during speech, not just the current value
        // Peak is reset when recognition.onresult fires (new speech segment)
        if (speechProb > state.peakVadProb) {
          state.peakVadProb = speechProb;
        }
        
        // CRITICAL FIX: Real-time barge-in detection in VAD processing loop
        // This enables INSTANT UI update when user interrupts agent (before transcript arrives)
        // Makes conversation feel natural and responsive
        if (speechProb > 0.5 && (isPlaying || audioQueue.length > 0)) {
          // High VAD probability + agent speaking = user is interrupting
          if (!state.bargeInVadCount) state.bargeInVadCount = 0;
          state.bargeInVadCount++;
          
          // INSTANT UI UPDATE: Change to listening mode at 2 frames (~60ms)
          // This makes caller feel heard immediately, before transcript is available
          if (state.bargeInVadCount >= 2 && state.status !== 'listening') {
            state.status = 'listening';
            updateUI(); // Instant visual feedback - caller sees they're being heard
            console.log(`[BARGE-IN] Real-time UI update to listening mode (VAD prob: ${speechProb.toFixed(2)}, frames: ${state.bargeInVadCount})`);
          }
          
          // Cancel audio at 3 frames (~90ms) to confirm barge-in
          if (state.bargeInVadCount >= 3) {
            console.log(`[BARGE-IN] Real-time audio cancel via VAD (prob: ${speechProb.toFixed(2)})`);
            // FIXED: Call cancelAudio() instead of duplicating logic
            // This ensures echo protection timeout is set up properly and audio handlers are cleaned up
            cancelAudio();
            ensureRecognitionActive();
            // Status already set to 'listening' above, just ensure UI is updated
            updateUI();
            state.bargeInVadCount = 0; // Reset counter
          }
        } else {
          // Reset counter if VAD drops or agent stops speaking
          state.bargeInVadCount = 0;
        }

        if (vadState.version === 5) {
          if (output.stateN) vadState.h = output.stateN;
        }
        else {
          if (output.hn) vadState.h = output.hn;
          if (output.cn) vadState.c = output.cn;
        }

        // Detect silence (only if VAD is enabled after speech detection)
        // FIXED: Don't stop recognition during confirmation step - user needs to respond
        // CRITICAL FIX: Extended grace period and longer silence threshold to prevent stuck feeling
        // Strategy:
        // 1. 10-second grace period after agent speech (gives users time to think)
        // 2. After grace period, require 15+ seconds TOTAL silence before stopping
        // 3. This ensures users never feel stuck - recognition stays active during natural pauses
        if (speechProb < SPEECH_THRESHOLD) {
          consecutiveSilenceFrames++;
          
          // Calculate time since agent speech ended (for grace period)
          const timeSinceAgentSpeech = state.lastAgentSpeechEndTime > 0 
            ? Date.now() - state.lastAgentSpeechEndTime 
            : Infinity;
          
          // Extended grace period: 10 seconds after agent speech ends
          // This gives users ample time to process the question and formulate a response
          const GRACE_PERIOD_MS = 10000; // 10 seconds grace period (increased from 5)
          const isInGracePeriod = timeSinceAgentSpeech < GRACE_PERIOD_MS;
          
          // After grace period, require 15+ seconds TOTAL silence before stopping
          // This prevents stopping during natural pauses or when user is still thinking
          const TOTAL_SILENCE_THRESHOLD_MS = 15000; // 15 seconds total silence required
          // FIXED: totalSilenceTime should measure time since USER last spoke, not agent
          // This prevents premature recognition stopping when user responds quickly after agent speech
          // If user hasn't responded yet, fall back to agent speech time
          const totalSilenceTime = state.lastUserResponseTime 
            ? Date.now() - state.lastUserResponseTime 
            : timeSinceAgentSpeech; // Fallback to agent speech time if user hasn't responded yet
          const hasExceededTotalSilence = totalSilenceTime >= TOTAL_SILENCE_THRESHOLD_MS;
          
          if (consecutiveSilenceFrames >= SILENCE_FRAMES_THRESHOLD && vadEnabled && !state.confirmationStep) {
            // Check if we're in grace period after agent speech
            if (isInGracePeriod) {
              // In grace period - don't stop recognition, just reset counter
              // This prevents premature stopping when user is thinking/processing the question
              console.log(`VAD: Silence detected but in grace period (${Math.round(timeSinceAgentSpeech/1000)}s since agent speech) - keeping recognition active`);
              consecutiveSilenceFrames = 0; // Reset but don't stop recognition
            } else if (!hasExceededTotalSilence) {
              // Grace period expired but total silence threshold not met
              // Keep recognition active - user might still be thinking or about to speak
              console.log(`VAD: Silence detected but total silence (${Math.round(totalSilenceTime/1000)}s) below threshold (15s) - keeping recognition active`);
              consecutiveSilenceFrames = 0; // Reset but don't stop recognition
            } else {
              // Both grace period expired AND total silence threshold exceeded
              // Only now is it safe to stop recognition (user has been silent for 15+ seconds)
              console.log(`VAD: Extended silence detected (${Math.round(totalSilenceTime/1000)}s), stopping recognition`);
              if (recognition && state.isCallActive) {
                recognition.stop();
                state.status = 'processing';
                updateUI();
                vadEnabled = false; // Reset for next utterance
              }
              consecutiveSilenceFrames = 0; // Reset
            }
          } else if (consecutiveSilenceFrames >= SILENCE_FRAMES_THRESHOLD && vadEnabled && state.confirmationStep) {
            // During confirmation step, don't stop recognition - just reset counter
            console.log("VAD: Silence detected during confirmation step, keeping recognition active");
            consecutiveSilenceFrames = 0; // Reset but don't stop recognition
          }
        }
        else {
          consecutiveSilenceFrames = 0; // Reset on speech
        }
      }
      catch (err) { console.error("VAD processing error:", err); }
    }

    async function setupVADAudioProcessing() {
      if (!aecStream) return;
      try {
        vadContext = new AudioContext({ sampleRate: vadSampleRate });
        const source = vadContext.createMediaStreamSource(aecStream);

        // Load AudioWorklet module
        await vadContext.audioWorklet.addModule('vad-processor.js');

        // Create AudioWorkletNode to replace deprecated ScriptProcessorNode
        vadProcessor = new AudioWorkletNode(vadContext, 'vad-processor');

        // Handle audio data messages from the worklet
        // FIXED: Queue processing to prevent out-of-order execution
        // FIXED: Define flag in outer scope to persist across all message handlers
        // If defined inside onmessage, it would be recreated for each message, causing the check to always be true
        let isProcessingVAD = false;
        const vadQueue = [];
        
        vadProcessor.port.onmessage = async (event) => {
          if (!state.isCallActive) return;
          if (event.data.type === 'audioData') {
            vadQueue.push(event.data.data);
            
            // Process queue sequentially to prevent race conditions
            // FIXED: Set flag BEFORE calling and use await to ensure sequential processing
            if (!isProcessingVAD) {
              isProcessingVAD = true; // Set flag immediately to prevent race condition
              // Start processing queue (will continue until empty)
              processVADQueue().catch(err => {
                console.error("VAD queue processing error:", err);
                isProcessingVAD = false; // Reset flag on error
              });
            }
          }
        };
        
        // FIXED: Convert from recursion to iteration to prevent stack overflow
        // If audio frames arrive faster than they can be processed, recursion would accumulate
        // on the call stack and eventually overflow. Iteration prevents this.
        async function processVADQueue() {
          while (vadQueue.length > 0) {
            const audioData = vadQueue.shift();
            
            try {
              await processAudioForVAD(audioData);
            } catch (err) {
              console.error("VAD processing error:", err);
              // Continue processing queue even after errors
              // Don't reset flag here - let it reset when queue is empty
              // This prevents deadlock: if processAudioForVAD repeatedly fails,
              // the queue will eventually empty and the flag will reset, allowing new items
            }
          }
          
          // Reset flag when queue is empty (handles both normal completion and errors)
          isProcessingVAD = false;
        }

        source.connect(vadProcessor);
        vadProcessor.connect(vadContext.destination);

        console.log("VAD audio processing started (AudioWorklet)");
      }
      catch (err) { console.error("Could not setup VAD audio processing:", err); }
    }

    // Auto-init
    window.onload = async () => {
      connectWebSocket();
      await initVAD(); // Load VAD model on page load
      
      // ENHANCED: Load persisted voiceprints from localStorage
      loadVoiceprintsFromStorage();
    };

    // --- Utils - OPTIMIZED FOR LATENCY ---
    // Fast RMS calculation (inline, no function call overhead for hot path)
    function calculateRMSFast(float32Array) {
      let sumSq = 0;
      const len = float32Array.length;
      // Unroll loop for small arrays (512 samples typical)
      for (let i = 0; i < len; i += 4) {
        const v0 = float32Array[i];
        const v1 = float32Array[i + 1] || 0;
        const v2 = float32Array[i + 2] || 0;
        const v3 = float32Array[i + 3] || 0;
        sumSq += v0 * v0 + v1 * v1 + v2 * v2 + v3 * v3;
      }
      return Math.sqrt(sumSq / len);
    }
    
    function calculateRMS(float32Array) {
      return calculateRMSFast(float32Array);
    }

    function isVolumeOutlier(rms) {
      if (!ENABLE_SPEAKER_FILTERING || !state.speakerProfile.calibrationComplete) return false;

      const baseline = state.speakerProfile.baselineVolume;
      if (!baseline) return false;

      // Check High outlier (Shouting/Background noise)
      if (rms > baseline * VOLUME_OUTLIER_THRESHOLD_HIGH) return true;

      // Check Low outlier (Distant background speaker)
      if (rms < baseline * VOLUME_OUTLIER_THRESHOLD_LOW) return true;

      return false;
    }

    // --- Voice ID Utils ---
    function calculateZeroCrossingRate(buffer) {
      let zcr = 0;
      for (let i = 1; i < buffer.length; i++) {
        if ((buffer[i - 1] > 0 && buffer[i] < 0) || (buffer[i - 1] < 0 && buffer[i] > 0)) { zcr++; }
      }
      return zcr / buffer.length;
    }
    
    /**
     * ENHANCED: Calculate proper spectral centroid using FFT
     * Spectral centroid represents the "brightness" of the sound
     * Higher values = brighter/more high-frequency content
     */
    function calculateSpectralCentroid(buffer, sampleRate) {
      if (!buffer || buffer.length < 256) return 0;
      
      // Use Web Audio API AnalyserNode for efficient FFT
      // For now, use a simple approximation with windowed FFT
      // In production, could use Web Audio API AnalyserNode
      
      const len = buffer.length;
      const fftSize = 512; // Power of 2 for FFT
      
      // Simple windowed FFT approximation (Hann window)
      let weightedSum = 0;
      let magnitudeSum = 0;
      
      // Calculate frequency bins
      const nyquist = sampleRate / 2;
      const binWidth = nyquist / (fftSize / 2);
      
      // Simplified FFT using autocorrelation in frequency domain
      // For production, use proper FFT library or Web Audio API
      for (let k = 0; k < Math.min(fftSize / 2, len / 2); k++) {
        let real = 0;
        let imag = 0;
        
        // DFT calculation (simplified, could use FFT for speed)
        for (let n = 0; n < len; n++) {
          const angle = -2 * Math.PI * k * n / len;
          const window = 0.5 * (1 - Math.cos(2 * Math.PI * n / len)); // Hann window
          const sample = buffer[n] * window;
          real += sample * Math.cos(angle);
          imag += sample * Math.sin(angle);
        }
        
        const magnitude = Math.sqrt(real * real + imag * imag);
        const frequency = k * binWidth;
        
        weightedSum += frequency * magnitude;
        magnitudeSum += magnitude;
      }
      
      return magnitudeSum > 0 ? weightedSum / magnitudeSum : 0;
    }
    
    /**
     * ENHANCED: Calculate MFCC (Mel-Frequency Cepstral Coefficients)
     * MFCCs are standard features for speaker recognition
     * Returns array of 13 MFCC coefficients
     */
    function calculateMFCC(buffer, sampleRate) {
      if (!buffer || buffer.length < 256) return null;
      
      // Simplified MFCC calculation (production would use optimized library)
      // This is a lightweight approximation suitable for real-time use
      
      const len = buffer.length;
      const numCoeffs = 13; // Standard: 13 MFCC coefficients
      const numMelFilters = 26; // Number of mel filter banks
      
      // Step 1: Pre-emphasis filter (high-pass)
      const preEmphasis = 0.97;
      const emphasized = new Float32Array(len);
      emphasized[0] = buffer[0];
      for (let i = 1; i < len; i++) {
        emphasized[i] = buffer[i] - preEmphasis * buffer[i - 1];
      }
      
      // Step 2: Window (Hann window)
      const windowed = new Float32Array(len);
      for (let i = 0; i < len; i++) {
        windowed[i] = emphasized[i] * 0.5 * (1 - Math.cos(2 * Math.PI * i / len));
      }
      
      // Step 3: Simplified FFT (use autocorrelation approximation for speed)
      // In production, use proper FFT
      const fftSize = 512;
      const powerSpectrum = new Float32Array(fftSize / 2);
      
      for (let k = 0; k < fftSize / 2; k++) {
        let real = 0;
        let imag = 0;
        
        for (let n = 0; n < Math.min(len, fftSize); n++) {
          const angle = -2 * Math.PI * k * n / fftSize;
          real += windowed[n] * Math.cos(angle);
          imag += windowed[n] * Math.sin(angle);
        }
        
        powerSpectrum[k] = real * real + imag * imag;
      }
      
      // Step 4: Mel filter bank (simplified)
      const melFilters = createMelFilterBank(numMelFilters, fftSize, sampleRate);
      const melSpectrum = new Float32Array(numMelFilters);
      
      for (let i = 0; i < numMelFilters; i++) {
        let sum = 0;
        for (let j = 0; j < fftSize / 2; j++) {
          sum += powerSpectrum[j] * melFilters[i][j];
        }
        melSpectrum[i] = Math.log(sum + 1e-10); // Log to compress dynamic range
      }
      
      // Step 5: DCT (Discrete Cosine Transform) to get cepstral coefficients
      const mfcc = new Float32Array(numCoeffs);
      for (let i = 0; i < numCoeffs; i++) {
        let sum = 0;
        for (let j = 0; j < numMelFilters; j++) {
          sum += melSpectrum[j] * Math.cos(Math.PI * i * (j + 0.5) / numMelFilters);
        }
        mfcc[i] = sum;
      }
      
      return Array.from(mfcc);
    }
    
    /**
     * Helper: Create Mel filter bank
     */
    function createMelFilterBank(numFilters, fftSize, sampleRate) {
      const nyquist = sampleRate / 2;
      const melMax = 2595 * Math.log10(1 + nyquist / 700);
      const melStep = melMax / (numFilters + 1);
      
      const filters = [];
      for (let i = 0; i < numFilters; i++) {
        const filter = new Float32Array(fftSize / 2);
        const melCenter = (i + 1) * melStep;
        const freqCenter = 700 * (Math.pow(10, melCenter / 2595) - 1);
        const binCenter = Math.floor(freqCenter * fftSize / sampleRate);
        
        const melStart = i * melStep;
        const freqStart = 700 * (Math.pow(10, melStart / 2595) - 1);
        const binStart = Math.floor(freqStart * fftSize / sampleRate);
        
        const melEnd = (i + 2) * melStep;
        const freqEnd = 700 * (Math.pow(10, melEnd / 2595) - 1);
        const binEnd = Math.floor(freqEnd * fftSize / sampleRate);
        
        for (let bin = binStart; bin < binEnd && bin < fftSize / 2; bin++) {
          if (bin < binCenter) {
            // FIXED: Prevent division by zero when binCenter == binStart
            // This can occur when FFT bin spacing is coarse relative to filter count,
            // causing adjacent mel filter center frequencies to map to the same bin after flooring
            const denominator = binCenter - binStart;
            if (denominator === 0) {
              // If center equals start, set filter value to 1.0 (peak at center)
              filter[bin] = 1.0;
            } else {
              filter[bin] = (bin - binStart) / denominator;
            }
          } else {
            // FIXED: Prevent division by zero when binEnd == binCenter
            const denominator = binEnd - binCenter;
            if (denominator === 0) {
              // If end equals center, set filter value to 1.0 (peak at center)
              filter[bin] = 1.0;
            } else {
              filter[bin] = (binEnd - bin) / denominator;
            }
          }
        }
        
        filters.push(filter);
      }
      
      return filters;
    }
    
    /**
     * ENHANCED: Load voiceprints from localStorage (persistent storage)
     */
    function loadVoiceprintsFromStorage() {
      try {
        const stored = localStorage.getItem('voice_agent_voiceprints');
        if (stored) {
          const voiceprints = JSON.parse(stored);
          state.speakerProfile.knownSpeakers.clear();
          
          for (const item of voiceprints) {
            if (item.speakerId && item.voiceprint) {
              state.speakerProfile.knownSpeakers.set(item.speakerId, item.voiceprint);
            }
          }
          
          console.log(`>> Loaded ${voiceprints.length} voiceprint(s) from localStorage`);
          return voiceprints.length;
        }
      } catch (e) {
        console.warn("Could not load voiceprints from localStorage:", e);
      }
      return 0;
    }
    
    /**
     * ENHANCED: Match current voice against known speakers (multi-speaker support)
     */
    function matchSpeaker(currentPitch, currentTimbre, currentVolume, mfccFeatures) {
      if (!state.speakerProfile.knownSpeakers || state.speakerProfile.knownSpeakers.size === 0) {
        return null;
      }
      
      let bestMatch = null;
      let bestSimilarity = 0;
      
      // Get current Delta/Delta-Delta features if available
      const currentDelta = state.speakerProfile.mfccDeltaHistory.length > 0 
        ? state.speakerProfile.mfccDeltaHistory[state.speakerProfile.mfccDeltaHistory.length - 1] 
        : null;
      const currentDeltaDelta = state.speakerProfile.mfccDeltaDeltaHistory.length > 0 
        ? state.speakerProfile.mfccDeltaDeltaHistory[state.speakerProfile.mfccDeltaDeltaHistory.length - 1] 
        : null;
      
      for (const [speakerId, voiceprint] of state.speakerProfile.knownSpeakers.entries()) {
        const similarity = calculateVoiceprintSimilarity(
          currentPitch, currentTimbre, currentVolume, voiceprint
        );
        
        // ENHANCED: If MFCC available, use enhanced similarity with Delta/Delta-Delta
        if (mfccFeatures && voiceprint.mfccBaseline) {
          const mfccSimilarity = calculateMFCCSimilarity(
            mfccFeatures, 
            voiceprint.mfccBaseline,
            currentDelta,
            voiceprint.mfccDeltaBaseline,
            currentDeltaDelta,
            voiceprint.mfccDeltaDeltaBaseline
          );
          
          // Weighted combination: 60% voiceprint, 40% enhanced MFCC (with Delta/Delta-Delta)
          const combinedSimilarity = similarity * 0.6 + mfccSimilarity * 0.4;
          
          // Use adaptive threshold if available, otherwise use 0.5
          const threshold = voiceprint.version >= 3 
            ? state.speakerProfile.conversationContext.adaptiveThreshold || 0.5
            : 0.5;
          
          if (combinedSimilarity > bestSimilarity && combinedSimilarity > threshold) {
            bestSimilarity = combinedSimilarity;
            bestMatch = { speakerId, voiceprint, similarity: combinedSimilarity };
          }
        } else if (similarity > bestSimilarity && similarity > 0.5) {
          // Fallback to basic similarity if MFCC not available
          bestSimilarity = similarity;
          bestMatch = { speakerId, voiceprint, similarity };
        }
      }
      
      return bestMatch;
    }
    
    /**
     * ENHANCED: Normalize MFCC using CMN (Cepstral Mean Normalization) and CVN (Cepstral Variance Normalization)
     * CMN removes channel effects, CVN normalizes variance for better robustness
     */
    function normalizeMFCC(mfcc, mfccHistory) {
      if (!mfcc || mfcc.length === 0) return mfcc;
      
      // If we have history, calculate mean and variance for normalization
      if (mfccHistory && mfccHistory.length > 0) {
        const numCoeffs = mfcc.length;
        const mean = new Array(numCoeffs).fill(0);
        const variance = new Array(numCoeffs).fill(0);
        
        // Calculate mean (CMN)
        for (let i = 0; i < mfccHistory.length; i++) {
          for (let j = 0; j < numCoeffs; j++) {
            mean[j] += mfccHistory[i][j];
          }
        }
        for (let j = 0; j < numCoeffs; j++) {
          mean[j] /= mfccHistory.length;
        }
        
        // Calculate variance (CVN)
        for (let i = 0; i < mfccHistory.length; i++) {
          for (let j = 0; j < numCoeffs; j++) {
            const diff = mfccHistory[i][j] - mean[j];
            variance[j] += diff * diff;
          }
        }
        for (let j = 0; j < numCoeffs; j++) {
          variance[j] = Math.sqrt(variance[j] / mfccHistory.length) || 1.0; // Avoid division by zero
        }
        
        // Apply CMN and CVN
        const normalized = new Array(numCoeffs);
        for (let j = 0; j < numCoeffs; j++) {
          normalized[j] = (mfcc[j] - mean[j]) / variance[j];
        }
        
        return normalized;
      }
      
      // No history yet, return original
      return mfcc;
    }
    
    /**
     * ENHANCED: Calculate Delta coefficients (first derivative - temporal dynamics)
     * Captures how MFCC features change over time
     */
    function calculateDeltaCoefficients(featureHistory) {
      if (!featureHistory || featureHistory.length < 2) return null;
      
      const current = featureHistory[featureHistory.length - 1];
      const previous = featureHistory[featureHistory.length - 2];
      
      if (!current || !previous || current.length !== previous.length) return null;
      
      const delta = new Array(current.length);
      for (let i = 0; i < current.length; i++) {
        delta[i] = current[i] - previous[i];
      }
      
      return delta;
    }
    
    /**
     * ENHANCED: Calculate MFCC similarity using cosine similarity with Delta/Delta-Delta support
     */
    function calculateMFCCSimilarity(mfcc1, mfcc2, delta1 = null, delta2 = null, deltaDelta1 = null, deltaDelta2 = null) {
      if (!mfcc1 || !mfcc2 || mfcc1.length !== mfcc2.length) return 0;
      
      // Static MFCC similarity (weight: 50%)
      let dotProduct = 0;
      let norm1 = 0;
      let norm2 = 0;
      
      for (let i = 0; i < mfcc1.length; i++) {
        dotProduct += mfcc1[i] * mfcc2[i];
        norm1 += mfcc1[i] * mfcc1[i];
        norm2 += mfcc2[i] * mfcc2[i];
      }
      
      const denominator = Math.sqrt(norm1 * norm2);
      const staticSimilarity = denominator > 0 ? dotProduct / denominator : 0;
      
      // If Delta coefficients available, include them (weight: 30%)
      let deltaSimilarity = 0;
      if (delta1 && delta2 && delta1.length === delta2.length) {
        let deltaDot = 0;
        let deltaNorm1 = 0;
        let deltaNorm2 = 0;
        
        for (let i = 0; i < delta1.length; i++) {
          deltaDot += delta1[i] * delta2[i];
          deltaNorm1 += delta1[i] * delta1[i];
          deltaNorm2 += delta2[i] * delta2[i];
        }
        
        const deltaDenom = Math.sqrt(deltaNorm1 * deltaNorm2);
        deltaSimilarity = deltaDenom > 0 ? deltaDot / deltaDenom : 0;
      }
      
      // If Delta-Delta coefficients available, include them (weight: 20%)
      let deltaDeltaSimilarity = 0;
      if (deltaDelta1 && deltaDelta2 && deltaDelta1.length === deltaDelta2.length) {
        let deltaDeltaDot = 0;
        let deltaDeltaNorm1 = 0;
        let deltaDeltaNorm2 = 0;
        
        for (let i = 0; i < deltaDelta1.length; i++) {
          deltaDeltaDot += deltaDelta1[i] * deltaDelta2[i];
          deltaDeltaNorm1 += deltaDelta1[i] * deltaDelta1[i];
          deltaDeltaNorm2 += deltaDelta2[i] * deltaDelta2[i];
        }
        
        const deltaDeltaDenom = Math.sqrt(deltaDeltaNorm1 * deltaDeltaNorm2);
        deltaDeltaSimilarity = deltaDeltaDenom > 0 ? deltaDeltaDot / deltaDeltaDenom : 0;
      }
      
      // Weighted combination: 50% static, 30% delta, 20% delta-delta
      const weights = {
        static: 0.5,
        delta: delta1 && delta2 ? 0.3 : 0,
        deltaDelta: deltaDelta1 && deltaDelta2 ? 0.2 : 0
      };
      
      // Normalize weights if delta/delta-delta not available
      const totalWeight = weights.static + weights.delta + weights.deltaDelta;
      if (totalWeight < 1.0) {
        weights.static = 1.0 / totalWeight * weights.static;
        weights.delta = 1.0 / totalWeight * weights.delta;
        weights.deltaDelta = 1.0 / totalWeight * weights.deltaDelta;
      }
      
      return weights.static * staticSimilarity + 
             weights.delta * deltaSimilarity + 
             weights.deltaDelta * deltaDeltaSimilarity;
    }
    
    /**
     * ENHANCED: Calculate weighted MFCC baseline with outlier removal and temporal weighting
     * Prioritizes recent samples and removes outliers for more robust baseline
     */
    function calculateWeightedMFCCBaseline(mfccHistory) {
      if (!mfccHistory || mfccHistory.length === 0) return null;
      
      const numCoeffs = mfccHistory[0].length;
      const baseline = new Array(numCoeffs).fill(0);
      
      // Temporal weighting: Recent samples have more weight (exponential decay)
      const weights = [];
      const decayFactor = 0.95; // 5% decay per sample
      let totalWeight = 0;
      
      for (let i = 0; i < mfccHistory.length; i++) {
        const weight = Math.pow(decayFactor, mfccHistory.length - 1 - i);
        weights.push(weight);
        totalWeight += weight;
      }
      
      // Calculate weighted mean
      for (let i = 0; i < mfccHistory.length; i++) {
        for (let j = 0; j < numCoeffs; j++) {
          baseline[j] += mfccHistory[i][j] * weights[i];
        }
      }
      
      for (let j = 0; j < numCoeffs; j++) {
        baseline[j] /= totalWeight;
      }
      
      // Outlier removal: Remove samples that are too far from mean
      const filteredHistory = [];
      const outlierThreshold = 2.5; // Standard deviations
      
      // Calculate standard deviation
      const stdDev = new Array(numCoeffs).fill(0);
      for (let i = 0; i < mfccHistory.length; i++) {
        for (let j = 0; j < numCoeffs; j++) {
          const diff = mfccHistory[i][j] - baseline[j];
          stdDev[j] += diff * diff * weights[i];
        }
      }
      
      for (let j = 0; j < numCoeffs; j++) {
        stdDev[j] = Math.sqrt(stdDev[j] / totalWeight);
      }
      
      // Filter outliers and recalculate baseline
      let filteredWeight = 0;
      const filteredBaseline = new Array(numCoeffs).fill(0);
      
      for (let i = 0; i < mfccHistory.length; i++) {
        let isOutlier = false;
        for (let j = 0; j < numCoeffs; j++) {
          const zScore = Math.abs((mfccHistory[i][j] - baseline[j]) / (stdDev[j] || 1.0));
          if (zScore > outlierThreshold) {
            isOutlier = true;
            break;
          }
        }
        
        if (!isOutlier) {
          filteredHistory.push(mfccHistory[i]);
          for (let j = 0; j < numCoeffs; j++) {
            filteredBaseline[j] += mfccHistory[i][j] * weights[i];
          }
          filteredWeight += weights[i];
        }
      }
      
      // Final baseline from filtered samples
      if (filteredWeight > 0) {
        for (let j = 0; j < numCoeffs; j++) {
          filteredBaseline[j] /= filteredWeight;
        }
        return filteredBaseline;
      }
      
      // Fallback to original baseline if all samples are outliers
      return baseline;
    }
    
    /**
     * ENHANCED: Calculate adaptive threshold based on conversation context
     * More lenient when user is answering questions, more strict when suspicious
     */
    function calculateAdaptiveThreshold(baseThreshold, conversationContext) {
      let threshold = baseThreshold; // Default: 0.3
      
      // More lenient when user is answering a question (context confirms identity)
      if (conversationContext.isAnsweringQuestion) {
        threshold *= 0.8; // 20% more lenient (0.3 -> 0.24)
      }
      
      // More lenient based on topic relevance
      if (conversationContext.topicRelevance > 0.7) {
        threshold *= 0.9; // 10% more lenient
      }
      
      // More lenient based on response timing (immediate response suggests real caller)
      if (conversationContext.responseTiming > 0 && conversationContext.responseTiming < 2000) {
        threshold *= 0.85; // 15% more lenient
      }
      
      // More lenient over time (confidence builds)
      if (conversationContext.confidenceHistory.length > 10) {
        const avgConfidence = conversationContext.confidenceHistory.slice(-10).reduce((a, b) => a + b, 0) / 10;
        if (avgConfidence > 0.7) {
          threshold *= 0.9; // 10% more lenient if consistently high confidence
        }
      }
      
      // Clamp threshold between 0.15 and 0.4
      return Math.max(0.15, Math.min(0.4, threshold));
    }
    
    /**
     * ENHANCED: Analyze temporal patterns to distinguish speech from noise
     * Speech has consistent temporal patterns (phonemes, formants), noise is more random
     */
    function analyzeTemporalPatterns(mfccHistory) {
      if (!mfccHistory || mfccHistory.length < 5) return true; // Not enough data, assume speech
      
      // Calculate variance of MFCC coefficients over time
      // Speech has lower variance (more consistent patterns), noise has higher variance (more random)
      const numCoeffs = mfccHistory[0].length;
      const variances = new Array(numCoeffs).fill(0);
      
      // Calculate mean for each coefficient
      const means = new Array(numCoeffs).fill(0);
      for (let i = 0; i < mfccHistory.length; i++) {
        for (let j = 0; j < numCoeffs; j++) {
          means[j] += mfccHistory[i][j];
        }
      }
      for (let j = 0; j < numCoeffs; j++) {
        means[j] /= mfccHistory.length;
      }
      
      // Calculate variance for each coefficient
      for (let i = 0; i < mfccHistory.length; i++) {
        for (let j = 0; j < numCoeffs; j++) {
          const diff = mfccHistory[i][j] - means[j];
          variances[j] += diff * diff;
        }
      }
      for (let j = 0; j < numCoeffs; j++) {
        variances[j] /= mfccHistory.length;
      }
      
      // Average variance across all coefficients
      const avgVariance = variances.reduce((a, b) => a + b, 0) / numCoeffs;
      
      // Speech typically has lower variance (more consistent), noise has higher variance
      // Threshold: if variance is very high (> 2.0), likely noise
      return avgVariance < 2.0;
    }
    
    /**
     * ENHANCED: Calculate multi-factor confidence score
     * Combines acoustic, linguistic, behavioral, and contextual factors
     */
    function calculateConfidenceScore(acousticScore, conversationContext, mfccSimilarity, pitchMatch, timbreMatch, volumeMatch) {
      // Acoustic confidence (40%): Voiceprint matching
      const acousticConfidence = acousticScore || 0;
      
      // Contextual confidence (30%): Conversation context
      let contextualConfidence = 0.5; // Default neutral
      if (conversationContext.isAnsweringQuestion) {
        contextualConfidence = 0.9; // High confidence if answering question
      } else if (conversationContext.topicRelevance > 0.7) {
        contextualConfidence = 0.8; // High confidence if topic relevant
      } else if (conversationContext.responseTiming > 0 && conversationContext.responseTiming < 2000) {
        contextualConfidence = 0.75; // Good confidence if immediate response
      }
      
      // Behavioral confidence (20%): Response patterns
      let behavioralConfidence = 0.5;
      if (conversationContext.confidenceHistory.length > 5) {
        const recentAvg = conversationContext.confidenceHistory.slice(-5).reduce((a, b) => a + b, 0) / 5;
        behavioralConfidence = recentAvg; // Use recent confidence history
      }
      
      // Feature consistency (10%): How well features match
      let featureConfidence = 0.5;
      const featureMatches = [mfccSimilarity > 0.3, pitchMatch, timbreMatch, volumeMatch].filter(Boolean).length;
      featureConfidence = featureMatches / 4; // Percentage of features matching
      
      // Weighted combination
      const confidence = 
        acousticConfidence * 0.4 +
        contextualConfidence * 0.3 +
        behavioralConfidence * 0.2 +
        featureConfidence * 0.1;
      
      return Math.max(0, Math.min(1, confidence)); // Clamp to [0, 1]
    }

    // OPTIMIZED: Fast pitch calculation with early exits and reduced search
    function calculatePitchFast(buffer, sampleRate) {
      if (!buffer || buffer.length < 256) return 0;
      
      const len = buffer.length;
      const minPeriod = Math.floor(sampleRate / 400);
      const maxPeriod = Math.min(Math.floor(sampleRate / 80), len - 1);
      
      // Early exit: Quick RMS check (inline, no function call)
      // FIXED: Count actual samples processed - loop processes every 4th sample (i += 4),
      // so we process len/4 samples, not len. Dividing by len instead of actual count
      // underestimates RMS by a factor of 2, causing valid quiet speech to be incorrectly rejected.
      let rms = 0;
      let sampleCount = 0;
      for (let i = 0; i < len; i += 4) {
        const v = buffer[i];
        rms += v * v;
        sampleCount++;
      }
      rms = Math.sqrt(rms / sampleCount);
      if (rms < 0.01) return 0;
      
      // OPTIMIZATION: Coarse-to-fine search (saves ~60% computation)
      // First pass: Search every 4th period (coarse)
      let bestOffset = -1;
      let bestCorrelation = -Infinity;
      const coarseStep = 4;
      
      for (let offset = minPeriod; offset <= maxPeriod; offset += coarseStep) {
        let correlation = 0;
        let norm1 = 0;
        let norm2 = 0;
        const limit = len - offset;
        
        // Unroll inner loop for speed
        for (let i = 0; i < limit; i += 2) {
          const v1 = buffer[i];
          const v2 = buffer[i + offset];
          correlation += v1 * v2;
          norm1 += v1 * v1;
          norm2 += v2 * v2;
        }
        
        const normalizedCorr = correlation / (Math.sqrt(norm1 * norm2) + 1e-10);
        if (normalizedCorr > bestCorrelation) {
          bestCorrelation = normalizedCorr;
          bestOffset = offset;
        }
      }
      
      // Early exit if no good correlation found
      if (bestCorrelation < 0.2) return 0;
      
      // Second pass: Fine search around best offset (¬±coarseStep)
      const fineStart = Math.max(minPeriod, bestOffset - coarseStep);
      const fineEnd = Math.min(maxPeriod, bestOffset + coarseStep);
      
      for (let offset = fineStart; offset <= fineEnd; offset++) {
        if (offset === bestOffset) continue; // Already checked
        
        let correlation = 0;
        let norm1 = 0;
        let norm2 = 0;
        const limit = len - offset;
        
        for (let i = 0; i < limit; i++) {
          const v1 = buffer[i];
          const v2 = buffer[i + offset];
          correlation += v1 * v2;
          norm1 += v1 * v1;
          norm2 += v2 * v2;
        }
        
        const normalizedCorr = correlation / (Math.sqrt(norm1 * norm2) + 1e-10);
        if (normalizedCorr > bestCorrelation) {
          bestCorrelation = normalizedCorr;
          bestOffset = offset;
        }
      }
      
      if (bestOffset > -1 && bestCorrelation > 0.3) {
        return sampleRate / bestOffset;
      }
      return 0;
    }
    
    // Alias for backward compatibility
    function calculatePitch(buffer, sampleRate) {
      return calculatePitchFast(buffer, sampleRate);
    }
    
    /**
     * OPTIMIZED: Production-level Voiceprint Similarity Calculation
     * Fast path with cached calculations and early exits
     */
    function calculateVoiceprintSimilarity(currentPitch, currentTimbre, currentVolume, voiceprint) {
      if (!voiceprint) return 1.0;
      
      // Cache square roots (avoid recalculating)
      const pitchStdDev = voiceprint.pitchVariance > 0 ? Math.sqrt(voiceprint.pitchVariance) : 0;
      const timbreStdDev = voiceprint.timbreVariance > 0 ? Math.sqrt(voiceprint.timbreVariance) : 0;
      
      let weightedSum = 0;
      let totalWeight = 0;
      
      // 1. Pitch Similarity (40% weight) - Fast path
      if (currentPitch > 50 && voiceprint.pitch > 0) {
        const pitchDiff = currentPitch > voiceprint.pitch 
          ? currentPitch - voiceprint.pitch 
          : voiceprint.pitch - currentPitch; // Faster than Math.abs
        
        if (pitchStdDev > 0) {
          const pitchZScore = pitchDiff / (pitchStdDev + 1e-10);
          const pitchSimilarity = pitchZScore < 3.0 ? Math.max(0, 1 - (pitchZScore / 3.0)) : 0;
          weightedSum += pitchSimilarity * 0.4;
          totalWeight += 0.4;
        } else {
          const pitchRatio = 1 - Math.min(1, pitchDiff / voiceprint.pitch);
          weightedSum += pitchRatio * 0.4;
          totalWeight += 0.4;
        }
      }
      
      // 2. Timbre Similarity (30% weight) - Fast path
      if (currentTimbre > 0 && voiceprint.timbre > 0) {
        const timbreDiff = currentTimbre > voiceprint.timbre 
          ? currentTimbre - voiceprint.timbre 
          : voiceprint.timbre - currentTimbre;
        
        if (timbreStdDev > 0) {
          const timbreZScore = timbreDiff / (timbreStdDev + 1e-10);
          const timbreSimilarity = timbreZScore < 3.0 ? Math.max(0, 1 - (timbreZScore / 3.0)) : 0;
          weightedSum += timbreSimilarity * 0.3;
          totalWeight += 0.3;
        } else {
          const timbreRatio = 1 - Math.min(1, timbreDiff / (voiceprint.timbre + 1e-10));
          weightedSum += timbreRatio * 0.3;
          totalWeight += 0.3;
        }
      }
      
      // 3. Volume Similarity (30% weight) - Fast path
      if (currentVolume > 0 && voiceprint.volume > 0) {
        const minVol = currentVolume < voiceprint.volume ? currentVolume : voiceprint.volume;
        const maxVol = currentVolume > voiceprint.volume ? currentVolume : voiceprint.volume;
        const volumeRatio = minVol / (maxVol + 1e-10);
        weightedSum += volumeRatio * 0.3;
        totalWeight += 0.3;
      }
      
      return totalWeight > 0 ? weightedSum / totalWeight : 1.0;
    }

    // OPTIMIZED: Pre-compute keyword sets for O(1) lookup
    const backgroundKeywordsSet = new Set([
      'did you', 'can you get', 'where is', 'come here', 'look at this',
      'turn on', 'turn off', 'what time', 'dinner', 'lunch', 'breakfast',
      'breaking news', 'weather forecast', 'sports update', 'coming up next',
      'tune in', 'stay tuned', 'commercial break', 'advertisement',
      'close the door', 'open the window', 'switch on', 'switch off',
      'turn the light', 'turn the fan', 'turn the tv', 'turn the radio',
      'answer the phone', 'pick up the phone', 'hang up',
      'start recording', 'stop recording', 'caption', 'subtitle',
      'voice mail', 'leave a message', 'press one', 'press two',
      'hey mom', 'hey dad', 'mom can you', 'dad can you',
      'what are you doing', 'where are you going', 'when are you',
      'play music', 'pause music', 'next song', 'previous song',
      'volume up', 'volume down', 'mute', 'unmute'
    ]);
    
    // Short keywords for fast exact match
    const shortKeywords = ['ok', 'no', 'hi', 'ya', 'yo', 'turn', 'on', 'off', 'up', 'down'];
    
    function isLikelyBackgroundNoise(text) {
      if (!ENABLE_SPEAKER_FILTERING || !ENABLE_KEYWORD_FILTERING) return false;

      const lowerText = text.toLowerCase().trim();
      const textLen = lowerText.length;
      
      // Fast path: Very short text - check exact match first
      if (textLen < 5) {
        return shortKeywords.includes(lowerText);
      }
      
      // Fast path: Exact match in Set (O(1))
      if (backgroundKeywordsSet.has(lowerText)) {
        return true;
      }
      
      // Medium text: Check if text contains any keyword (early exit)
      if (textLen < 30) {
        for (const keyword of backgroundKeywordsSet) {
          if (lowerText.includes(keyword)) return true;
        }
        return false;
      }
      
      // Long text: Check substring matches (only for longer keywords to avoid false positives)
      for (const keyword of backgroundKeywordsSet) {
        if (keyword.length >= 8 && lowerText.includes(keyword)) {
          return true;
        }
      }
      
      return false;
    }

    // MULTI-LAYER FILTER DECISION (Enhanced with hysteresis and echo-aware windows)
    function shouldAcceptInput(transcript, volumeRMS, currentSpokenText) {
      // Fast exit if speaker filtering is disabled
      if (!ENABLE_SPEAKER_FILTERING) return true;
      
      if (!ENABLE_VOLUME_FILTERING || !state.speakerProfile.calibrationComplete) return true;

      const reasons = [];
      const isOutlier = isVolumeOutlier(volumeRMS);
      const isKeyword = isLikelyBackgroundNoise(transcript);

      // Note: Mirroring Echo Logic from onresult here for the decision matrix
      // We don't implement full echo check here to avoid code duplication, 
      // but we rely on the outlier flag to boost decision.

      if (isOutlier) reasons.push('volume_outlier');
      if (isKeyword) reasons.push('background_keyword');

      // REJECT IF: 
      // 1. Volume Outlier AND Keyword (High Confidence Background)
      if (isOutlier && isKeyword) {
        console.log(`>> REJECTED (${reasons.join('+')}): "${transcript}"`);
        return false;
      }

      // 2. Volume Outlier (EXTREME) - e.g. 4x baseline (Shouting) or 0.1x (Whisper)
      // We use stricter thresholds for "pure volume" rejection
      // Fast path: Skip if not calibrated yet
      // CRITICAL: During calibration, always accept real caller's speech
      // This ensures caller is heard from the start, making conversation feel natural
      if (!state.speakerProfile.calibrationComplete) {
        // If we have an initial baseline from wake phrase/confirmation, use it for basic filtering
        // But be very lenient - only reject extremely high volume (likely noise)
        if (state.speakerProfile.baselineVolume && state.speakerProfile.baselineVolume > 0) {
          const baseline = state.speakerProfile.baselineVolume;
          // During calibration, only reject if volume is extremely high (> 20x) which is likely noise
          if (volumeRMS > baseline * 20.0) {
            console.log(`[REJECT] Extremely high volume during calibration (likely noise): "${transcript}" (RMS: ${volumeRMS.toFixed(4)} > ${(baseline * 20.0).toFixed(4)})`);
            return false;
          }
          // Otherwise, accept it - we're still learning the caller's voice
        }
        return true; // Accept until calibration is complete
      }

      const baseline = state.speakerProfile.baselineVolume;
      if (!baseline || baseline <= 0) {
        return true; // Safety check
      }

      // IMPROVED: Better debug logging with more context
      if (volumeRMS < baseline * VOLUME_OUTLIER_THRESHOLD_LOW || volumeRMS > baseline * 4.0) {
        const mismatchReason = state.speakerProfile.lastMismatchReason || 'None';
        console.log(`[FILTER] RMS: ${volumeRMS.toFixed(5)} | Base: ${baseline.toFixed(5)} | Ratio: ${(volumeRMS / baseline).toFixed(2)}x | Mismatch: ${mismatchReason}`);
      }

      // High volume outlier (shouting/background noise spike)
      // CRITICAL FIX: Make threshold adaptive and less aggressive to ensure real caller is always heard
      // If voice features match (no mismatch), be very lenient with volume (10x threshold)
      // If voice features don't match, use stricter threshold (5x) to filter background noise
      const hasVoiceMatch = !state.speakerProfile.lastMismatchReason || 
                           state.speakerProfile.lastMismatchReason.includes('None');
      const highVolumeThreshold = hasVoiceMatch ? 10.0 : 5.0; // Real caller gets 10x, others get 5x
      
      if (volumeRMS > baseline * highVolumeThreshold) {
        // CRITICAL: Even if volume is high, check if voice features match
        // If voice matches (real caller), accept it regardless of volume
        // This ensures real caller is ALWAYS heard, even if they speak loudly
        if (hasVoiceMatch && state.speakerProfile.callInitiatorDetected) {
          console.log(`[ALLOW] High volume but voice matches (real caller) - accepting: "${transcript}" (RMS: ${volumeRMS.toFixed(4)}, threshold: ${(baseline * highVolumeThreshold).toFixed(4)})`);
          // Accept it - real caller should always be heard
        } else {
          console.log(`>> REJECTED (High Volume Outlier): "${transcript}" (RMS: ${volumeRMS.toFixed(4)} > ${baseline.toFixed(4)} * ${highVolumeThreshold})`);
          return false;
        }
      }

      // Low volume outlier (distant background speaker)
      // IMPROVED: Only reject if volume is VERY low AND we have a mismatch reason
      // This prevents rejecting the main speaker who might be speaking quietly
      if (volumeRMS < baseline * VOLUME_OUTLIER_THRESHOLD_LOW) {
        // If we have a biometric mismatch, it's likely background noise
        if (state.speakerProfile.lastMismatchReason) {
          console.log(`>> REJECTED (Low Volume/Background + Biometric Mismatch): "${transcript}" (RMS: ${volumeRMS.toFixed(4)} < Threshold: ${(baseline * VOLUME_OUTLIER_THRESHOLD_LOW).toFixed(4)})`);
          return false;
        }
        // LESS AGGRESSIVE: Only reject if volume is extremely low (< 0.05x baseline) to allow quiet speech
        else if (volumeRMS < baseline * 0.05) {
          console.log(`>> REJECTED (Extremely Low Volume): "${transcript}" (RMS: ${volumeRMS.toFixed(4)} < ${(baseline * 0.05).toFixed(4)})`);
          return false;
        }
        // If volume is low but not extremely low, allow it (main speaker might be speaking quietly)
        else {
          console.log(`[ALLOW] Low volume but no mismatch, allowing: "${transcript}" (RMS: ${volumeRMS.toFixed(4)})`);
        }
      }

      // CRITICAL FIX: Real caller priority - if voice matches and caller is locked, always accept
      // This ensures the real caller is ALWAYS heard, making conversation feel natural and interactive
      if (state.speakerProfile.callInitiatorDetected && state.speakerProfile.calibrationComplete) {
        const hasVoiceMatch = !state.speakerProfile.lastMismatchReason || 
                             state.speakerProfile.lastMismatchReason.includes('None');
        
        // If voice matches (real caller speaking), be very lenient with volume
        // Accept even if volume is 15x baseline (real caller might speak loudly)
        if (hasVoiceMatch) {
          // Real caller is speaking - accept it regardless of volume (within reason)
          // Only reject if volume is extremely high (> 20x) which is likely background noise/shouting
          if (volumeRMS <= baseline * 20.0) {
            console.log(`[ALLOW] Real caller voice match - accepting: "${transcript}" (RMS: ${volumeRMS.toFixed(4)}, ratio: ${(volumeRMS / baseline).toFixed(2)}x)`);
            return true; // Always accept real caller
          } else {
            console.log(`[REJECT] Extremely high volume (likely noise/shouting): "${transcript}" (RMS: ${volumeRMS.toFixed(4)} > ${(baseline * 20.0).toFixed(4)})`);
            return false;
          }
        }
      }
      
      // 3. VOICE ID LITE (Biometric Check) with HYSTERESIS
      // Enforce Pitch/Timbre matching if calibrated
      // This check uses the mismatch flag set in processAudioForVAD (real-time analysis)
      if (state.speakerProfile.calibrationComplete && state.speakerProfile.lastMismatchReason) {
        // HYSTERESIS: Don't reject on single mismatch, require 30+ consecutive mismatches (~1 second)
        // FIXED: Changed threshold from 3 to 30 frames to prevent false positives from brief noise
        // Previous threshold of 3 frames (90ms) was too low and caused frequent false rejections
        const MISMATCH_FRAME_THRESHOLD = 30; // ~1 second of continuous mismatch at 30ms per frame
        if (state.mismatchCount < MISMATCH_FRAME_THRESHOLD) {
          console.log(`[HYSTERESIS] Mismatch detected but count (${state.mismatchCount}) < ${MISMATCH_FRAME_THRESHOLD}, allowing: "${transcript}"`);
          return true; // Accept despite mismatch (hysteresis)
        } else {
          console.log(`>> REJECTED (Biometric Verification - ${state.mismatchCount} consecutive mismatches): ${state.speakerProfile.lastMismatchReason}`);
          return false;
        }
      } else {
        // No mismatch - reset counter and clear mismatch timestamp
        if (state.mismatchCount > 0) {
          console.log(`[HYSTERESIS] Mismatch cleared, resetting counter`);
          state.mismatchCount = 0;
        }
        // FIXED: Clear lastMismatchTime when mismatch resolves to prevent false wrong lock detection
        if (state.lastMismatchTime) {
          state.lastMismatchTime = null;
          console.log(`[HYSTERESIS] Mismatch timestamp cleared`);
        }
      }

      // 4. ECHO-AWARE WINDOWS (More permissive during/after agent speech)
      const timeSinceAgentSpeech = Date.now() - state.lastAgentSpeechEndTime;
      const isInEchoWindow = currentSpokenText || timeSinceAgentSpeech < 2000; // During or 2s after agent speech
      
      if (isInEchoWindow && state.speakerProfile.lastMismatchReason) {
        // In echo window: be more permissive (accept even if slightly uncertain)
        // This prevents blocking real caller during barge-in
        console.log(`[ECHO-AWARE] In echo window, accepting despite mismatch: "${transcript}"`);
        return true; // Accept (but don't adapt baseline - handled in processAudioForVAD)
      }

      // 5. GRACE WINDOWS (More permissive in certain situations)
      const timeSinceLock = Date.now() - (state.speakerProfile.calibrationStartTime || 0);
      const isInGraceWindow = timeSinceLock < 5000; // First 5 seconds after lock
      
      if (isInGraceWindow && state.speakerProfile.lastMismatchReason) {
        // In grace window: be more permissive
        console.log(`[GRACE] In grace window (${Math.round(timeSinceLock/1000)}s), accepting: "${transcript}"`);
        return true;
      }

      return true;
    }

    function levenshtein(a, b) {
      const matrix = [];
      for (let i = 0; i <= b.length; i++) { matrix[i] = [i]; }
      for (let j = 0; j <= a.length; j++) { matrix[0][j] = j; }
      for (let i = 1; i <= b.length; i++) {
        for (let j = 1; j <= a.length; j++) {
          if (b.charAt(i - 1) === a.charAt(j - 1)) {
            matrix[i][j] = matrix[i - 1][j - 1];
          } else {
            matrix[i][j] = Math.min(
              matrix[i - 1][j - 1] + 1,
              matrix[i][j - 1] + 1,
              matrix[i - 1][j] + 1
            );
          }
        }
      }
      return matrix[b.length][a.length];
    }
  </script>
</body>

</html>
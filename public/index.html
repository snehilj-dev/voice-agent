<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Voice AI Agent</title>
  <link rel="icon" type="image/svg+xml" href="/favicon.svg">
  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.19.0/dist/ort.min.js"></script>
  <style>
    :root {
      --primary-color: #667eea;
      --secondary-color: #764ba2;
      --bg-gradient: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      --glass-bg: rgba(255, 255, 255, 0.95);
      --text-color: #333;
      --error-color: #dc3545;
      --success-color: #28a745;
    }

    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    body {
      font-family: 'Segoe UI', Roboto, Helvetica, Arial, sans-serif;
      background: var(--bg-gradient);
      min-height: 100vh;
      display: flex;
      justify-content: center;
      align-items: center;
      color: var(--text-color);
    }

    .app {
      width: 100%;
      padding: 20px;
      display: flex;
      justify-content: center;
    }

    .container {
      background: var(--glass-bg);
      backdrop-filter: blur(10px);
      border-radius: 20px;
      padding: 40px;
      width: 100%;
      max-width: 500px;
      box-shadow: 0 20px 60px rgba(0, 0, 0, 0.3);
      text-align: center;
    }

    .header {
      margin-bottom: 30px;
    }

    .header h1 {
      font-size: 28px;
      margin-bottom: 10px;
      color: #333;
    }

    .subtitle {
      color: #666;
      font-size: 16px;
    }

    .status-card {
      background: #f8f9fa;
      border-radius: 15px;
      padding: 20px;
      margin-bottom: 30px;
      border: 1px solid #e9ecef;
      display: flex;
      flex-direction: column;
      align-items: center;
      gap: 15px;
    }

    .status-indicator {
      display: flex;
      flex-direction: column;
      align-items: center;
      gap: 10px;
    }

    .pulse {
      width: 15px;
      height: 15px;
      border-radius: 50%;
      background-color: #ccc;
      transition: all 0.3s ease;
    }

    .pulse.listening {
      background-color: var(--error-color);
      box-shadow: 0 0 0 0 rgba(220, 53, 69, 0.7);
      animation: pulse-red 1.5s infinite;
    }

    .pulse.speaking {
      background-color: var(--success-color);
      box-shadow: 0 0 0 0 rgba(40, 167, 69, 0.7);
      animation: pulse-green 1.5s infinite;
    }

    .pulse.processing {
      background-color: var(--primary-color);
      animation: spin 1s infinite linear;
      /* Make it look like a spinner for processing if desired, or just pulse blue */
      animation: pulse-blue 1.5s infinite;
    }

    @keyframes pulse-red {
      0% {
        transform: scale(0.95);
        box-shadow: 0 0 0 0 rgba(220, 53, 69, 0.7);
      }

      70% {
        transform: scale(1);
        box-shadow: 0 0 0 10px rgba(220, 53, 69, 0);
      }

      100% {
        transform: scale(0.95);
        box-shadow: 0 0 0 0 rgba(220, 53, 69, 0);
      }
    }

    @keyframes pulse-green {
      0% {
        transform: scale(0.95);
        box-shadow: 0 0 0 0 rgba(40, 167, 69, 0.7);
      }

      70% {
        transform: scale(1);
        box-shadow: 0 0 0 10px rgba(40, 167, 69, 0);
      }

      100% {
        transform: scale(0.95);
        box-shadow: 0 0 0 0 rgba(40, 167, 69, 0);
      }
    }

    @keyframes pulse-blue {
      0% {
        transform: scale(0.95);
        box-shadow: 0 0 0 0 rgba(102, 126, 234, 0.7);
      }

      70% {
        transform: scale(1);
        box-shadow: 0 0 0 10px rgba(102, 126, 234, 0);
      }

      100% {
        transform: scale(0.95);
        box-shadow: 0 0 0 0 rgba(102, 126, 234, 0);
      }
    }

    .status-text {
      font-weight: 600;
      color: #444;
      font-size: 18px;
    }

    .connection-status {
      font-size: 12px;
      color: #666;
      display: flex;
      align-items: center;
      gap: 5px;
    }

    .status-dot {
      width: 8px;
      height: 8px;
      border-radius: 50%;
      background-color: #ccc;
    }

    .status-dot.connected {
      background-color: var(--success-color);
    }

    .status-dot.disconnected {
      background-color: var(--error-color);
    }

    .error-message {
      color: var(--error-color);
      font-size: 14px;
      background: #fff5f5;
      padding: 8px 12px;
      border-radius: 6px;
      width: 100%;
    }

    .controls {
      display: flex;
      justify-content: center;
      margin-bottom: 30px;
    }

    .btn {
      padding: 15px 40px;
      border: none;
      border-radius: 30px;
      font-size: 18px;
      font-weight: 600;
      cursor: pointer;
      transition: all 0.3s ease;
      display: flex;
      align-items: center;
      gap: 10px;
      box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);
    }

    .btn:disabled {
      opacity: 0.6;
      cursor: not-allowed;
    }

    .btn-start {
      background: var(--primary-color);
      color: white;
    }

    .btn-start:hover:not(:disabled) {
      transform: translateY(-2px);
      box-shadow: 0 6px 20px rgba(102, 126, 234, 0.4);
    }

    .btn-end {
      background: #ff4757;
      color: white;
    }

    .btn-end:hover {
      transform: translateY(-2px);
      box-shadow: 0 6px 20px rgba(255, 71, 87, 0.4);
    }

    .info {
      text-align: left;
      background: #f8f9fa;
      padding: 20px;
      border-radius: 15px;
      font-size: 14px;
      color: #555;
    }

    .info p {
      margin-bottom: 10px;
    }

    .info ul {
      list-style-position: inside;
      padding-left: 10px;
    }

    .info li {
      margin-bottom: 5px;
    }
  </style>
</head>

<body>
  <div class="app">
    <div class="container">
      <div class="header">
        <h1>üéôÔ∏è Voice AI Agent</h1>
        <p class="subtitle">Talk to Riya - Your Admissions Assistant</p>
      </div>

      <div class="status-card">
        <div class="status-indicator">
          <div id="pulse-indicator" class="pulse"></div>
          <span id="status-text" class="status-text">Ready to start</span>
        </div>

        <div id="error-container" style="display: none;" class="error-message"></div>

        <div class="connection-status">
          <span id="conn-dot" class="status-dot disconnected"></span>
          <span id="conn-text">Disconnected</span>
        </div>
      </div>

      <div class="controls">
        <button id="start-btn" class="btn btn-start">
          <span class="btn-icon">üìû</span> Start Call
        </button>
        <button id="end-btn" class="btn btn-end" style="display: none;">
          <span class="btn-icon">üì¥</span> End Call
        </button>
      </div>

      <div class="info">
        <p>üí° <strong>How it works:</strong></p>
        <ul>
          <li>Click "Start Call" to begin</li>
          <li>Speak naturally - the agent will listen</li>
          <li>Wait for Riya to respond</li>
          <li>Click "End Call" when finished</li>
        </ul>
      </div>
    </div>
  </div>

  <script>
    // --- Application State ---
    let state = {
      isConnected: false,
      isCallActive: false, // User clicked Start Call
      status: 'idle', // idle, listening, processing, speaking
      error: null
    };

    // --- DOM Elements ---
    const startBtn = document.getElementById('start-btn');
    const endBtn = document.getElementById('end-btn');
    const statusText = document.getElementById('status-text');
    const pulse = document.getElementById('pulse-indicator');
    const connDot = document.getElementById('conn-dot');
    const connText = document.getElementById('conn-text');
    const errorContainer = document.getElementById('error-container');

    // --- Audio & Logic Variables ---
    let socket = null;
    let recognition = null;
    let audioQueue = [];
    let isPlaying = false;
    let activeAudio = null;
    let silenceTimeout = null;
    const SILENCE_THRESHOLD = 2000; // 2 seconds

    // Echo Cancellation
    let aecStream = null;
    let currentSpokenText = ""; // Text currently being spoken by bot

    // Silero VAD
    let vadModel = null;
    let vadContext = null;
    let vadProcessor = null;
    let vadState = null; // VAD internal state
    let vadSampleRate = 16000;
    let consecutiveSilenceFrames = 0;
    const SILENCE_FRAMES_THRESHOLD = 3; // ~96ms of silence

    // --- UI Update Helper ---
    function updateUI() {
      // connection
      if (state.isConnected) {
        connDot.className = 'status-dot connected';
        connText.innerText = 'Connected';
        startBtn.disabled = false;
      } else {
        connDot.className = 'status-dot disconnected';
        connText.innerText = 'Disconnected';
        startBtn.disabled = true;
      }

      // Error
      if (state.error) {
        errorContainer.style.display = 'block';
        errorContainer.innerText = '‚ö†Ô∏è ' + state.error;
      } else {
        errorContainer.style.display = 'none';
      }

      // Call Buttons
      if (state.isCallActive) {
        startBtn.style.display = 'none';
        endBtn.style.display = 'flex';
      } else {
        startBtn.style.display = 'flex';
        endBtn.style.display = 'none';
      }

      // Status Text & Pulse
      pulse.className = 'pulse'; // reset
      if (!state.isCallActive) {
        statusText.innerText = 'Ready to start';
        state.status = 'idle';
      } else {
        // Mapping internal status to UI
        switch (state.status) {
          case 'listening':
            statusText.innerText = 'üéß Listening...';
            pulse.classList.add('listening');
            break;
          case 'processing':
            statusText.innerText = '‚öôÔ∏è Processing...';
            pulse.classList.add('processing');
            break;
          case 'speaking':
            statusText.innerText = 'üîä Agent speaking...';
            pulse.classList.add('speaking');
            break;
          default:
            statusText.innerText = 'Active';
        }
      }
    }

    // --- WebSocket Logic ---
    function connectWebSocket() {
      const protocol = window.location.protocol === "https:" ? "wss:" : "ws:";
      const host = window.location.host;
      const wsUrl = `${protocol}//${host}`;

      socket = new WebSocket(wsUrl);
      socket.binaryType = "arraybuffer";

      socket.onopen = () => {
        console.log("WS Connected");
        state.isConnected = true;
        state.error = null;
        updateUI();
      };

      socket.onclose = () => {
        console.log("WS Closed");
        state.isConnected = false;
        // Auto-reconnect after 3s
        setTimeout(connectWebSocket, 3000);
        updateUI();
      };

      socket.onerror = (err) => {
        console.error(err);
        state.isConnected = false;
        state.error = "Connection failed";
        updateUI();
      };

      socket.onmessage = (event) => {
        if (typeof event.data === "string") {
          try {
            const parsed = JSON.parse(event.data);
            if (parsed.type === "llm_reply") {
              // Store text for echo detection
              // We append to a temporary buffer or queue if needed, 
              // but for now, simple "last received" is okay or we track per audio.
              // Ideally, we attach text to the audio URL, but let's simplify:
              // We can't easily sync text to audio blobs here without complex queueing.
              // IMPROVEMENT: For now, we will relax the check to just "isSpeaking".
              currentSpokenText = parsed.text;
            } else if (parsed.type === "llm_error" || parsed.type === "tts_error") {
              state.error = parsed.error;
              state.status = 'listening'; // Go back to listening if error
              updateUI();
            }
          } catch (e) { }
        } else {
          // Binary Audio
          const arrayBuffer = event.data;
          const blob = new Blob([arrayBuffer], { type: "audio/mpeg" });
          const url = URL.createObjectURL(blob);
          queueAudio(url);
        }
      };
    }

    // --- Audio Queue ---
    function queueAudio(url) {
      audioQueue.push(url);
      processQueue();
    }

    function processQueue() {
      if (isPlaying || audioQueue.length === 0) return;

      isPlaying = true;
      state.status = 'speaking';
      updateUI();

      const url = audioQueue.shift();
      activeAudio = new Audio(url);

      activeAudio.onended = () => {
        isPlaying = false;
        activeAudio = null;
        // If more audio, keep speaking, else back to listening
        if (audioQueue.length > 0) {
          processQueue();
        } else {
          state.status = 'listening';
          currentSpokenText = ""; // Clear text when done
          updateUI();
        }
      };

      activeAudio.onerror = (err) => {
        console.error("Audio playback error", err);
        isPlaying = false;
        activeAudio = null;
        state.status = 'listening';
        updateUI();
        processQueue();
      };

      activeAudio.play().catch(e => {
        if (e.name === 'AbortError') return; // Ignore intentional stops
        console.error("Autoplay failed", e);
        state.error = "Audio blocked. Click page.";
        updateUI();
      });
    }

    function cancelAudio() {
      if (activeAudio) {
        activeAudio.pause();
        activeAudio = null;
      }
      audioQueue = [];
      isPlaying = false;
      currentSpokenText = ""; // Clear text on cancel
      // If we cancel, we usually go back to listening immediately
      state.status = 'listening';
      updateUI();
    }

    // --- Web Speech API Logic ---
    async function startCall() {
      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
      if (!SpeechRecognition) {
        state.error = "Browser not supported (Use Chrome)";
        updateUI();
        return;
      }

      // 1. AEC Hack: Request mic with echoCancellation to force Hardware AEC
      try {
        aecStream = await navigator.mediaDevices.getUserMedia({
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true
          }
        });
        // We don't play this stream, just hold it open.
        console.log("AEC Stream active");

        // Setup VAD audio processing if model is loaded
        if (vadModel) {
          await setupVADAudioProcessing();
        }
      } catch (err) {
        console.warn("Could not get AEC stream", err);
      }

      state.isCallActive = true;
      state.status = 'listening';
      updateUI();

      if (!recognition) {
        recognition = new SpeechRecognition();
        recognition.continuous = true;
        recognition.interimResults = true;
        recognition.lang = "en-US";

        recognition.onstart = () => {
          console.log("Recognition started");
          state.status = 'listening';
          updateUI();
        };

        recognition.onerror = (event) => {
          if (event.error === 'no-speech') return;
          console.warn("Recognition error:", event.error);
          // Don't kill calls on temporary errors
        };

        recognition.onend = () => {
          console.log("Recognition ended");
          // Auto-restart if call is active
          if (state.isCallActive) {
            setTimeout(() => {
              try { recognition.start(); } catch (e) { }
            }, 100);
          }
        };

        recognition.onresult = (event) => {
          clearTimeout(silenceTimeout);

          let finalTranscript = "";
          let interimTranscript = "";

          for (let i = event.resultIndex; i < event.results.length; ++i) {
            if (event.results[i].isFinal) {
              finalTranscript += event.results[i][0].transcript;
            } else {
              interimTranscript += event.results[i][0].transcript;
            }
          }

          let fullTranscript = (finalTranscript + interimTranscript).trim().toLowerCase();

          // --- ECHO DETECTION (FUZZY) ---
          if (isPlaying && currentSpokenText) {
            const textToMatch = currentSpokenText.toLowerCase();
            const cleanInput = fullTranscript.toLowerCase();

            // 1. Direct Inclusion (Fast)
            if (textToMatch.includes(cleanInput) || cleanInput.includes(textToMatch)) {
              console.log(">> Echo detected (Exact/Substr):", cleanInput);
              return;
            }

            // 2. Fuzzy Match (Levenshtein) - For partial/noisy echoes
            // Only run if input is decent length (> 5 chars) to avoid false positives on short words like "hi"
            if (cleanInput.length > 5) {
              const distance = levenshtein(cleanInput, textToMatch);
              const maxLen = Math.max(cleanInput.length, textToMatch.length);
              const similarity = 1 - (distance / maxLen);

              // If 60% similar, assume it's an echo
              if (similarity > 0.6) {
                console.log(`>> Echo detected (Fuzzy ${Math.round(similarity * 100)}%):`, cleanInput);
                return;
              }
            }
          }

          // Barge-in
          if (finalTranscript || interimTranscript) {
            if (isPlaying || audioQueue.length > 0) {
              cancelAudio();
            }
          }

          // Silence Detection: VAD (fast) or Timer (fallback)
          if (finalTranscript || interimTranscript) {
            if (!vadModel) {
              // Fallback: Use 2-second timer if VAD didn't load
              clearTimeout(silenceTimeout);
              silenceTimeout = setTimeout(() => {
                console.log("Fallback timer: Silence detected -> Stop & Send");
                recognition.stop();
                state.status = 'processing';
                updateUI();
              }, SILENCE_THRESHOLD);
            }
            // If VAD is loaded, it handles silence detection automatically
          }

          if (finalTranscript) {
            console.log("Sending:", finalTranscript);
            if (socket && socket.readyState === WebSocket.OPEN) {
              socket.send(JSON.stringify({ type: 'user_text', text: finalTranscript }));
            }
          }
        };
      }

      try { recognition.start(); } catch (e) { }
    }

    function stopCall() {
      state.isCallActive = false;
      // Logic to stop everything
      if (recognition) {
        recognition.stop();
      }
      // Stop AEC Stream
      if (aecStream) {
        aecStream.getTracks().forEach(track => track.stop());
        aecStream = null;
      }
      // Stop VAD Processing
      if (vadProcessor) {
        vadProcessor.disconnect();
        vadProcessor = null;
      }
      if (vadContext) {
        vadContext.close();
        vadContext = null;
      }
      consecutiveSilenceFrames = 0; // Reset VAD state
      cancelAudio();
      state.status = 'idle';
      updateUI();
    }

    // --- Event Listeners ---
    startBtn.addEventListener('click', startCall);
    endBtn.addEventListener('click', stopCall);

    // --- Silero VAD Functions ---
    async function initVAD() {
      try {
        console.log("Loading Silero VAD model...");
        vadModel = await ort.InferenceSession.create(
          "https://huggingface.co/onnx-community/silero-vad/resolve/main/onnx/model.onnx"
        );

        // Initialize VAD state (required by model) - 2 layers x 1 batch x 64 hidden = 128 total
        const h = new ort.Tensor("float32", new Float32Array(2 * 1 * 64).fill(0), [2, 1, 64]);
        vadState = { h }; // Only need h for the HF model

        console.log("Silero VAD loaded successfully");
      } catch (err) {
        console.warn("Could not load VAD model, falling back to timer:", err);
      }
    }

    async function processAudioForVAD(audioData) {
      if (!vadModel || !state.isCallActive) return;

      try {
        // Prepare input tensor (512 samples at 16kHz)
        const audioTensor = new ort.Tensor("float32", audioData, [1, audioData.length]);
        const sr = new ort.Tensor("int64", BigInt64Array.from([BigInt(vadSampleRate)]));

        // Hugging Face model expects 'state' instead of separate h/c
        const inputs = {
          input: audioTensor,
          state: vadState.h, // Use h as state (model will handle internally)
          sr: sr
        };

        const output = await vadModel.run(inputs);
        const speechProb = output.output.data[0]; // 0.0 to 1.0

        // Update state for next iteration (if model returns stateN)
        if (output.stateN) {
          vadState.h = output.stateN;
        }

        // Detect silence
        if (speechProb < 0.5) {
          consecutiveSilenceFrames++;
          if (consecutiveSilenceFrames >= SILENCE_FRAMES_THRESHOLD) {
            // User stopped speaking!
            console.log("VAD: Silence detected, stopping recognition");
            if (recognition && state.isCallActive) {
              recognition.stop();
              state.status = 'processing';
              updateUI();
            }
            consecutiveSilenceFrames = 0; // Reset
          }
        } else {
          consecutiveSilenceFrames = 0; // Reset on speech
        }
      } catch (err) {
        console.error("VAD processing error:", err);
      }
    }

    async function setupVADAudioProcessing() {
      if (!aecStream) return;

      try {
        vadContext = new AudioContext({ sampleRate: vadSampleRate });
        const source = vadContext.createMediaStreamSource(aecStream);

        // Create ScriptProcessor for audio chunks
        vadProcessor = vadContext.createScriptProcessor(512, 1, 1);

        vadProcessor.onaudioprocess = (e) => {
          if (!state.isCallActive) return;

          const inputData = e.inputBuffer.getChannelData(0);
          processAudioForVAD(inputData);
        };

        source.connect(vadProcessor);
        vadProcessor.connect(vadContext.destination);

        console.log("VAD audio processing started");
      } catch (err) {
        console.error("Could not setup VAD audio processing:", err);
      }
    }

    // Auto-init
    window.onload = async () => {
      connectWebSocket();
      await initVAD(); // Load VAD model on page load
    };

    // --- Utils ---
    function levenshtein(a, b) {
      const matrix = [];
      for (let i = 0; i <= b.length; i++) { matrix[i] = [i]; }
      for (let j = 0; j <= a.length; j++) { matrix[0][j] = j; }
      for (let i = 1; i <= b.length; i++) {
        for (let j = 1; j <= a.length; j++) {
          if (b.charAt(i - 1) === a.charAt(j - 1)) {
            matrix[i][j] = matrix[i - 1][j - 1];
          } else {
            matrix[i][j] = Math.min(
              matrix[i - 1][j - 1] + 1,
              matrix[i][j - 1] + 1,
              matrix[i - 1][j] + 1
            );
          }
        }
      }
      return matrix[b.length][a.length];
    }
  </script>
</body>

</html>